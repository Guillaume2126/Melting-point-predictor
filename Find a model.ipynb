{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e72b674",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bc87c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"final_dataframe_train.csv\")\n",
    "df_train = df_train.drop('Unnamed: 0', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdc37623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAGiCAYAAAASgEe5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2T0lEQVR4nO3dfVzUdb7//yeXo2hAqIDmFRsmsuJq2sp0YVoIKe1NFqjstGatp866aKuYFm5r6ZaUlVa7Jdvu2dX9nayzmdmJpGTJlBJS6bhfUfFqvSh1wFZlEhSGge8fffn8nNSiYhje8Ljfbt7k8/68Zj6v8Xb7ME/fnyu/pqamJgEAABjK39cNAAAAfB+EGQAAYDTCDAAAMBphBgAAGI0wAwAAjEaYAQAARiPMAAAAoxFmAACA0QgzAADAaIQZAABgNK+HmaNHj+pnP/uZevTooa5duyohIUHbtm2z1jc1NWnBggXq3bu3unbtqqSkJO3bt8/jPU6ePKm77rpLoaGhCg8P17Rp03TmzBlvtw4AAAzg1TBz6tQpXXfddQoKClJBQYF27dqlZ599VpdffrlVs2TJEr3wwgvKy8vTxx9/rG7duiklJUXnzp2zau666y7t3LlThYWFys/P16ZNm3T//fd7s3UAAGAIP28+aPLhhx/WRx99pOLi4ouub2pqUp8+fTRnzhw9+OCDkqTq6mpFRUVpxYoVmjx5snbv3q34+Hht3bpVo0aNkiS9++67mjhxoj777DP16dPHW+0DAAADBHrzzf/nf/5HKSkpuu2227Rx40ZdccUV+uUvf6n77rtPknTw4EE5HA4lJSVZrwkLC9Po0aNVUlKiyZMnq6SkROHh4VaQkaSkpCT5+/vr448/1k9/+tMLtltXV6e6ujprubGxUSdPnlSPHj3k5+fnxU8MAABaS1NTk7744gv16dNH/v6XPpjk1TDzz3/+U8uXL1d2drbmz5+vrVu36oEHHlBwcLCmTp0qh8MhSYqKivJ4XVRUlLXO4XAoMjLSs+nAQEVERFg1X5Wbm6uFCxd64RMBAIC29umnn6pv376XXO/VMNPY2KhRo0Zp8eLFkqQRI0aovLxceXl5mjp1qte2m5OTo+zsbGu5urpa/fv318GDB3XZZZd5bbsA2p7L5dKGDRs0btw4BQUF+bodAK3oiy++UExMzDd+d3s1zPTu3Vvx8fEeY0OGDNEbb7whSYqOjpYkVVZWqnfv3lZNZWWlhg8fbtVUVVV5vEdDQ4NOnjxpvf6rbDabbDbbBeMREREKDQ39zp8HQPvjcrkUEhKiHj16EGaADqZ5n/6mU0S8ejXTddddpz179niM7d27VwMGDJAkxcTEKDo6WkVFRdZ6p9Opjz/+WHa7XZJkt9t1+vRplZWVWTXvv/++GhsbNXr0aG+2DwAADODVmZnZs2fr2muv1eLFi3X77bdry5Ytevnll/Xyyy9L+jJpzZo1S48//rgGDRqkmJgY/eY3v1GfPn2UlpYm6cuZnFtuuUX33Xef8vLy5HK5NGPGDE2ePJkrmQAAgHfDzDXXXKM333xTOTk5WrRokWJiYvTcc8/prrvusmrmzZunmpoa3X///Tp9+rSuv/56vfvuu+rSpYtV88orr2jGjBm6+eab5e/vr4yMDL3wwgvebB0AABjCq/eZaS+cTqfCwsJUXV3NOTNAB+NyubRu3TpNnDiRc2aADqal3988mwkAABiNMAMAAIxGmAEAAEYjzAAAAKMRZgAYy+12a+PGjdq0aZM2btwot9vt65YA+ABhBoCR1qxZo9jYWI0fP15Lly7V+PHjFRsbqzVr1vi6NQBtjDADwDhr1qxRZmamEhISVFxcrFdffVXFxcVKSEhQZmYmgQboZLjPDACjuN1uxcbGKiEhQWvXrpXb7bbuMxMQEKC0tDSVl5dr3759CggI8HW7AL4H7jMDoEMqLi7WoUOHNH/+fPn7e/4K8/f3V05Ojg4ePKji4mIfdQigrRFmABjl+PHjkqShQ4dedH3zeHMdgI6PMAPAKL1795YklZeXX3R983hzHYCOjzADwCg33HCDBg4cqMWLF6uxsdFjXWNjo3JzcxUTE6MbbrjBRx0CaGuEGQBGCQgI0LPPPqv8/HylpaWptLRUZ8+eVWlpqdLS0pSfn69nnnmGk3+BTiTQ1w0AwLeVnp6u1atXa86cORozZow1HhMTo9WrVys9Pd2H3QFoa1yaDcBYbrdbGzZsUEFBgSZMmKBx48YxIwN0IC39/mZmBoCxAgICdOONN6qmpkY33ngjQQbopDhnBgAAGI0wAwAAjEaYAQAARiPMAAAAoxFmAACA0QgzAADAaIQZAABgNMIMAAAwGmEGgLHcbrc2btyoTZs2aePGjXK73b5uCYAPEGYAGGnNmjWKjY3V+PHjtXTpUo0fP16xsbFas2aNr1sD0MYIMwCMs2bNGmVmZiohIUHFxcV69dVXVVxcrISEBGVmZhJogE6GB00CMIrb7VZsbKwSEhK0du1aud1urVu3ThMnTlRAQIDS0tJUXl6uffv28awmwHAt/f5mZgaAUYqLi3Xo0CHNnz9f/v6ev8L8/f2Vk5OjgwcPqri42EcdAmhrhBkARjl+/LgkaejQoRc9AXjo0KEedQA6vkBfNwAA30bv3r0lSb///e/1hz/8QYcOHZIkLV26VAMHDtT999/vUQeg4+OcGQBGcbvd6t27t06cOKFbb71VDz30kD777DP17dtXTz31lPLz8xUZGaljx45xzgxgOM6ZAdBh+fn5WT83/3+sE/y/DMAleDXMPPbYY/Lz8/P4ExcXZ60/d+6csrKy1KNHD3Xv3l0ZGRmqrKz0eI8jR44oNTVVISEhioyM1Ny5c9XQ0ODNtgG0Y8XFxaqqqlJubq7Ky8s1ZswY3XnnnRozZox27typxYsXq6qqihOAgU7E6zMzP/zhD3X8+HHrz4cffmitmz17tt5++229/vrr2rhxo44dO6b09HRrvdvtVmpqqurr67V582atXLlSK1as0IIFC7zdNoB2qvnE3hkzZmj//v0qLCxUdna2CgsLtW/fPs2YMcOjDkDH5/UwExgYqOjoaOtPz549JUnV1dX6z//8Ty1dulQ33XSTRo4cqb/85S/avHmzSktLJUnr16/Xrl279F//9V8aPny4JkyYoN/+9rd68cUXVV9f7+3WAbRDzSf2lpeXX3R98zgnAAOdh9evZtq3b5/69OmjLl26yG63Kzc3V/3791dZWZlcLpeSkpKs2ri4OPXv318lJSVKTExUSUmJEhISFBUVZdWkpKRo+vTp2rlzp0aMGHHRbdbV1amurs5adjqdkiSXyyWXy+WlTwqgLSQmJmrgwIGaMWOGPv/8cx0+fFjSl1czDRgwQD179lRMTIwSExPZ3wHDtXQf9mqYGT16tFasWKHBgwfr+PHjWrhwoW644QaVl5fL4XAoODhY4eHhHq+JioqSw+GQJDkcDo8g07y+ed2l5ObmauHChReMr1+/XiEhId/zUwHwteHDh2vt2rUKDw/XL3/5S40aNUrbtm3TqlWrdPjwYaWlpem9997zdZsAvqfa2toW1Xk1zEyYMMH6ediwYRo9erQGDBigv/3tb+ratavXtpuTk6Ps7Gxr2el0ql+/fkpOTubSbMBwbrdbs2bN0tVXX61//etfeumll6x1AwcO1A9+8AP94x//UEpKCpdmA4ZrPrLyTdr0pnnh4eG66qqrtH//fo0fP1719fU6ffq0x+xMZWWloqOjJUnR0dHasmWLx3s0X+3UXHMxNptNNpvtgvGgoCAFBQW1wicB4CsfffSRDh06pP/4j/9QXl6ex7qmpiZlZmZq/vz5Ki0t1dixY33TJIBW0dLv7Da9z8yZM2d04MAB9e7dWyNHjlRQUJCKioqs9Xv27NGRI0dkt9slSXa7XTt27FBVVZVVU1hYqNDQUMXHx7dl6wDaiearlHJycpSQkKAXXnhBM2bM0AsvvKCEhATNnz/fow5Ax+fVmZkHH3xQP/nJTzRgwAAdO3ZMjz76qAICAnTnnXcqLCxM06ZNU3Z2tiIiIhQaGqqZM2fKbrcrMTFRkpScnKz4+HhNmTJFS5YskcPh0COPPKKsrKyLzrwA6PgiIyMlfXnBQHl5ufLz8611AwcOVFxcnCoqKqw6AB2fV2dmPvvsM915550aPHiwbr/9dvXo0UOlpaXq1auXJGnZsmW69dZblZGRoTFjxig6Olpr1qyxXh8QEKD8/HwFBATIbrfrZz/7me6++24tWrTIm20DMEBFRYWGDh2q4uJivfrqqyouLtbQoUNVUVHh69YAtDGvzsy89tprX7u+S5cuevHFF/Xiiy9esmbAgAFat25da7cGwFBfvZLxUo8z+LorHgF0LDw1G4BRTpw4IUmaPn26CgoKNGbMGGtdTEyMfvGLXygvL8+qA9Dx8aBJAEZpPkx96NAh7d271+NxBnv27LFuotdcB6DjI8wAMMoVV1whSSooKFBGRoZsNpuuueYa2Ww2ZWRkqKCgwKMOQMfn1/TVA80dkNPpVFhYmKqrq7lpHmA4t9ut2NhY9ezZUydOnLBmYqQvr2bq2bOn/vWvf2nfvn3cNA8wXEu/vzlnBoBRAgIC9OyzzyozM1OpqanKzs7Wvn37NGjQIBUWFuqdd97R6tWrCTJAJ0KYAWCc9PR0rV69WnPmzPG4z0xMTIxWr16t9PR0H3YHoK1xmAmAsdxutzZs2KCCggJNmDBB48aNY0YG6EA4zASgwwsICNCNN96ompoa3XjjjQQZoJPiaiYAAGA0wgwAADAaYQaAsc6ePasHHnhAjz32mB544AGdPXvW1y0B8AFOAAZgpLS0NL311lsXjE+aNElr165t+4YAtDpOAAbQYTUHmeDgYKWnpyskJES1tbVas2aN3nrrLaWlpRFogE6EmRkARjl79qxCQkIUGBioK664wuMOwAMGDNDRo0fV0NCg2tpade3a1YedAvi+Wvr9zTkzAIwyd+5cSVJDQ4OGDRum4uJivfrqqyouLtawYcPU0NDgUQeg4yPMADDK3r17JUk33XST3njjDZ07d05bt27VuXPn9MYbb2jcuHEedQA6Ps6ZAWCUbt26Sfry7r9XXXWVDh06JElaunSpBg4cqH79+nnUAej4mJkBYJS0tDRJ0saNG/XDH/7Q4zBT8/L5dQA6PmZmABilb9++1s+FhYUaMmSIYmJiVFZWpsLCwovWAejYCDMAjBQREaGTJ0/qmWeeueg4gM6DMAPAKFVVVZKkU6dOaeLEiQoODtaBAwd05ZVXqr6+XgUFBR51ADo+zpkBYJTevXtLkhYvXqxdu3Zp7dq12rFjh9auXavdu3friSee8KgD0PERZgAY5YYbbtDAgQO1efNm7d27V4WFhcrOzlZhYaH27NmjkpISxcTE6IYbbvB1qwDaCGEGgFECAgL07LPPKj8/XxkZGbLZbLrmmmtks9mUkZGh/Px8PfPMMwoICPB1qwDaCOfMADBOenq6Vq9erezsbI0ZM8YaHzhwoFavXq309HQfdgegrTEzA8BYfn5+vm4BQDtAmAFgnDVr1igzM1MJCQkeN81LSEhQZmam1qxZ4+sWAbQhnpoNwChut1uxsbFKSEjQ2rVr5Xa7tW7dOk2cOFEBAQFKS0tTeXm59u3bx3kzgOF4ajaADqm4uFiHDh3S/Pnz5e/v+SvM399fOTk5OnjwoPVYAwAdH2EGgFGOHz8uSRo6dOhF1zePN9cB6PgIMwCM0nwzvPLy8ouubx7npnlA50GYAWCU5pvmLV68WI2NjR7rGhsblZuby03zgE6GMAPAKOffNC8tLU2lpaU6e/asSktLlZaWxk3zgE6ozcLMk08+KT8/P82aNcsaO3funLKystSjRw91795dGRkZqqys9HjdkSNHlJqaqpCQEEVGRmru3LlqaGhoq7YBtEPNN83bsWOHxowZozvvvFNjxoxReXk5N80DOqE2uQPw1q1b9Yc//EHDhg3zGJ89e7beeecdvf766woLC9OMGTOUnp6ujz76SNKXl2CmpqYqOjpamzdv1vHjx3X33XcrKChIixcvbovWAbRT6enpmjRpkjZs2KCCggJNmDBB48aNY0YG6IS8fp+ZM2fO6Oqrr9ZLL72kxx9/XMOHD9dzzz2n6upq9erVS6tWrVJmZqYkqaKiQkOGDFFJSYkSExNVUFCgW2+9VceOHVNUVJQkKS8vTw899JBOnDih4ODgi26zrq5OdXV11rLT6VS/fv30+eefc58ZoINxuVwqLCzU+PHjFRQU5Ot2ALQip9Opnj17fuN9Zrw+M5OVlaXU1FQlJSXp8ccft8bLysrkcrmUlJRkjcXFxal///5WmCkpKVFCQoIVZCQpJSVF06dP186dOzVixIiLbjM3N1cLFy68YHz9+vUKCQlpxU8HwJfcbrd27dqlU6dOaceOHYqPj2dmBuhAamtrW1Tn1TDz2muv6ZNPPtHWrVsvWOdwOBQcHKzw8HCP8aioKDkcDqvm/CDTvL553aXk5OQoOzvbWm6emUlOTmZmBugg3nzzTT300EM6dOiQNTZw4EA99dRT+ulPf+q7xgC0GqfT2aI6r4WZTz/9VL/61a9UWFioLl26eGszF2Wz2WSz2S4YDwoKYhoa6ADWrFmjyZMna8KECRoxYoT279+v2NhY1dXVafLkyZwEDHQQLf3O9lqYKSsrU1VVla6++mprzO12a9OmTfr973+v9957T/X19Tp9+rTH7ExlZaWio6MlSdHR0dqyZYvH+zZf7dRcA6BzcbvdmjNnjiIjI7Vu3TprfMeOHZK+nL198MEHNWnSJA45AZ2E1y7Nvvnmm7Vjxw5t377d+jNq1Cjddddd1s9BQUEqKiqyXrNnzx4dOXJEdrtdkmS327Vjxw5VVVVZNYWFhQoNDVV8fLy3WgfQjjU/m6myslLBwcGaN2+eli9frnnz5ik4OFiVlZU8mwnoZLw2M3PZZZdd8OyUbt26qUePHtb4tGnTlJ2drYiICIWGhmrmzJmy2+1KTEyUJCUnJys+Pl5TpkzRkiVL5HA49MgjjygrK+uih5EAdHyHDx+WJAUHB+v06dP66KOPrEuzH3vsMYWHh6u+vt6qA9Dx+fQOwMuWLdOtt96qjIwMjRkzRtHR0VqzZo21PiAgQPn5+QoICJDdbtfPfvYz3X333Vq0aJEPuwbgS2vXrpX05cxtXFycxo8fr6VLl2r8+PGKi4vT6NGjPeoAdHxev89Me+B0OhUWFvaN16kDaP+Sk5NVWFgoSfLz89P5v8LOXx4/frzWr1/vkx4BtI6Wfn/zbCYARrnyyiutn4OCgjR37lwtX75cc+fO9bjy4fw6AB1bmzzOAABayy233KK8vDxJUo8ePfT0009b6/r06aNjx45ZdQA6B2ZmABhl9erV1s/Hjx/3WNccZL5aB6BjI8wAMMqZM2datQ6A+QgzAIzSfOsGf39/XXHFFR7rrrjiCvn7+3vUAej4OGcGgFGa7+rb2Nioo0ePeqw7f5m7/wKdBzMzAIxy8ODBVq0DYD7CDACjtPTWWJ3gFloA/h/CDACjuN3uVq0DYD7uAAzAKAEBAWpsbPzGOn9/fwINYDjuAAygQzo/yPj5+XmsO3+5JYEHQMfA1UwAjHXLLbfoyiuv1N69e3XVVVfpwIEDKigo8HVbANoYh5kAGMVut6u0tFTSl89mcrlc1rrzlxMTE1VSUuKTHgG0jpZ+fzMzA6DN1dbWqqKi4ju9Ni4uzgoz5weZry7HxcXpk08++U7vHxIS8p16A+AbzMwAaHOffPKJRo4c6es2LqqsrExXX321r9sAIGZmALRjcXFxKisr+06vdbvdSklJ0alTpy5ZExERoXffffc73QU4Li7uO/UFwHcIMwDaXEhIyPea/fjTn/6kzMxM2Ww2nTt3zhrv2rWrzp07pz/+8Y+65pprWqNVAAbg0mwAxklPT9fq1asVHR3tMR4dHa3Vq1crPT3dR50B8AXOmQFgLLfbrb+8nq+cVR8p99+u07233coDJoEOhHNmAHR4AQEBGmW/Xt22B2qUPZEgA3RSHGYCAABGI8wAAACjEWYAAIDRCDMAAMBohBkAAGA0wgwAADAaYQYAABiNMAMAAIxGmAEAAEYjzAAAAKMRZgAAgNEIMwAAwGheDTPLly/XsGHDFBoaqtDQUNntdhUUFFjrz507p6ysLPXo0UPdu3dXRkaGKisrPd7jyJEjSk1NVUhIiCIjIzV37lw1NDR4s20AAGAQr4aZvn376sknn1RZWZm2bdumm266SZMmTdLOnTslSbNnz9bbb7+t119/XRs3btSxY8eUnp5uvd7tdis1NVX19fXavHmzVq5cqRUrVmjBggXebBsAABjEr6mpqaktNxgREaGnn35amZmZ6tWrl1atWqXMzExJUkVFhYYMGaKSkhIlJiaqoKBAt956q44dO6aoqChJUl5enh566CGdOHFCwcHBLdqm0+lUWFiYqqurFRoa6rXPBqDtbT/8L6UtL9Xa6YkaPqCHr9sB0Ipa+v0d2FYNud1uvf7666qpqZHdbldZWZlcLpeSkpKsmri4OPXv398KMyUlJUpISLCCjCSlpKRo+vTp2rlzp0aMGHHRbdXV1amurs5adjqdkiSXyyWXy+WlTwjAF5oPOzc0NLB/Ax1MS/dpr4eZHTt2yG6369y5c+revbvefPNNxcfHa/v27QoODlZ4eLhHfVRUlBwOhyTJ4XB4BJnm9c3rLiU3N1cLFy68YHz9+vUKCQn5np8IQHvy6RlJClRpaamOlvu6GwCtqba2tkV1Xg8zgwcP1vbt21VdXa3Vq1dr6tSp2rhxo1e3mZOTo+zsbGvZ6XSqX79+Sk5O5jAT0MH848hJacc2JSYm6kf9I3zdDoBW1Hxk5Zt4PcwEBwcrNjZWkjRy5Eht3bpVzz//vO644w7V19fr9OnTHrMzlZWVio6OliRFR0dry5YtHu/XfLVTc83F2Gw22Wy2C8aDgoIUFBT0fT8SgHYkMDDQ+pv9G+hYWrpPt/l9ZhobG1VXV6eRI0cqKChIRUVF1ro9e/boyJEjstvtkiS73a4dO3aoqqrKqiksLFRoaKji4+PbunUAANAOeXVmJicnRxMmTFD//v31xRdfaNWqVfrggw/03nvvKSwsTNOmTVN2drYiIiIUGhqqmTNnym63KzExUZKUnJys+Ph4TZkyRUuWLJHD4dAjjzyirKysi868AACAzserYaaqqkp33323jh8/rrCwMA0bNkzvvfeexo8fL0latmyZ/P39lZGRobq6OqWkpOill16yXh8QEKD8/HxNnz5ddrtd3bp109SpU7Vo0SJvtg0AAAzS5veZ8QXuMwN0XNxnBui4Wvr9zbOZAACA0QgzAADAaIQZAABgNMIMAAAwGmEGAAAYjTADAACMRpgBAABGI8wAAACjEWYAAIDRCDMAAMBohBkAAGA0wgwAADAaYQYAABiNMAMAAIxGmAEAAEYjzAAAAKMRZgAAgNEIMwAAwGiEGQAAYDTCDAAAMBphBgAAGI0wAwAAjEaYAQAARiPMAAAAoxFmAACA0QgzAADAaIQZAABgNMIMAAAwGmEGAAAYjTADAACMRpgBAABGI8wAAACjEWYAAIDRvBpmcnNzdc011+iyyy5TZGSk0tLStGfPHo+ac+fOKSsrSz169FD37t2VkZGhyspKj5ojR44oNTVVISEhioyM1Ny5c9XQ0ODN1gEAgCG8GmY2btyorKwslZaWqrCwUC6XS8nJyaqpqbFqZs+erbfffluvv/66Nm7cqGPHjik9Pd1a73a7lZqaqvr6em3evFkrV67UihUrtGDBAm+2DgAADOHX1NTU1FYbO3HihCIjI7Vx40aNGTNG1dXV6tWrl1atWqXMzExJUkVFhYYMGaKSkhIlJiaqoKBAt956q44dO6aoqChJUl5enh566CGdOHFCwcHB37hdp9OpsLAwVVdXKzQ01KufEUDb2n74X0pbXqq10xM1fEAPX7cDoBW19Ps7sA17UnV1tSQpIiJCklRWViaXy6WkpCSrJi4uTv3797fCTElJiRISEqwgI0kpKSmaPn26du7cqREjRlywnbq6OtXV1VnLTqdTkuRyueRyubzy2QD4RvMh54aGBvZvoINp6T7dZmGmsbFRs2bN0nXXXaehQ4dKkhwOh4KDgxUeHu5RGxUVJYfDYdWcH2Sa1zevu5jc3FwtXLjwgvH169crJCTk+34UAO3Ip2ckKVClpaU6Wu7rbgC0ptra2hbVtVmYycrKUnl5uT788EOvbysnJ0fZ2dnWstPpVL9+/ZScnMxhJqCD+ceRk9KObUpMTNSP+kf4uh0Araj5yMo3aZMwM2PGDOXn52vTpk3q27evNR4dHa36+nqdPn3aY3amsrJS0dHRVs2WLVs83q/5aqfmmq+y2Wyy2WwXjAcFBSkoKOj7fhwA7UhgYKD1N/s30LG0dJ/26tVMTU1NmjFjht588029//77iomJ8Vg/cuRIBQUFqaioyBrbs2ePjhw5IrvdLkmy2+3asWOHqqqqrJrCwkKFhoYqPj7em+0DAAADeHVmJisrS6tWrdJbb72lyy67zDrHJSwsTF27dlVYWJimTZum7OxsRUREKDQ0VDNnzpTdbldiYqIkKTk5WfHx8ZoyZYqWLFkih8OhRx55RFlZWRedfQEAAJ2LV8PM8uXLJUljx471GP/LX/6ie+65R5K0bNky+fv7KyMjQ3V1dUpJSdFLL71k1QYEBCg/P1/Tp0+X3W5Xt27dNHXqVC1atMibrQMAAEN4Ncy05BY2Xbp00YsvvqgXX3zxkjUDBgzQunXrWrM1AADQQfBsJgAAYDTCDAAAMBphBgAAGI0wAwAAjEaYAQAARiPMAAAAo7XpU7MBmO/g5zWqqWvwdRuWAydqrL+bH23QHnSzBSqmZzdftwF0Cu1nzwfQ7h38vEbjnvnA121c1JzVO3zdwgU2PDiWQAO0AcIMgBZrnpF57o7hio3s7uNuvlRztk75H5To1rF2devaPh5xsr/qjGb99/Z2NYMFdGSEGQDfWmxkdw29IszXbUiSXC6XHL2kqwdczlOzgU6KE4ABAIDRCDMAAMBohBkAAGA0wgwAADAaYQYAABiNMAMAAIxGmAEAAEYjzAAAAKMRZgAAgNEIMwAAwGiEGQAAYDTCDAAAMBphBgAAGI0wAwAAjEaYAQAARiPMAAAAoxFmAACA0QgzAADAaIQZAABgNMIMAAAwGmEGAAAYjTADAACMRpgBAABGC/Tmm2/atElPP/20ysrKdPz4cb355ptKS0uz1jc1NenRRx/VH//4R50+fVrXXXedli9frkGDBlk1J0+e1MyZM/X222/L399fGRkZev7559W9e3dvtg7gIurc5+Tf5agOOvfIv0v72AcbGhp0rOGYdp/crcBAr/5Ka7GDzjPy73JUde5zksJ83Q7Q4Xl1z6+pqdGPfvQj/fznP1d6evoF65csWaIXXnhBK1euVExMjH7zm98oJSVFu3btUpcuXSRJd911l44fP67CwkK5XC7de++9uv/++7Vq1Spvtg7gIo7VHFa3mN9p/hZfd3Khl959ydcteOgWIx2rGa6RivJ1K0CH59fU1NTUJhvy8/OYmWlqalKfPn00Z84cPfjgg5Kk6upqRUVFacWKFZo8ebJ2796t+Ph4bd26VaNGjZIkvfvuu5o4caI+++wz9enTp0XbdjqdCgsLU3V1tUJDQ73y+YDOoOxIpW7781t6/o7hujKy/czMfPThR7ru+uvazczMgaoz+tV/b9frP5+kkf0JM8B31dLvb5/t+QcPHpTD4VBSUpI1FhYWptGjR6ukpESTJ09WSUmJwsPDrSAjSUlJSfL399fHH3+sn/70pxd977q6OtXV1VnLTqdTkuRyueRyubz0iYCOL6ApUI3nrlC/kCs1qJ38x8Dlculg4EHFXharoKAgX7cjSao/41TjuRMKaArkdw7wPbR0//FZmHE4HJKkqCjP/7VERUVZ6xwOhyIjIz3WBwYGKiIiwqq5mNzcXC1cuPCC8fXr1yskJOT7tg50Wp+ekaRAffjhhzrcPiZmLIWFhb5uwdKe/50Ak9TW1raorn3MybaynJwcZWdnW8tOp1P9+vVTcnIyh5mA72HnMaee2VGq66+/Xj/s0z72JZfLpcLCQo0fP77dzMy0x38nwETNR1a+ic/CTHR0tCSpsrJSvXv3tsYrKys1fPhwq6aqqsrjdQ0NDTp58qT1+oux2Wyy2WwXjAcFBbWbX3aAiZrPSQkMDGx3+1J72r/b878TYJKW7j8+u89MTEyMoqOjVVRUZI05nU59/PHHstvtkiS73a7Tp0+rrKzMqnn//ffV2Nio0aNHt3nPAACg/fHqzMyZM2e0f/9+a/ngwYPavn27IiIi1L9/f82aNUuPP/64Bg0aZF2a3adPH+uKpyFDhuiWW27Rfffdp7y8PLlcLs2YMUOTJ09u8ZVMAACgY/NqmNm2bZvGjRtnLTefxzJ16lStWLFC8+bNU01Nje6//36dPn1a119/vd59913rHjOS9Morr2jGjBm6+eabrZvmvfDCC95sGwAAGMSrYWbs2LH6utvY+Pn5adGiRVq0aNElayIiIrhBHgAAuCSezQQAAIxGmAEAAEYjzAAAAKMRZgAAgNEIMwAAwGiEGQAAYDTCDAAAMBphBgAAGI0wAwAAjEaYAQAARiPMAAAAoxFmAACA0QgzAADAaIQZAABgNMIMAAAwGmEGAAAYLdDXDQAwx1mXW5JUfrTax538/2rO1mnbCSn68Cl162rzdTuSpP1VZ3zdAtCpEGYAtNiB//cl/fCaHT7u5KsC9f/t3+rrJi7QzcavWKAtsKcBaLHkH0ZLkq6M7K6uQQE+7uZLe45Xa87qHXo2M0GDe4f5uh1LN1ugYnp283UbQKdAmAHQYhHdgjX5x/193YaHhoYGSdKVvbpp6BXtJ8wAaDucAAwAAIxGmAEAAEYjzAAAAKMRZgAAgNEIMwAAwGiEGQAAYDTCDAAAMBphBgAAGI0wAwAAjEaYAQAARiPMAAAAoxFmAACA0QgzAADAaMaEmRdffFEDBw5Uly5dNHr0aG3ZssXXLQEAgHbAiDDz3//938rOztajjz6qTz75RD/60Y+UkpKiqqoqX7cGAAB8zIgws3TpUt1333269957FR8fr7y8PIWEhOjPf/6zr1sDAAA+FujrBr5JfX29ysrKlJOTY435+/srKSlJJSUlF31NXV2d6urqrGWn0ylJcrlccrlc3m0YQJtqaGiw/mb/BjqWlu7T7T7MfP7553K73YqKivIYj4qKUkVFxUVfk5ubq4ULF14wvn79eoWEhHilTwC+8ekZSQpUaWmpjpb7uhsAram2trZFde0+zHwXOTk5ys7OtpadTqf69eun5ORkhYaG+rAzAK3tH0dOSju2KTExUT/qH+HrdgC0ouYjK9+k3YeZnj17KiAgQJWVlR7jlZWVio6OvuhrbDabbDbbBeNBQUEKCgrySp8AfCMwMND6m/0b6Fhauk+3+xOAg4ODNXLkSBUVFVljjY2NKioqkt1u92FnAACgPWj3MzOSlJ2dralTp2rUqFH68Y9/rOeee041NTW69957fd0aAADwMSPCzB133KETJ05owYIFcjgcGj58uN59990LTgoGAACdjxFhRpJmzJihGTNm+LoNAADQzrT7c2YAAAC+DmEGAAAYjTADAACMRpgBAABGI8wAAACjEWYAAIDRCDMAAMBohBkAAGA0wgwAADAaYQYAABiNMAMAAIxGmAEAAEYjzAAAAKMRZgAAgNEIMwAAwGiEGQAAYDTCDAAAMBphBgAAGI0wAwAAjEaYAQAARiPMAAAAoxFmAACA0QgzAADAaIQZAABgNMIMAAAwGmEGAAAYjTADAACMRpgBAABGI8wAAACjEWYAAIDRCDMAAMBohBkAAGA0wgwAADCa18LME088oWuvvVYhISEKDw+/aM2RI0eUmpqqkJAQRUZGau7cuWpoaPCo+eCDD3T11VfLZrMpNjZWK1as8FbLAADAQF4LM/X19brttts0ffr0i653u91KTU1VfX29Nm/erJUrV2rFihVasGCBVXPw4EGlpqZq3Lhx2r59u2bNmqV///d/13vvveettgEAgGECvfXGCxculKRLzqSsX79eu3bt0t///ndFRUVp+PDh+u1vf6uHHnpIjz32mIKDg5WXl6eYmBg9++yzkqQhQ4boww8/1LJly5SSknLJbdfV1amurs5adjqdkiSXyyWXy9VKnxBAe9A8m9vQ0MD+DXQwLd2nvRZmvklJSYkSEhIUFRVljaWkpGj69OnauXOnRowYoZKSEiUlJXm8LiUlRbNmzfra987NzbXC1PnWr1+vkJCQVukfQPvw6RlJClRpaamOlvu6GwCtqba2tkV1PgszDofDI8hIspYdDsfX1jidTp09e1Zdu3a96Hvn5OQoOzvbWnY6nerXr5+Sk5MVGhramh8DgI/948hJacc2JSYm6kf9I3zdDoBW1Hxk5Zt8qzDz8MMP66mnnvramt27dysuLu7bvG2rs9lsstlsF4wHBQUpKCjIBx0B8JbAwEDrb/ZvoGNp6T79rcLMnDlzdM8993xtzQ9+8IMWvVd0dLS2bNniMVZZWWmta/67eez8mtDQ0EvOygAAgM7lW4WZXr16qVevXq2yYbvdrieeeEJVVVWKjIyUJBUWFio0NFTx8fFWzbp16zxeV1hYKLvd3io9AAAA83nt0uwjR45o+/btOnLkiNxut7Zv367t27frzJkzkqTk5GTFx8drypQp+sc//qH33ntPjzzyiLKysqxDRL/4xS/0z3/+U/PmzVNFRYVeeukl/e1vf9Ps2bO91TYAADCM104AXrBggVauXGktjxgxQpK0YcMGjR07VgEBAcrPz9f06dNlt9vVrVs3TZ06VYsWLbJeExMTo3feeUezZ8/W888/r759++pPf/rT116WDQAAOhe/pqamJl834W1Op1NhYWGqrq7maiagg9l++F9KW16qtdMTNXxAD1+3A6AVtfT7m2czAQAAoxFmAACA0QgzAADAaIQZAABgNJ89zgAAvq/9+/frx/Hxcrlc+vHSIO3atUuxsbG+bgtAGyPMADCSv7+/zr8Y0+VyadCgQfLz81NjY6MPOwPQ1jjMBMA4Xw0y52tqapK/P7/agM6EPR6AUfbv33/JINOsqalJ+/fvb6OOAPgah5kAtLna2lpVVFR8p9eOGjWqRXVXXXWVtm3b9q3fPy4uTiEhId/6dQB8hzADoM1VVFRo5MiRXt1GU1PTd9pGWVmZrr76ai90BMBbCDMA2lxcXJzKysq+02u/TUD5LtuIi4v71q8B4FuEGQBtLiQkpE1mP5hhAToHTgAGAABGI8wAAACjEWYAAIDRCDMAjNLSG+Jx4zyg82BvB2CU4ODgVq0DYD7CDACjfNPdf79tHQDzEWYAGKWurq5V6wCYjzADAACMRpgBYJSwsLBWrQNgPsIMAKM8//zzrVoHwHyEGQBG+etf/9qqdQDMR5gBYJS9e/e2ah0A8xFmABglNDS0VesAmI8wA8AoMTExrVoHwHyEGQBGiY6ObtU6AOYjzAAwyu7du1u1DoD5CDMAjHL27NlWrQNgvkBfNwAA38aZM2esn3v16qUxY8bo1KlTuvzyy7Vp0yadOHHigjoAHRthBoBRevbsqX379snPz0+ff/653njjDWudn5+f/Pz81NTUpJ49e/qwSwBticNMAIzygx/8QNKXT8X+6pOxzx9rrgPQ8XktzBw6dEjTpk1TTEyMunbtqiuvvFKPPvqo6uvrPer+z//5P7rhhhvUpUsX9evXT0uWLLngvV5//XXFxcWpS5cuSkhI0Lp167zVNoB27u67727VOgDm81qYqaioUGNjo/7whz9o586dWrZsmfLy8jR//nyrxul0Kjk5WQMGDFBZWZmefvppPfbYY3r55Zetms2bN+vOO+/UtGnT9L//+79KS0tTWlqaysvLvdU6gHZs7Nix8vPz+9oaPz8/jR07tm0aAuBzfk1fnaf1oqefflrLly/XP//5T0nS8uXL9etf/1oOh0PBwcGSpIcfflhr165VRUWFJOmOO+5QTU2N8vPzrfdJTEzU8OHDlZeX16LtOp1OhYWFqbq6mruCAoYrKipSUlLSN9b9/e9/180339wGHQHwlpZ+f7fpCcDV1dWKiIiwlktKSjRmzBgryEhSSkqKnnrqKevqhJKSEmVnZ3u8T0pKitauXXvJ7dTV1amurs5adjqdkiSXyyWXy9VKnwaALxQVFVk/N5/se7HloqIijRkzps37A9B6Wvqd3WZhZv/+/frd736nZ555xhpzOBwX3HI8KirKWnf55ZfL4XBYY+fXOByOS24rNzdXCxcuvGB8/fr1CgkJ+T4fA4CP7dmzR5LUrVs3/fnPf9bevXut//xcddVV+vnPf66amhrt2bOH8+sAw9XW1rao7luHmYcfflhPPfXU19bs3r1bcXFx1vLRo0d1yy236LbbbtN99933bTf5reXk5HjM5jidTvXr10/JyckcZgIM1xxQoqOjNWnSJLndbhUWFmr8+PEKCAhQdHS0Dhw4oJ49e2rixIk+7hbA99F8ZOWbfOswM2fOHN1zzz1fW3P+JZHHjh3TuHHjdO2113qc2Ct9+cuosrLSY6x5ufm5Kpeq+brnrthsNtlstgvGg4KCFBQU9LW9A2jfAgO//LV14MABZWZmat68eTp79qzKysq0ZMkSHThwwKpjfwfM1tJ9+FuHmV69eqlXr14tqj169KjGjRunkSNH6i9/+Yv8/T0vnrLb7fr1r38tl8tlNVxYWKjBgwfr8ssvt2qKioo0a9Ys63WFhYWy2+3ftnUAHcCgQYOsn4uKijwuDjj/MPL5dQA6Nq9dzXT06FGNHTtWAwYM0MqVKxUQEGCta55Vqa6u1uDBg5WcnKyHHnpI5eXl+vnPf65ly5bp/vvvl/Tlpdk33nijnnzySaWmpuq1117T4sWL9cknn2jo0KEt6oWrmYCOo76+Xt26dVO3bt0UHh6uw4cPW+sGDhyoU6dOqaamRjU1NR4XFwAwj8+vZiosLNT+/fu1f/9+9e3b12Ndc34KCwvT+vXrlZWVpZEjR6pnz55asGCBFWQk6dprr9WqVav0yCOPaP78+Ro0aJDWrl3b4iADoGMJDg7W7Nmz9fTTT8tms2nWrFmqra1VSEiIVq1aperqas2dO5cgA3QibXqfGV9hZgboeObNm6dly5apoaHBGgsMDNTs2bMveidxAOZp6fc3YQaAserr6/W73/1O77//vm666SbNnDmTGRmgA/H5YSYA8Lbg4GA98MADio2N1cSJE7l6CeikeGo2AAAwGmEGAAAYjTADAACMRpgBAABGI8wAAACjEWYAAIDRCDMAAMBohBkAAGA0wgwAADBap7gDcPMTG5xOp487AdDaXC6Xamtr5XQ6uQMw0ME0f29/05OXOkWY+eKLLyRJ/fr183EnAADg2/riiy8UFhZ2yfWd4kGTjY2NOnbsmC677DL5+fn5uh0ArcjpdKpfv3769NNPeZAs0ME0NTXpiy++UJ8+feTvf+kzYzpFmAHQcbX0qboAOi5OAAYAAEYjzAAAAKMRZgAYzWaz6dFHH5XNZvN1KwB8hHNmAACA0ZiZAQAARiPMAAAAoxFmAACA0QgzAADAaIQZAABgNMIMgA6tqalJL7/8skaPHq3u3bsrPDxco0aN0nPPPafa2lpftwegFRBmAHRoU6ZM0axZszRp0iRt2LBB27dv129+8xu99dZbWr9+va/bA9AKCDMA2oWxY8dq5syZmjVrli6//HJFRUXpj3/8o2pqanTvvffqsssuU2xsrAoKCiRJH3zwgfz8/PTOO+9o2LBh6tKlixITE1VeXm6959/+9je98sorevXVVzV//nxdc801GjhwoCZNmqT3339f48aN89XHBdCKCDMA2o2VK1eqZ8+e2rJli2bOnKnp06frtttu07XXXqtPPvlEycnJmjJlisfhoblz5+rZZ5/V1q1b1atXL/3kJz+Ry+WSJL3yyisaPHiwJk2adMG2/Pz8FBYW1mafDYD3cAdgAO3C2LFj5Xa7VVxcLElyu90KCwtTenq6/vrXv0qSHA6HevfurZKSEp07d07jxo3Ta6+9pjvuuEOSdPLkSfXt21crVqzQ7bffrvj4eA0aNEhvvfWWzz4XAO9jZgZAuzFs2DDr54CAAPXo0UMJCQnWWFRUlCSpqqrKGrPb7dbPERERGjx4sHbv3i3py5N/AXR8hBkA7UZQUJDHsp+fn8eYn5+fJKmxsbFF73fVVVepoqKi9RoE0C4RZgAYrbS01Pr51KlT2rt3r4YMGSJJ+rd/+zft3bv3ooeZmpqaVF1d3WZ9AvAewgwAoy1atEhFRUUqLy/XPffco549eyotLU2SdPvtt+uOO+7QnXfeqcWLF2vbtm06fPiw8vPzlZSUpA0bNvi2eQCtItDXDQDA9/Hkk0/qV7/6lfbt26fhw4fr7bffVnBwsKQvD0utWrVKL7/8sv785z/riSeeUGBgoAYNGqS7775bKSkpPu4eQGvgaiYARvrggw80btw4nTp1SuHh4b5uB4APcZgJAAAYjTADAACMxmEmAABgNGZmAACA0QgzAADAaIQZAABgNMIMAAAwGmEGAAAYjTADAACMRpgBAABGI8wAAACj/V8vtJXM5BQS9QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train[['mpC']].boxplot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3e85ea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "correlation_matrix = df_train.corr()\n",
    "column_names = correlation_matrix.columns\n",
    "sns.heatmap(correlation_matrix, xticklabels=column_names, yticklabels=column_names,cmap= \"bwr\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a11f724",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>smiles</th>\n",
       "      <th>mpC</th>\n",
       "      <th>Number of H</th>\n",
       "      <th>Number of B</th>\n",
       "      <th>Number of C</th>\n",
       "      <th>Number of N</th>\n",
       "      <th>Number of O</th>\n",
       "      <th>Number of F</th>\n",
       "      <th>Number of Si</th>\n",
       "      <th>Number of P</th>\n",
       "      <th>Number of S</th>\n",
       "      <th>Number of Cl</th>\n",
       "      <th>Number of Br</th>\n",
       "      <th>Number of I</th>\n",
       "      <th>C-O (1.5)</th>\n",
       "      <th>N-H (1.0)</th>\n",
       "      <th>P-O (2.0)</th>\n",
       "      <th>N-B (1.0)</th>\n",
       "      <th>O-Si (1.0)</th>\n",
       "      <th>Si-Br (1.0)</th>\n",
       "      <th>Si-F (1.0)</th>\n",
       "      <th>C-Cl (1.0)</th>\n",
       "      <th>C-H (1.0)</th>\n",
       "      <th>F-Si (1.0)</th>\n",
       "      <th>S-P (1.0)</th>\n",
       "      <th>N-N (1.5)</th>\n",
       "      <th>Si-N (1.0)</th>\n",
       "      <th>P-C (1.0)</th>\n",
       "      <th>O-N (1.0)</th>\n",
       "      <th>F-C (1.0)</th>\n",
       "      <th>C-P (1.0)</th>\n",
       "      <th>I-O (1.0)</th>\n",
       "      <th>N-Br (1.0)</th>\n",
       "      <th>H-O (1.0)</th>\n",
       "      <th>B-Br (1.0)</th>\n",
       "      <th>C-S (1.0)</th>\n",
       "      <th>P-S (2.0)</th>\n",
       "      <th>N-N (1.0)</th>\n",
       "      <th>N-N (3.0)</th>\n",
       "      <th>C-O (2.0)</th>\n",
       "      <th>C-B (1.0)</th>\n",
       "      <th>B-C (1.0)</th>\n",
       "      <th>Cl-O (1.0)</th>\n",
       "      <th>B-S (1.0)</th>\n",
       "      <th>Si-P (1.0)</th>\n",
       "      <th>Cl-C (1.0)</th>\n",
       "      <th>N-Cl (1.0)</th>\n",
       "      <th>P-F (1.0)</th>\n",
       "      <th>F-S (1.0)</th>\n",
       "      <th>C-C (2.0)</th>\n",
       "      <th>C-N (3.0)</th>\n",
       "      <th>Si-O (1.0)</th>\n",
       "      <th>S-N (1.0)</th>\n",
       "      <th>B-B (1.0)</th>\n",
       "      <th>S-C (2.0)</th>\n",
       "      <th>O-P (1.0)</th>\n",
       "      <th>C-N (1.0)</th>\n",
       "      <th>N-O (2.0)</th>\n",
       "      <th>C-C (3.0)</th>\n",
       "      <th>B-N (1.0)</th>\n",
       "      <th>Si-Cl (1.0)</th>\n",
       "      <th>C-O (3.0)</th>\n",
       "      <th>C-S (1.5)</th>\n",
       "      <th>N-O (1.5)</th>\n",
       "      <th>Cl-B (1.0)</th>\n",
       "      <th>P-N (1.0)</th>\n",
       "      <th>O-H (1.0)</th>\n",
       "      <th>S-Br (1.0)</th>\n",
       "      <th>Cl-P (1.0)</th>\n",
       "      <th>O-N (2.0)</th>\n",
       "      <th>N-P (2.0)</th>\n",
       "      <th>C-P (2.0)</th>\n",
       "      <th>O-C (1.5)</th>\n",
       "      <th>P-Si (1.0)</th>\n",
       "      <th>O-C (3.0)</th>\n",
       "      <th>N-C (2.0)</th>\n",
       "      <th>S-C (1.5)</th>\n",
       "      <th>Cl-N (1.0)</th>\n",
       "      <th>C-Si (1.0)</th>\n",
       "      <th>O-O (1.0)</th>\n",
       "      <th>Br-Si (1.0)</th>\n",
       "      <th>C-N (2.0)</th>\n",
       "      <th>S-N (1.5)</th>\n",
       "      <th>P-Cl (1.0)</th>\n",
       "      <th>H-N (1.0)</th>\n",
       "      <th>Si-C (1.0)</th>\n",
       "      <th>S-S (1.5)</th>\n",
       "      <th>O-C (1.0)</th>\n",
       "      <th>S-Cl (1.0)</th>\n",
       "      <th>S-C (1.0)</th>\n",
       "      <th>C-S (2.0)</th>\n",
       "      <th>O-P (2.0)</th>\n",
       "      <th>C-C (1.0)</th>\n",
       "      <th>C-Br (1.0)</th>\n",
       "      <th>Cl-Si (1.0)</th>\n",
       "      <th>N-Si (1.0)</th>\n",
       "      <th>S-P (2.0)</th>\n",
       "      <th>O-B (1.0)</th>\n",
       "      <th>Br-P (1.0)</th>\n",
       "      <th>P-N (2.0)</th>\n",
       "      <th>S-F (1.0)</th>\n",
       "      <th>F-P (1.0)</th>\n",
       "      <th>C-I (1.0)</th>\n",
       "      <th>C-C (1.5)</th>\n",
       "      <th>C-F (1.0)</th>\n",
       "      <th>S-S (1.0)</th>\n",
       "      <th>Br-Br (1.0)</th>\n",
       "      <th>S-O (1.0)</th>\n",
       "      <th>P-O (1.0)</th>\n",
       "      <th>O-S (1.0)</th>\n",
       "      <th>S-I (1.0)</th>\n",
       "      <th>Br-B (1.0)</th>\n",
       "      <th>S-O (2.0)</th>\n",
       "      <th>C-N (1.5)</th>\n",
       "      <th>N-S (1.5)</th>\n",
       "      <th>N-I (1.0)</th>\n",
       "      <th>I-C (1.0)</th>\n",
       "      <th>P-S (1.0)</th>\n",
       "      <th>O-S (2.0)</th>\n",
       "      <th>N-P (1.0)</th>\n",
       "      <th>Br-C (1.0)</th>\n",
       "      <th>B-Cl (1.0)</th>\n",
       "      <th>O-N (1.5)</th>\n",
       "      <th>Cl-S (1.0)</th>\n",
       "      <th>B-F (1.0)</th>\n",
       "      <th>O-I (1.0)</th>\n",
       "      <th>N-N (2.0)</th>\n",
       "      <th>P-Br (1.0)</th>\n",
       "      <th>Si-Si (1.0)</th>\n",
       "      <th>O-C (2.0)</th>\n",
       "      <th>N-C (1.5)</th>\n",
       "      <th>N-C (3.0)</th>\n",
       "      <th>N-O (1.0)</th>\n",
       "      <th>F-B (1.0)</th>\n",
       "      <th>C-O (1.0)</th>\n",
       "      <th>N-S (1.0)</th>\n",
       "      <th>O-Cl (1.0)</th>\n",
       "      <th>B-O (1.0)</th>\n",
       "      <th>N-C (1.0)</th>\n",
       "      <th>H-C (1.0)</th>\n",
       "      <th>I-I (1.0)</th>\n",
       "      <th>SH</th>\n",
       "      <th>SiH3</th>\n",
       "      <th>SiH2</th>\n",
       "      <th>Bh3</th>\n",
       "      <th>CH</th>\n",
       "      <th>SiH4</th>\n",
       "      <th>OH</th>\n",
       "      <th>NH</th>\n",
       "      <th>PH2</th>\n",
       "      <th>PH3</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CH2</th>\n",
       "      <th>CH3</th>\n",
       "      <th>PH</th>\n",
       "      <th>BH</th>\n",
       "      <th>NH2</th>\n",
       "      <th>SiH</th>\n",
       "      <th>Bh2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3-i-pr-5-mephenyl-n-me carbamate</td>\n",
       "      <td>O=C(Oc1cc(C)cc(c1)C(C)C)NC</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b-2-furylacrylic acid</td>\n",
       "      <td>O=C(O)C=Cc1occc1</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name                      smiles    mpC  \\\n",
       "0  3-i-pr-5-mephenyl-n-me carbamate  O=C(Oc1cc(C)cc(c1)C(C)C)NC   87.0   \n",
       "1             b-2-furylacrylic acid            O=C(O)C=Cc1occc1  133.0   \n",
       "\n",
       "   Number of H  Number of B  Number of C  Number of N  Number of O  \\\n",
       "0            0            0           12            1            2   \n",
       "1            0            0            7            0            3   \n",
       "\n",
       "   Number of F  Number of Si  Number of P  Number of S  Number of Cl  \\\n",
       "0            0             0            0            0             0   \n",
       "1            0             0            0            0             0   \n",
       "\n",
       "   Number of Br  Number of I  C-O (1.5)  N-H (1.0)  P-O (2.0)  N-B (1.0)  \\\n",
       "0             0            0          0          0          0          0   \n",
       "1             0            0          1          0          0          0   \n",
       "\n",
       "   O-Si (1.0)  Si-Br (1.0)  Si-F (1.0)  C-Cl (1.0)  C-H (1.0)  F-Si (1.0)  \\\n",
       "0           0            0           0           0          0           0   \n",
       "1           0            0           0           0          0           0   \n",
       "\n",
       "   S-P (1.0)  N-N (1.5)  Si-N (1.0)  P-C (1.0)  O-N (1.0)  F-C (1.0)  \\\n",
       "0          0          0           0          0          0          0   \n",
       "1          0          0           0          0          0          0   \n",
       "\n",
       "   C-P (1.0)  I-O (1.0)  N-Br (1.0)  H-O (1.0)  B-Br (1.0)  C-S (1.0)  \\\n",
       "0          0          0           0          0           0          0   \n",
       "1          0          0           0          0           0          0   \n",
       "\n",
       "   P-S (2.0)  N-N (1.0)  N-N (3.0)  C-O (2.0)  C-B (1.0)  B-C (1.0)  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "\n",
       "   Cl-O (1.0)  B-S (1.0)  Si-P (1.0)  Cl-C (1.0)  N-Cl (1.0)  P-F (1.0)  \\\n",
       "0           0          0           0           0           0          0   \n",
       "1           0          0           0           0           0          0   \n",
       "\n",
       "   F-S (1.0)  C-C (2.0)  C-N (3.0)  Si-O (1.0)  S-N (1.0)  B-B (1.0)  \\\n",
       "0          0          0          0           0          0          0   \n",
       "1          0          1          0           0          0          0   \n",
       "\n",
       "   S-C (2.0)  O-P (1.0)  C-N (1.0)  N-O (2.0)  C-C (3.0)  B-N (1.0)  \\\n",
       "0          0          0          1          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "\n",
       "   Si-Cl (1.0)  C-O (3.0)  C-S (1.5)  N-O (1.5)  Cl-B (1.0)  P-N (1.0)  \\\n",
       "0            0          0          0          0           0          0   \n",
       "1            0          0          0          0           0          0   \n",
       "\n",
       "   O-H (1.0)  S-Br (1.0)  Cl-P (1.0)  O-N (2.0)  N-P (2.0)  C-P (2.0)  \\\n",
       "0          0           0           0          0          0          0   \n",
       "1          0           0           0          0          0          0   \n",
       "\n",
       "   O-C (1.5)  P-Si (1.0)  O-C (3.0)  N-C (2.0)  S-C (1.5)  Cl-N (1.0)  \\\n",
       "0          0           0          0          0          0           0   \n",
       "1          1           0          0          0          0           0   \n",
       "\n",
       "   C-Si (1.0)  O-O (1.0)  Br-Si (1.0)  C-N (2.0)  S-N (1.5)  P-Cl (1.0)  \\\n",
       "0           0          0            0          0          0           0   \n",
       "1           0          0            0          0          0           0   \n",
       "\n",
       "   H-N (1.0)  Si-C (1.0)  S-S (1.5)  O-C (1.0)  S-Cl (1.0)  S-C (1.0)  \\\n",
       "0          0           0          0          1           0          0   \n",
       "1          0           0          0          0           0          0   \n",
       "\n",
       "   C-S (2.0)  O-P (2.0)  C-C (1.0)  C-Br (1.0)  Cl-Si (1.0)  N-Si (1.0)  \\\n",
       "0          0          0          4           0            0           0   \n",
       "1          0          0          2           0            0           0   \n",
       "\n",
       "   S-P (2.0)  O-B (1.0)  Br-P (1.0)  P-N (2.0)  S-F (1.0)  F-P (1.0)  \\\n",
       "0          0          0           0          0          0          0   \n",
       "1          0          0           0          0          0          0   \n",
       "\n",
       "   C-I (1.0)  C-C (1.5)  C-F (1.0)  S-S (1.0)  Br-Br (1.0)  S-O (1.0)  \\\n",
       "0          0          6          0          0            0          0   \n",
       "1          0          3          0          0            0          0   \n",
       "\n",
       "   P-O (1.0)  O-S (1.0)  S-I (1.0)  Br-B (1.0)  S-O (2.0)  C-N (1.5)  \\\n",
       "0          0          0          0           0          0          0   \n",
       "1          0          0          0           0          0          0   \n",
       "\n",
       "   N-S (1.5)  N-I (1.0)  I-C (1.0)  P-S (1.0)  O-S (2.0)  N-P (1.0)  \\\n",
       "0          0          0          0          0          0          0   \n",
       "1          0          0          0          0          0          0   \n",
       "\n",
       "   Br-C (1.0)  B-Cl (1.0)  O-N (1.5)  Cl-S (1.0)  B-F (1.0)  O-I (1.0)  \\\n",
       "0           0           0          0           0          0          0   \n",
       "1           0           0          0           0          0          0   \n",
       "\n",
       "   N-N (2.0)  P-Br (1.0)  Si-Si (1.0)  O-C (2.0)  N-C (1.5)  N-C (3.0)  \\\n",
       "0          0           0            0          1          0          0   \n",
       "1          0           0            0          1          0          0   \n",
       "\n",
       "   N-O (1.0)  F-B (1.0)  C-O (1.0)  N-S (1.0)  O-Cl (1.0)  B-O (1.0)  \\\n",
       "0          0          0          1          0           0          0   \n",
       "1          0          0          1          0           0          0   \n",
       "\n",
       "   N-C (1.0)  H-C (1.0)  I-I (1.0)  SH  SiH3  SiH2  Bh3  CH  SiH4  OH  NH  \\\n",
       "0          1          0          0   0     0     0    0   4     0   0   1   \n",
       "1          0          0          0   0     0     0    0   5     0   1   0   \n",
       "\n",
       "   PH2  PH3  NH3  CH2  CH3  PH  BH  NH2  SiH  Bh2  \n",
       "0    0    0    0    0    4   0   0    0    0    0  \n",
       "1    0    0    0    0    0   0   0    0    0    0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"display.max_column\", None)\n",
    "df_train.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c849c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create the features:\n",
    "X = df_train.drop(columns=[\"name\", \"smiles\", \"mpC\"])\n",
    "y = df_train[\"mpC\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b362fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne des scores RMSE : 46.21592311237597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Crez un modle de fort alatoire\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X, y)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')  # Mtrique de performance : Erreur quadratique moyenne ngative\n",
    "\n",
    "# Le score sera ngatif, donc prenez l'oppos pour obtenir l'erreur quadratique moyenne positive\n",
    "rmse_scores = (-scores) ** 0.5\n",
    "\n",
    "# Calculez la moyenne des scores RMSE\n",
    "mean_rmse_score = rmse_scores.mean()\n",
    "print(\"Moyenne des scores RMSE :\", mean_rmse_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8ae3368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs hyperparamtres trouvs :\n",
      "{'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 30}\n",
      "RMSE moyen correspondant :  45.86518053761778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Crer un modle de fort alatoire\n",
    "model = RandomForestRegressor(random_state=0)\n",
    "\n",
    "# Dfinir la grille des hyperparamtres  explorer\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "# Dfinir le nombre d'itrations de la recherche\n",
    "n_iter = 10  # Vous pouvez augmenter cela si ncessaire\n",
    "\n",
    "# Crer l'objet RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=n_iter, scoring='neg_mean_squared_error', cv=5, random_state=0)\n",
    "\n",
    "# Effectuer la recherche sur les hyperparamtres\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Afficher les meilleurs hyperparamtres et la performance correspondante\n",
    "print(\"Meilleurs hyperparamtres trouvs :\")\n",
    "print(random_search.best_params_)\n",
    "print(\"RMSE moyen correspondant : \", np.sqrt(-random_search.best_score_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "899645f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne des scores RMSE : 45.49644252506192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Crez un modle de rseau de neurones\n",
    "model = MLPRegressor(hidden_layer_sizes=(100, 50), max_iter=1000)\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X, y)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')  # Mtrique de performance : Erreur quadratique moyenne ngative\n",
    "\n",
    "# Le score sera ngatif, donc prenez l'oppos pour obtenir l'erreur quadratique moyenne positive\n",
    "rmse_scores = (-scores) ** 0.5\n",
    "\n",
    "# Calculez la moyenne des scores RMSE\n",
    "mean_rmse_score = rmse_scores.mean()\n",
    "print(\"Moyenne des scores RMSE :\", mean_rmse_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32ad314a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaumelewagon/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/guillaumelewagon/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/guillaumelewagon/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/guillaumelewagon/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/guillaumelewagon/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/home/guillaumelewagon/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:709: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Crez un modle de rseau de neurones\n",
    "model = MLPRegressor(max_iter=1000)\n",
    "\n",
    "# Dfinissez les hyperparamtres que vous souhaitez rechercher\n",
    "param_dist = {\n",
    "    'alpha': np.logspace(-6, -2, 5),  # Recherche sur une chelle logarithmique\n",
    "    'activation': ['logistic', 'tanh', 'relu'],\n",
    "    'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "}\n",
    "\n",
    "# Crez un objet RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=10, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Entranez le modle avec recherche alatoire\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Affichez les meilleurs paramtres et la meilleure performance\n",
    "print(\"Meilleurs paramtres :\", random_search.best_params_)\n",
    "print(\"Meilleur score RMSE :\", (-random_search.best_score_) ** 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "480ea2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne des scores RMSE : 61.26565285475256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "# Crez un modle SVR\n",
    "model = SVR(kernel='linear', C=1.0)\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X, y)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')  # Mtrique de performance : Erreur quadratique moyenne ngative\n",
    "\n",
    "# Le score sera ngatif, donc prenez l'oppos pour obtenir l'erreur quadratique moyenne positive\n",
    "rmse_scores = (-scores) ** 0.5\n",
    "\n",
    "# Calculez la moyenne des scores RMSE\n",
    "mean_rmse_score = rmse_scores.mean()\n",
    "print(\"Moyenne des scores RMSE :\", mean_rmse_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b588c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "import time\n",
    "\n",
    "# Crez un modle SVR\n",
    "model = SVR(kernel='linear', C=1.0)\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X, y)\n",
    "\n",
    "# Dfinissez les hyperparamtres que vous souhaitez optimiser\n",
    "param_dist = {\n",
    "    'C': reciprocal(0.1, 100),  # Distribution rciproque pour C\n",
    "    'kernel': ['linear']  # Types de noyau possibles\n",
    "}\n",
    "\n",
    "# Crez un objet RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    SVR(),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Vous pouvez ajuster ce nombre en fonction de vos ressources\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Commencez  chronomtrer le temps\n",
    "start_time = time.time()\n",
    "\n",
    "# Excutez la recherche alatoire\n",
    "random_search.fit(X, y)\n",
    "\n",
    "# Fin du chronomtre\n",
    "end_time = time.time()\n",
    "\n",
    "# Accdez aux meilleurs hyperparamtres et  la meilleure performance\n",
    "print(\"Meilleurs hyperparamtres:\", random_search.best_params_)\n",
    "print(\"Meilleure erreur quadratique moyenne ngative:\", random_search.best_score_)\n",
    "\n",
    "# Affichez le temps d'excution\n",
    "print(f\"Temps d'excution : {end_time - start_time} secondes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69aa9ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moyenne des scores RMSE : 45.262975671179746\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Crez un modle XGBoost\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100)\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X, y)\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')  # Mtrique de performance : Erreur quadratique moyenne ngative\n",
    "\n",
    "# Le score sera ngatif, donc prenez l'oppos pour obtenir l'erreur quadratique moyenne positive\n",
    "rmse_scores = (-scores) ** 0.5\n",
    "\n",
    "# Calculez la moyenne des scores RMSE\n",
    "mean_rmse_score = rmse_scores.mean()\n",
    "print(\"Moyenne des scores RMSE :\", mean_rmse_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e37f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import expon, reciprocal\n",
    "import time\n",
    "\n",
    "# Crez un modle XGBoost avec des valeurs par dfaut\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Dfinissez uniquement les hyperparamtres que vous souhaitez optimiser\n",
    "param_dist = {\n",
    "    'max_depth': [3, 4, 5, 6, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# Crez un objet RandomizedSearchCV\n",
    "random_search_xgboost = RandomizedSearchCV(\n",
    "    xgb.XGBRegressor(objective='reg:squarederror'),\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=10,  # Vous pouvez ajuster ce nombre en fonction de vos ressources\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Commencez  chronomtrer le temps\n",
    "start_time = time.time()\n",
    "\n",
    "# Excutez la recherche alatoire\n",
    "random_search_xgboost.fit(X, y)\n",
    "\n",
    "# Fin du chronomtre\n",
    "end_time = time.time()\n",
    "\n",
    "# Accdez aux meilleurs hyperparamtres et  la meilleure performance\n",
    "print(\"Meilleurs hyperparamtres:\", random_search_xgboost.best_params_)\n",
    "print(\"Meilleure erreur quadratique moyenne ngative:\", random_search_xgboost.best_score_)\n",
    "\n",
    "# Affichez le temps d'excution\n",
    "print(f\"Temps d'excution : {end_time - start_time} secondes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "435ec4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "498/498 [==============================] - 4s 5ms/step - loss: 4791.9468 - val_loss: 3133.6565\n",
      "Epoch 2/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3047.4202 - val_loss: 2798.6958\n",
      "Epoch 3/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2744.8638 - val_loss: 2689.2917\n",
      "Epoch 4/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2586.6768 - val_loss: 2846.9819\n",
      "Epoch 5/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2599.1667 - val_loss: 2594.1631\n",
      "Epoch 6/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2655.5100 - val_loss: 2538.8201\n",
      "Epoch 7/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2456.5447 - val_loss: 2914.1653\n",
      "Epoch 8/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2396.7092 - val_loss: 2687.5312\n",
      "Epoch 9/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2313.6526 - val_loss: 2516.8430\n",
      "Epoch 10/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2302.5327 - val_loss: 2567.1350\n",
      "Epoch 11/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2271.4709 - val_loss: 2512.6938\n",
      "Epoch 12/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2219.1230 - val_loss: 2389.8291\n",
      "Epoch 13/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2179.2056 - val_loss: 2366.5835\n",
      "Epoch 14/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2190.8499 - val_loss: 2865.4932\n",
      "Epoch 15/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 2192.3269 - val_loss: 2570.2634\n",
      "Epoch 16/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2120.0620 - val_loss: 2746.8660\n",
      "Epoch 17/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2199.3599 - val_loss: 2368.9517\n",
      "Epoch 18/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2071.3867 - val_loss: 2501.4314\n",
      "Epoch 19/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2116.3247 - val_loss: 2276.9961\n",
      "Epoch 20/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2073.2722 - val_loss: 2305.5510\n",
      "Epoch 21/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2078.9768 - val_loss: 2482.1333\n",
      "Epoch 22/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2045.6774 - val_loss: 2283.1111\n",
      "Epoch 23/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2052.7566 - val_loss: 2481.7778\n",
      "Epoch 24/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2049.2891 - val_loss: 2304.6719\n",
      "Epoch 25/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2030.2531 - val_loss: 2754.9404\n",
      "Epoch 26/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2041.0219 - val_loss: 2288.4514\n",
      "Epoch 27/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1995.1453 - val_loss: 2621.0193\n",
      "Epoch 28/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2013.0171 - val_loss: 2424.4197\n",
      "Epoch 29/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2023.8767 - val_loss: 2627.2915\n",
      "Epoch 30/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1969.4797 - val_loss: 2359.7480\n",
      "Epoch 31/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1988.7555 - val_loss: 2598.7285\n",
      "Epoch 32/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1898.9819 - val_loss: 2292.0698\n",
      "Epoch 33/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1949.7657 - val_loss: 2270.9666\n",
      "Epoch 34/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1990.2722 - val_loss: 2344.1279\n",
      "Epoch 35/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1919.1949 - val_loss: 2385.7305\n",
      "Epoch 36/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1953.4520 - val_loss: 2359.6245\n",
      "Epoch 37/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1891.1511 - val_loss: 2293.5166\n",
      "Epoch 38/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1905.2911 - val_loss: 2248.7188\n",
      "Epoch 39/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1948.3301 - val_loss: 2313.2234\n",
      "Epoch 40/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1943.6145 - val_loss: 2500.6196\n",
      "Epoch 41/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1980.9954 - val_loss: 2274.5803\n",
      "Epoch 42/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1901.5162 - val_loss: 2253.9082\n",
      "Epoch 43/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1951.3572 - val_loss: 2578.1611\n",
      "Epoch 44/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1916.3696 - val_loss: 2327.4282\n",
      "Epoch 45/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1891.8120 - val_loss: 2642.4697\n",
      "Epoch 46/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1916.7173 - val_loss: 2348.1975\n",
      "Epoch 47/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1869.6757 - val_loss: 2305.2036\n",
      "Epoch 48/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1867.7883 - val_loss: 2360.5896\n",
      "Epoch 49/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1851.9154 - val_loss: 2374.9011\n",
      "Epoch 50/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1839.6609 - val_loss: 2295.7864\n",
      "Epoch 51/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1829.9503 - val_loss: 2316.1101\n",
      "Epoch 52/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1879.6127 - val_loss: 2255.2920\n",
      "Epoch 53/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1841.8885 - val_loss: 2329.0232\n",
      "Epoch 54/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1833.7026 - val_loss: 2282.3958\n",
      "Epoch 55/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1803.1537 - val_loss: 2433.7952\n",
      "Epoch 56/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1821.0581 - val_loss: 2412.7778\n",
      "Epoch 57/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1836.8534 - val_loss: 2653.0317\n",
      "Epoch 58/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1783.7234 - val_loss: 2333.5325\n",
      "Epoch 59/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1805.0341 - val_loss: 2548.6870\n",
      "Epoch 60/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1865.9972 - val_loss: 2509.6653\n",
      "Epoch 61/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1821.4308 - val_loss: 2374.2075\n",
      "Epoch 62/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1772.4049 - val_loss: 2608.1143\n",
      "Epoch 63/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1780.9521 - val_loss: 2530.8157\n",
      "Epoch 64/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1725.9291 - val_loss: 2380.8293\n",
      "Epoch 65/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1848.9988 - val_loss: 2255.1995\n",
      "Epoch 66/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1779.9833 - val_loss: 2342.0034\n",
      "Epoch 67/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1799.9828 - val_loss: 2242.3508\n",
      "Epoch 68/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1764.1122 - val_loss: 2327.9734\n",
      "Epoch 69/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1719.3217 - val_loss: 2402.7332\n",
      "Epoch 70/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1879.9116 - val_loss: 2837.4509\n",
      "Epoch 71/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1747.0959 - val_loss: 2653.7891\n",
      "Epoch 72/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1804.6151 - val_loss: 2366.8940\n",
      "Epoch 73/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1741.5245 - val_loss: 2473.1182\n",
      "Epoch 74/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1782.5253 - val_loss: 2465.4414\n",
      "Epoch 75/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1758.6754 - val_loss: 2313.0005\n",
      "Epoch 76/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1774.5262 - val_loss: 2410.9846\n",
      "Epoch 77/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1744.8632 - val_loss: 2420.4136\n",
      "Epoch 78/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1691.8856 - val_loss: 2479.9939\n",
      "Epoch 79/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1834.4108 - val_loss: 2510.5327\n",
      "Epoch 80/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1813.7629 - val_loss: 2368.7524\n",
      "Epoch 81/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1831.1539 - val_loss: 2409.3579\n",
      "Epoch 82/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1715.4608 - val_loss: 2284.4534\n",
      "Epoch 83/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1676.9972 - val_loss: 2486.2981\n",
      "Epoch 84/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1693.1700 - val_loss: 2374.4878\n",
      "Epoch 85/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1709.6359 - val_loss: 2669.6633\n",
      "Epoch 86/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1704.9163 - val_loss: 2596.1907\n",
      "Epoch 87/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1731.2877 - val_loss: 2377.2043\n",
      "Epoch 88/200\n",
      "498/498 [==============================] - 3s 6ms/step - loss: 1731.9855 - val_loss: 2352.8223\n",
      "Epoch 89/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1739.0398 - val_loss: 2333.2827\n",
      "Epoch 90/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1647.5112 - val_loss: 2621.8491\n",
      "Epoch 91/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1670.6414 - val_loss: 2419.3772\n",
      "Epoch 92/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1667.4998 - val_loss: 2441.9126\n",
      "Epoch 93/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1776.5917 - val_loss: 2747.7830\n",
      "Epoch 94/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1744.7373 - val_loss: 2454.7395\n",
      "Epoch 95/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1669.7812 - val_loss: 2409.4226\n",
      "Epoch 96/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1696.6599 - val_loss: 2406.1404\n",
      "Epoch 97/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1700.7968 - val_loss: 2315.9417\n",
      "Epoch 98/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1645.9553 - val_loss: 2337.4563\n",
      "Epoch 99/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1641.1305 - val_loss: 2425.4658\n",
      "Epoch 100/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1686.9919 - val_loss: 2340.4048\n",
      "Epoch 101/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1692.5853 - val_loss: 2687.4385\n",
      "Epoch 102/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1740.9858 - val_loss: 2452.8347\n",
      "Epoch 103/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1657.0250 - val_loss: 2354.8657\n",
      "Epoch 104/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1672.6246 - val_loss: 2439.9084\n",
      "Epoch 105/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1603.2738 - val_loss: 2351.8728\n",
      "Epoch 106/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1697.8302 - val_loss: 2359.1934\n",
      "Epoch 107/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1642.6210 - val_loss: 2437.5024\n",
      "Epoch 108/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1659.6211 - val_loss: 2393.3025\n",
      "Epoch 109/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1740.8585 - val_loss: 2395.4661\n",
      "Epoch 110/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1781.5505 - val_loss: 2470.5703\n",
      "Epoch 111/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1686.5790 - val_loss: 2379.6455\n",
      "Epoch 112/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1634.7290 - val_loss: 2427.9492\n",
      "Epoch 113/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1615.1395 - val_loss: 2304.2119\n",
      "Epoch 114/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1631.1404 - val_loss: 2373.8770\n",
      "Epoch 115/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1668.4001 - val_loss: 2445.2839\n",
      "Epoch 116/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1638.2416 - val_loss: 2391.0522\n",
      "Epoch 117/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1630.9053 - val_loss: 2490.0557\n",
      "Epoch 118/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1762.1721 - val_loss: 2336.0710\n",
      "Epoch 119/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1661.8065 - val_loss: 2398.5793\n",
      "Epoch 120/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1613.6641 - val_loss: 2582.0952\n",
      "Epoch 121/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1639.2358 - val_loss: 2481.5693\n",
      "Epoch 122/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1647.2776 - val_loss: 2492.3113\n",
      "Epoch 123/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1629.4214 - val_loss: 2339.0757\n",
      "Epoch 124/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1625.3605 - val_loss: 2449.0027\n",
      "Epoch 125/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1797.6061 - val_loss: 2648.1287\n",
      "Epoch 126/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1675.3168 - val_loss: 2551.6604\n",
      "Epoch 127/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1630.2014 - val_loss: 2438.3730\n",
      "Epoch 128/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1606.8160 - val_loss: 2374.0615\n",
      "Epoch 129/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1650.2366 - val_loss: 2409.1833\n",
      "Epoch 130/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1598.7935 - val_loss: 2428.8813\n",
      "Epoch 131/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1612.8464 - val_loss: 2396.1062\n",
      "Epoch 132/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1661.2792 - val_loss: 2508.6396\n",
      "Epoch 133/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1591.4587 - val_loss: 2473.4302\n",
      "Epoch 134/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1603.7642 - val_loss: 2508.4983\n",
      "Epoch 135/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1579.6636 - val_loss: 2335.4937\n",
      "Epoch 136/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1581.2645 - val_loss: 2504.9722\n",
      "Epoch 137/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1632.8043 - val_loss: 2375.6375\n",
      "Epoch 138/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1608.5638 - val_loss: 2731.5007\n",
      "Epoch 139/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1643.9424 - val_loss: 2358.5776\n",
      "Epoch 140/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1723.0737 - val_loss: 2599.6963\n",
      "Epoch 141/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1610.3129 - val_loss: 2475.4099\n",
      "Epoch 142/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1573.0836 - val_loss: 2446.5400\n",
      "Epoch 143/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1576.7559 - val_loss: 2399.6074\n",
      "Epoch 144/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1641.7841 - val_loss: 2362.2324\n",
      "Epoch 145/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1580.6427 - val_loss: 2412.8635\n",
      "Epoch 146/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1619.4740 - val_loss: 2396.0337\n",
      "Epoch 147/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1565.2812 - val_loss: 2336.4539\n",
      "Epoch 148/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1607.2708 - val_loss: 2441.2666\n",
      "Epoch 149/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1585.1660 - val_loss: 2342.4875\n",
      "Epoch 150/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1609.9929 - val_loss: 2501.2922\n",
      "Epoch 151/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1583.2126 - val_loss: 2416.6877\n",
      "Epoch 152/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1572.1339 - val_loss: 2529.8044\n",
      "Epoch 153/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1554.3923 - val_loss: 2414.0542\n",
      "Epoch 154/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1616.5035 - val_loss: 2465.8696\n",
      "Epoch 155/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1587.3055 - val_loss: 2403.1147\n",
      "Epoch 156/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1578.0505 - val_loss: 2406.0518\n",
      "Epoch 157/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1593.1257 - val_loss: 2624.9182\n",
      "Epoch 158/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1633.4083 - val_loss: 2485.1167\n",
      "Epoch 159/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1597.2250 - val_loss: 2498.5608\n",
      "Epoch 160/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1573.9386 - val_loss: 2822.0342\n",
      "Epoch 161/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1563.5273 - val_loss: 2409.2937\n",
      "Epoch 162/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1543.3331 - val_loss: 2454.3833\n",
      "Epoch 163/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1560.9796 - val_loss: 2507.1997\n",
      "Epoch 164/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1568.8518 - val_loss: 2434.7542\n",
      "Epoch 165/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1556.9329 - val_loss: 2514.7710\n",
      "Epoch 166/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1555.0424 - val_loss: 2366.1843\n",
      "Epoch 167/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1542.7764 - val_loss: 2433.0337\n",
      "Epoch 168/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1552.0038 - val_loss: 2454.4731\n",
      "Epoch 169/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1604.7291 - val_loss: 2433.3279\n",
      "Epoch 170/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1668.0706 - val_loss: 2424.5425\n",
      "Epoch 171/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1584.1342 - val_loss: 2613.7314\n",
      "Epoch 172/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1618.5801 - val_loss: 2334.9355\n",
      "Epoch 173/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1548.5419 - val_loss: 2465.1431\n",
      "Epoch 174/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1537.7705 - val_loss: 2366.0542\n",
      "Epoch 175/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1578.8856 - val_loss: 2410.6768\n",
      "Epoch 176/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1559.7903 - val_loss: 2464.3630\n",
      "Epoch 177/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1561.6998 - val_loss: 2577.4211\n",
      "Epoch 178/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1563.7039 - val_loss: 2477.2808\n",
      "Epoch 179/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1541.6090 - val_loss: 2395.8721\n",
      "Epoch 180/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1535.8296 - val_loss: 2344.1155\n",
      "Epoch 181/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1547.8477 - val_loss: 2516.0320\n",
      "Epoch 182/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1555.0649 - val_loss: 2433.6528\n",
      "Epoch 183/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1550.7123 - val_loss: 2360.4756\n",
      "Epoch 184/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1568.0984 - val_loss: 2993.1702\n",
      "Epoch 185/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1652.0731 - val_loss: 2420.2720\n",
      "Epoch 186/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1563.5084 - val_loss: 2573.5833\n",
      "Epoch 187/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1574.8518 - val_loss: 2468.2478\n",
      "Epoch 188/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1556.0667 - val_loss: 2877.3965\n",
      "Epoch 189/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1529.0511 - val_loss: 2409.5181\n",
      "Epoch 190/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1527.2029 - val_loss: 3121.7119\n",
      "Epoch 191/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1597.5393 - val_loss: 2371.8665\n",
      "Epoch 192/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1521.6957 - val_loss: 2361.4526\n",
      "Epoch 193/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1536.6591 - val_loss: 2720.4568\n",
      "Epoch 194/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1572.0543 - val_loss: 3693.8416\n",
      "Epoch 195/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1558.2966 - val_loss: 2535.7590\n",
      "Epoch 196/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1525.8206 - val_loss: 2451.3201\n",
      "Epoch 197/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1528.8822 - val_loss: 2503.4456\n",
      "Epoch 198/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1552.7466 - val_loss: 2436.0227\n",
      "Epoch 199/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1493.9720 - val_loss: 2631.3403\n",
      "Epoch 200/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1514.8108 - val_loss: 2402.5261\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2402.5261\n",
      "RMSE: 50.641709723032406\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Charger vos donnes dans un dataframe (assurez-vous que vos donnes sont correctement formates)\n",
    "# Supposons que le dataframe contienne des colonnes 'Feature1', 'Feature2', ..., 'FeatureN', et 'MeltingPoint'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Normalisez les caractristiques\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crez votre modle de rgression\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.04)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# valuez le modle\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "rmse = np.sqrt(loss)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe6e2b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "498/498 [==============================] - 3s 4ms/step - loss: 4546.6377 - val_loss: 2992.6421\n",
      "Epoch 2/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2799.2964 - val_loss: 2716.4629\n",
      "Epoch 3/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2584.0986 - val_loss: 2689.1304\n",
      "Epoch 4/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2462.7222 - val_loss: 2477.4036\n",
      "Epoch 5/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2307.8696 - val_loss: 2418.8560\n",
      "Epoch 6/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2248.7886 - val_loss: 2405.7095\n",
      "Epoch 7/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2178.7942 - val_loss: 2314.2649\n",
      "Epoch 8/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2136.5669 - val_loss: 2317.0186\n",
      "Epoch 9/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2095.4744 - val_loss: 2231.2273\n",
      "Epoch 10/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2038.2371 - val_loss: 2409.0312\n",
      "Epoch 11/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1980.5688 - val_loss: 2236.0022\n",
      "Epoch 12/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1946.6871 - val_loss: 2206.6809\n",
      "Epoch 13/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1930.6136 - val_loss: 2192.8208\n",
      "Epoch 14/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1895.5674 - val_loss: 2393.0115\n",
      "Epoch 15/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1864.4667 - val_loss: 2153.6118\n",
      "Epoch 16/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1828.1759 - val_loss: 2277.1377\n",
      "Epoch 17/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1818.7294 - val_loss: 2172.5098\n",
      "Epoch 18/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1781.1761 - val_loss: 2286.2410\n",
      "Epoch 19/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1765.7418 - val_loss: 2057.4617\n",
      "Epoch 20/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1739.5287 - val_loss: 2099.7834\n",
      "Epoch 21/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1696.1523 - val_loss: 2217.3159\n",
      "Epoch 22/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1679.5094 - val_loss: 2264.6252\n",
      "Epoch 23/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1657.7874 - val_loss: 2329.7737\n",
      "Epoch 24/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1648.5796 - val_loss: 2132.8184\n",
      "Epoch 25/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1632.6219 - val_loss: 2239.0166\n",
      "Epoch 26/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1641.8522 - val_loss: 2157.8655\n",
      "Epoch 27/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1597.6290 - val_loss: 2192.1213\n",
      "Epoch 28/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1583.4669 - val_loss: 2249.2834\n",
      "Epoch 29/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1569.9392 - val_loss: 2109.8220\n",
      "Epoch 30/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1561.7297 - val_loss: 2153.5325\n",
      "Epoch 31/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1523.0419 - val_loss: 2303.5315\n",
      "Epoch 32/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1528.9585 - val_loss: 2153.8516\n",
      "Epoch 33/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1497.2965 - val_loss: 2087.7686\n",
      "Epoch 34/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1480.0168 - val_loss: 2100.3533\n",
      "Epoch 35/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1476.2391 - val_loss: 2192.0977\n",
      "Epoch 36/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1465.0582 - val_loss: 2146.9170\n",
      "Epoch 37/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1432.3353 - val_loss: 2268.5059\n",
      "Epoch 38/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1456.6843 - val_loss: 2181.1384\n",
      "Epoch 39/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1444.5542 - val_loss: 2180.2246\n",
      "Epoch 40/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1413.4873 - val_loss: 2164.9795\n",
      "Epoch 41/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1405.0708 - val_loss: 2168.4048\n",
      "Epoch 42/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1392.7476 - val_loss: 2211.9172\n",
      "Epoch 43/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1381.1533 - val_loss: 2305.8870\n",
      "Epoch 44/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1381.5614 - val_loss: 2180.9587\n",
      "Epoch 45/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1359.0330 - val_loss: 2172.8528\n",
      "Epoch 46/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1373.2386 - val_loss: 2153.8027\n",
      "Epoch 47/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1353.6486 - val_loss: 2167.2617\n",
      "Epoch 48/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1314.5726 - val_loss: 2183.4758\n",
      "Epoch 49/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1328.3146 - val_loss: 2168.2910\n",
      "Epoch 50/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1317.8472 - val_loss: 2220.3486\n",
      "Epoch 51/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1302.3280 - val_loss: 2267.5964\n",
      "Epoch 52/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1292.1176 - val_loss: 2099.1147\n",
      "Epoch 53/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1271.7463 - val_loss: 2185.4414\n",
      "Epoch 54/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1277.5000 - val_loss: 2278.5881\n",
      "Epoch 55/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1274.1587 - val_loss: 2268.0618\n",
      "Epoch 56/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1291.4762 - val_loss: 2246.3228\n",
      "Epoch 57/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1277.3716 - val_loss: 2285.9614\n",
      "Epoch 58/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1248.9413 - val_loss: 2273.4221\n",
      "Epoch 59/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1238.3365 - val_loss: 2170.3218\n",
      "Epoch 60/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1233.9150 - val_loss: 2237.0330\n",
      "Epoch 61/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1229.1353 - val_loss: 2271.3533\n",
      "Epoch 62/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1225.9791 - val_loss: 2313.1143\n",
      "Epoch 63/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1220.3124 - val_loss: 2250.8381\n",
      "Epoch 64/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1596.0214 - val_loss: 2223.6160\n",
      "Epoch 65/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1199.9867 - val_loss: 2183.2617\n",
      "Epoch 66/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1190.3710 - val_loss: 2211.2351\n",
      "Epoch 67/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1185.0035 - val_loss: 2192.9236\n",
      "Epoch 68/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1180.8925 - val_loss: 2209.3887\n",
      "Epoch 69/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1174.7042 - val_loss: 2173.4800\n",
      "Epoch 70/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1160.3541 - val_loss: 2212.4863\n",
      "Epoch 71/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1157.2023 - val_loss: 2189.5295\n",
      "Epoch 72/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1145.2103 - val_loss: 2372.2366\n",
      "Epoch 73/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1161.7045 - val_loss: 2238.3032\n",
      "Epoch 74/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1147.9087 - val_loss: 2256.3784\n",
      "Epoch 75/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1145.2277 - val_loss: 2263.7905\n",
      "Epoch 76/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1185.1760 - val_loss: 2216.8398\n",
      "Epoch 77/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1171.6011 - val_loss: 2207.7153\n",
      "Epoch 78/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1148.4945 - val_loss: 2330.4954\n",
      "Epoch 79/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1140.3418 - val_loss: 2306.9363\n",
      "Epoch 80/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1100.4258 - val_loss: 2194.2300\n",
      "Epoch 81/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1130.8865 - val_loss: 2208.8992\n",
      "Epoch 82/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1123.7926 - val_loss: 2193.7808\n",
      "Epoch 83/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1099.6851 - val_loss: 2196.7549\n",
      "Epoch 84/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1116.6266 - val_loss: 2399.8599\n",
      "Epoch 85/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1260.8391 - val_loss: 2256.2585\n",
      "Epoch 86/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1129.5425 - val_loss: 2182.5200\n",
      "Epoch 87/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1088.4551 - val_loss: 2280.5186\n",
      "Epoch 88/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1079.6674 - val_loss: 2239.0850\n",
      "Epoch 89/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1079.1022 - val_loss: 2189.7068\n",
      "Epoch 90/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1071.4836 - val_loss: 2244.6873\n",
      "Epoch 91/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1118.0544 - val_loss: 2219.6733\n",
      "Epoch 92/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1080.8182 - val_loss: 2252.4084\n",
      "Epoch 93/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1065.1388 - val_loss: 2356.6536\n",
      "Epoch 94/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1052.2303 - val_loss: 2294.8079\n",
      "Epoch 95/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1165.7340 - val_loss: 2259.4685\n",
      "Epoch 96/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1057.2838 - val_loss: 2284.2620\n",
      "Epoch 97/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1050.4244 - val_loss: 2207.0908\n",
      "Epoch 98/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1035.7256 - val_loss: 2339.0127\n",
      "Epoch 99/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1045.5686 - val_loss: 2236.3162\n",
      "Epoch 100/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1053.5308 - val_loss: 2235.6641\n",
      "Epoch 101/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1036.5170 - val_loss: 2308.2014\n",
      "Epoch 102/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1024.4696 - val_loss: 2333.4075\n",
      "Epoch 103/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1006.7431 - val_loss: 2323.8872\n",
      "Epoch 104/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1020.6505 - val_loss: 2265.0784\n",
      "Epoch 105/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 1302.5352 - val_loss: 2326.0264\n",
      "Epoch 106/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1107.3374 - val_loss: 2331.7017\n",
      "Epoch 107/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1246.2094 - val_loss: 2375.5928\n",
      "Epoch 108/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1018.0236 - val_loss: 2258.0852\n",
      "Epoch 109/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 983.3851 - val_loss: 2260.6521\n",
      "Epoch 110/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 992.4820 - val_loss: 2278.2393\n",
      "Epoch 111/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 995.3237 - val_loss: 2262.5752\n",
      "Epoch 112/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 992.9784 - val_loss: 2211.6426\n",
      "Epoch 113/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 976.6064 - val_loss: 2299.4265\n",
      "Epoch 114/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 974.4557 - val_loss: 2329.7463\n",
      "Epoch 115/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 998.6440 - val_loss: 2306.6467\n",
      "Epoch 116/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 983.0836 - val_loss: 2254.7583\n",
      "Epoch 117/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 980.1665 - val_loss: 2230.6548\n",
      "Epoch 118/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 985.9485 - val_loss: 2276.2593\n",
      "Epoch 119/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 978.5056 - val_loss: 2301.6482\n",
      "Epoch 120/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 958.6407 - val_loss: 2354.0750\n",
      "Epoch 121/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 972.1440 - val_loss: 2320.2322\n",
      "Epoch 122/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 965.4846 - val_loss: 2365.2563\n",
      "Epoch 123/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 961.7852 - val_loss: 2270.8701\n",
      "Epoch 124/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1033.0911 - val_loss: 2338.5554\n",
      "Epoch 125/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 967.1560 - val_loss: 2329.8960\n",
      "Epoch 126/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 965.9357 - val_loss: 2243.0503\n",
      "Epoch 127/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 960.4981 - val_loss: 2347.4451\n",
      "Epoch 128/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 934.3273 - val_loss: 2390.7100\n",
      "Epoch 129/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 952.9021 - val_loss: 2321.2036\n",
      "Epoch 130/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 943.7258 - val_loss: 2379.5132\n",
      "Epoch 131/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 952.4263 - val_loss: 2250.3826\n",
      "Epoch 132/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1070.4702 - val_loss: 2339.6140\n",
      "Epoch 133/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 979.5840 - val_loss: 2356.8940\n",
      "Epoch 134/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 932.9416 - val_loss: 2298.6724\n",
      "Epoch 135/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 926.7363 - val_loss: 2300.6172\n",
      "Epoch 136/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 930.2678 - val_loss: 2269.8479\n",
      "Epoch 137/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 921.1503 - val_loss: 2316.8723\n",
      "Epoch 138/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 934.9981 - val_loss: 2398.1672\n",
      "Epoch 139/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 910.0815 - val_loss: 2303.6555\n",
      "Epoch 140/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 918.7001 - val_loss: 2310.7698\n",
      "Epoch 141/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 921.1981 - val_loss: 2302.2439\n",
      "Epoch 142/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 915.2112 - val_loss: 2367.9180\n",
      "Epoch 143/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 903.3383 - val_loss: 2322.5283\n",
      "Epoch 144/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 903.7361 - val_loss: 2358.8643\n",
      "Epoch 145/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 912.2153 - val_loss: 2341.9517\n",
      "Epoch 146/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 904.5263 - val_loss: 2335.6421\n",
      "Epoch 147/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 913.5327 - val_loss: 2326.3198\n",
      "Epoch 148/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 897.2565 - val_loss: 2393.8745\n",
      "Epoch 149/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 900.5706 - val_loss: 2371.1182\n",
      "Epoch 150/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 893.6212 - val_loss: 2606.8823\n",
      "Epoch 151/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 922.0373 - val_loss: 2488.4944\n",
      "Epoch 152/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 896.3304 - val_loss: 2440.2954\n",
      "Epoch 153/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 919.9360 - val_loss: 2435.4839\n",
      "Epoch 154/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 888.8018 - val_loss: 2424.6472\n",
      "Epoch 155/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 908.2040 - val_loss: 2376.7449\n",
      "Epoch 156/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 892.3602 - val_loss: 2306.6980\n",
      "Epoch 157/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 895.5161 - val_loss: 2378.3477\n",
      "Epoch 158/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 887.0651 - val_loss: 2324.5552\n",
      "Epoch 159/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 877.5848 - val_loss: 2388.7751\n",
      "Epoch 160/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 880.8667 - val_loss: 2456.4092\n",
      "Epoch 161/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 875.4089 - val_loss: 2457.7188\n",
      "Epoch 162/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 904.0195 - val_loss: 2571.4355\n",
      "Epoch 163/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 882.6793 - val_loss: 2435.7156\n",
      "Epoch 164/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 877.9454 - val_loss: 2421.6985\n",
      "Epoch 165/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 865.4163 - val_loss: 2414.2485\n",
      "Epoch 166/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 872.3134 - val_loss: 2381.4209\n",
      "Epoch 167/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 874.6397 - val_loss: 2357.3325\n",
      "Epoch 168/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 859.5983 - val_loss: 2474.2095\n",
      "Epoch 169/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 860.0153 - val_loss: 2568.1594\n",
      "Epoch 170/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 853.4691 - val_loss: 2459.7380\n",
      "Epoch 171/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 882.0585 - val_loss: 2422.3020\n",
      "Epoch 172/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 897.0400 - val_loss: 2469.7344\n",
      "Epoch 173/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 856.0997 - val_loss: 2496.0396\n",
      "Epoch 174/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 863.9467 - val_loss: 2493.7842\n",
      "Epoch 175/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 865.1439 - val_loss: 2584.4277\n",
      "Epoch 176/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 858.4695 - val_loss: 2492.4844\n",
      "Epoch 177/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 847.7917 - val_loss: 2422.1150\n",
      "Epoch 178/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 859.6924 - val_loss: 2480.0022\n",
      "Epoch 179/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 855.3009 - val_loss: 2469.1753\n",
      "Epoch 180/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 847.7442 - val_loss: 2442.0972\n",
      "Epoch 181/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 834.2169 - val_loss: 2444.7119\n",
      "Epoch 182/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 858.9246 - val_loss: 2578.5115\n",
      "Epoch 183/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 847.7424 - val_loss: 2431.7434\n",
      "Epoch 184/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 852.5186 - val_loss: 2481.1067\n",
      "Epoch 185/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 846.5350 - val_loss: 2655.2166\n",
      "Epoch 186/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 855.3062 - val_loss: 2656.2505\n",
      "Epoch 187/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 841.2144 - val_loss: 2463.4836\n",
      "Epoch 188/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 840.7178 - val_loss: 2520.3628\n",
      "Epoch 189/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 835.1993 - val_loss: 2483.3306\n",
      "Epoch 190/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 831.9649 - val_loss: 2463.2356\n",
      "Epoch 191/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 830.8877 - val_loss: 2528.8328\n",
      "Epoch 192/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 837.1594 - val_loss: 2439.2234\n",
      "Epoch 193/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 833.3277 - val_loss: 2527.3855\n",
      "Epoch 194/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 832.2194 - val_loss: 2448.6472\n",
      "Epoch 195/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 837.2411 - val_loss: 2394.2144\n",
      "Epoch 196/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 838.2898 - val_loss: 2464.2551\n",
      "Epoch 197/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 848.5236 - val_loss: 2419.2217\n",
      "Epoch 198/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 830.4426 - val_loss: 2630.5093\n",
      "Epoch 199/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 836.2947 - val_loss: 2461.0383\n",
      "Epoch 200/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 813.8212 - val_loss: 2444.7751\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2444.7751\n",
      "RMSE: 50.641709723032406\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Charger vos donnes dans un dataframe (assurez-vous que vos donnes sont correctement formates)\n",
    "# Supposons que le dataframe contienne des colonnes 'Feature1', 'Feature2', ..., 'FeatureN', et 'MeltingPoint'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Normalisez les caractristiques\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crez votre modle de rgression\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.005)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# valuez le modle\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd4a64c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "498/498 [==============================] - 3s 4ms/step - loss: 5946.0273 - val_loss: 3695.9937\n",
      "Epoch 2/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3156.6675 - val_loss: 2939.5806\n",
      "Epoch 3/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2693.9995 - val_loss: 2755.5459\n",
      "Epoch 4/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2460.8894 - val_loss: 2684.9702\n",
      "Epoch 5/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2325.9150 - val_loss: 2486.2883\n",
      "Epoch 6/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2227.6616 - val_loss: 2390.9751\n",
      "Epoch 7/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2171.3669 - val_loss: 2360.5664\n",
      "Epoch 8/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2102.9043 - val_loss: 2450.0317\n",
      "Epoch 9/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2060.1431 - val_loss: 2309.1604\n",
      "Epoch 10/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2020.1501 - val_loss: 2329.4192\n",
      "Epoch 11/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2006.7217 - val_loss: 2284.4316\n",
      "Epoch 12/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1967.8645 - val_loss: 2230.2046\n",
      "Epoch 13/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1947.4552 - val_loss: 2228.4639\n",
      "Epoch 14/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1935.4065 - val_loss: 2267.7163\n",
      "Epoch 15/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1901.9523 - val_loss: 2247.4275\n",
      "Epoch 16/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1883.3219 - val_loss: 2195.7856\n",
      "Epoch 17/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1860.9033 - val_loss: 2115.6335\n",
      "Epoch 18/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1845.3247 - val_loss: 2198.0247\n",
      "Epoch 19/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1832.3575 - val_loss: 2179.6992\n",
      "Epoch 20/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1810.3635 - val_loss: 2234.6631\n",
      "Epoch 21/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1791.0355 - val_loss: 2183.5664\n",
      "Epoch 22/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1784.6012 - val_loss: 2202.9329\n",
      "Epoch 23/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1773.1036 - val_loss: 2213.8091\n",
      "Epoch 24/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1757.0443 - val_loss: 2272.5405\n",
      "Epoch 25/200\n",
      "498/498 [==============================] - 3s 7ms/step - loss: 1748.1818 - val_loss: 2182.5540\n",
      "Epoch 26/200\n",
      "498/498 [==============================] - 4s 7ms/step - loss: 1747.6946 - val_loss: 2226.3684\n",
      "Epoch 27/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1715.6259 - val_loss: 2235.2810\n",
      "Epoch 28/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1711.0831 - val_loss: 2217.8054\n",
      "Epoch 29/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1704.2156 - val_loss: 2130.0308\n",
      "Epoch 30/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1686.4089 - val_loss: 2201.9463\n",
      "Epoch 31/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1682.6740 - val_loss: 2125.9517\n",
      "Epoch 32/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1665.9509 - val_loss: 2186.5911\n",
      "Epoch 33/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1657.2272 - val_loss: 2175.0461\n",
      "Epoch 34/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1657.3309 - val_loss: 2227.1399\n",
      "Epoch 35/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1629.6969 - val_loss: 2152.9192\n",
      "Epoch 36/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1638.7042 - val_loss: 2153.3987\n",
      "Epoch 37/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1630.5471 - val_loss: 2203.3699\n",
      "Epoch 38/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1610.0613 - val_loss: 2171.7017\n",
      "Epoch 39/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1598.0946 - val_loss: 2145.7737\n",
      "Epoch 40/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1587.9198 - val_loss: 2170.1111\n",
      "Epoch 41/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1580.2926 - val_loss: 2156.7222\n",
      "Epoch 42/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1578.1444 - val_loss: 2159.9990\n",
      "Epoch 43/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1583.0078 - val_loss: 2162.1560\n",
      "Epoch 44/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1559.5524 - val_loss: 2134.4304\n",
      "Epoch 45/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1563.4182 - val_loss: 2150.7295\n",
      "Epoch 46/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1551.1173 - val_loss: 2162.2175\n",
      "Epoch 47/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1542.6722 - val_loss: 2130.2986\n",
      "Epoch 48/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1534.3494 - val_loss: 2180.4512\n",
      "Epoch 49/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1531.3094 - val_loss: 2130.0298\n",
      "Epoch 50/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1525.2924 - val_loss: 2129.3027\n",
      "Epoch 51/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1513.3801 - val_loss: 2196.5867\n",
      "Epoch 52/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1498.2905 - val_loss: 2219.4180\n",
      "Epoch 53/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1495.5903 - val_loss: 2128.7158\n",
      "Epoch 54/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1486.1005 - val_loss: 2142.2329\n",
      "Epoch 55/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1478.8635 - val_loss: 2175.8599\n",
      "Epoch 56/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1480.8145 - val_loss: 2167.8550\n",
      "Epoch 57/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1472.1737 - val_loss: 2168.3469\n",
      "Epoch 58/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1468.2976 - val_loss: 2191.4092\n",
      "Epoch 59/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1466.8251 - val_loss: 2145.5591\n",
      "Epoch 60/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1460.2408 - val_loss: 2277.7141\n",
      "Epoch 61/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1461.3783 - val_loss: 2185.3330\n",
      "Epoch 62/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1443.2178 - val_loss: 2182.8955\n",
      "Epoch 63/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1428.3422 - val_loss: 2186.2585\n",
      "Epoch 64/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1439.7645 - val_loss: 2178.6963\n",
      "Epoch 65/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1418.7098 - val_loss: 2200.1536\n",
      "Epoch 66/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1421.4146 - val_loss: 2196.8770\n",
      "Epoch 67/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1413.6974 - val_loss: 2168.4639\n",
      "Epoch 68/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1418.2080 - val_loss: 2259.3430\n",
      "Epoch 69/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1411.5908 - val_loss: 2270.8030\n",
      "Epoch 70/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1402.3740 - val_loss: 2252.8979\n",
      "Epoch 71/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1393.2073 - val_loss: 2185.6616\n",
      "Epoch 72/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1384.0037 - val_loss: 2175.8970\n",
      "Epoch 73/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1378.2469 - val_loss: 2155.2556\n",
      "Epoch 74/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1372.3440 - val_loss: 2229.0432\n",
      "Epoch 75/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1377.9895 - val_loss: 2145.9531\n",
      "Epoch 76/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1368.1421 - val_loss: 2172.4741\n",
      "Epoch 77/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1354.7090 - val_loss: 2206.5754\n",
      "Epoch 78/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1362.2108 - val_loss: 2190.9006\n",
      "Epoch 79/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1347.4015 - val_loss: 2156.0415\n",
      "Epoch 80/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1343.7450 - val_loss: 2265.3621\n",
      "Epoch 81/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1354.9968 - val_loss: 2253.8120\n",
      "Epoch 82/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1342.2853 - val_loss: 2244.3628\n",
      "Epoch 83/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1323.5553 - val_loss: 2184.0627\n",
      "Epoch 84/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1324.8932 - val_loss: 2201.5186\n",
      "Epoch 85/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1325.3217 - val_loss: 2273.8896\n",
      "Epoch 86/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1322.7637 - val_loss: 2264.9463\n",
      "Epoch 87/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1329.6936 - val_loss: 2233.2280\n",
      "Epoch 88/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1329.5299 - val_loss: 2177.6675\n",
      "Epoch 89/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1311.1077 - val_loss: 2230.4792\n",
      "Epoch 90/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1308.7539 - val_loss: 2185.1987\n",
      "Epoch 91/200\n",
      "498/498 [==============================] - 3s 6ms/step - loss: 1304.2233 - val_loss: 2209.0613\n",
      "Epoch 92/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1295.4114 - val_loss: 2223.1936\n",
      "Epoch 93/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1292.7495 - val_loss: 2245.9331\n",
      "Epoch 94/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1292.7740 - val_loss: 2181.1973\n",
      "Epoch 95/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1278.9329 - val_loss: 2219.5259\n",
      "Epoch 96/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1281.2622 - val_loss: 2213.0710\n",
      "Epoch 97/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1276.8063 - val_loss: 2232.7231\n",
      "Epoch 98/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1270.0233 - val_loss: 2201.8237\n",
      "Epoch 99/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1277.4342 - val_loss: 2247.4939\n",
      "Epoch 100/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1273.2374 - val_loss: 2209.5300\n",
      "Epoch 101/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1260.9315 - val_loss: 2185.2085\n",
      "Epoch 102/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1253.1160 - val_loss: 2309.0481\n",
      "Epoch 103/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1249.7480 - val_loss: 2236.5945\n",
      "Epoch 104/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1248.3328 - val_loss: 2187.5486\n",
      "Epoch 105/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1248.4532 - val_loss: 2221.0940\n",
      "Epoch 106/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1241.7052 - val_loss: 2232.5430\n",
      "Epoch 107/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1245.9734 - val_loss: 2220.0535\n",
      "Epoch 108/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1229.8337 - val_loss: 2213.0349\n",
      "Epoch 109/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1227.8594 - val_loss: 2222.3640\n",
      "Epoch 110/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1233.0601 - val_loss: 2193.8499\n",
      "Epoch 111/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1220.6331 - val_loss: 2251.6267\n",
      "Epoch 112/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1215.9270 - val_loss: 2208.2490\n",
      "Epoch 113/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1218.7189 - val_loss: 2219.5947\n",
      "Epoch 114/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1220.1979 - val_loss: 2242.8589\n",
      "Epoch 115/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1216.6318 - val_loss: 2176.6580\n",
      "Epoch 116/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1202.0304 - val_loss: 2276.8394\n",
      "Epoch 117/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1200.4388 - val_loss: 2393.6018\n",
      "Epoch 118/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1193.7850 - val_loss: 2268.9019\n",
      "Epoch 119/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1194.2363 - val_loss: 2314.5474\n",
      "Epoch 120/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1195.5143 - val_loss: 2247.7739\n",
      "Epoch 121/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1182.8402 - val_loss: 2263.4326\n",
      "Epoch 122/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1180.1866 - val_loss: 2243.1760\n",
      "Epoch 123/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1193.4235 - val_loss: 2252.4583\n",
      "Epoch 124/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1185.1588 - val_loss: 2231.1311\n",
      "Epoch 125/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1172.7784 - val_loss: 2255.4221\n",
      "Epoch 126/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1176.3799 - val_loss: 2270.5679\n",
      "Epoch 127/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1166.5103 - val_loss: 2272.2991\n",
      "Epoch 128/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1170.6696 - val_loss: 2274.4888\n",
      "Epoch 129/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1151.3052 - val_loss: 2240.9629\n",
      "Epoch 130/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1154.0594 - val_loss: 2297.8450\n",
      "Epoch 131/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1153.5206 - val_loss: 2262.2888\n",
      "Epoch 132/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1152.9502 - val_loss: 2289.4866\n",
      "Epoch 133/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1158.5491 - val_loss: 2260.7363\n",
      "Epoch 134/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1139.7773 - val_loss: 2260.4695\n",
      "Epoch 135/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1153.1052 - val_loss: 2230.8005\n",
      "Epoch 136/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1145.3013 - val_loss: 2239.0444\n",
      "Epoch 137/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1141.1007 - val_loss: 2258.1543\n",
      "Epoch 138/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1123.1447 - val_loss: 2215.7070\n",
      "Epoch 139/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1130.7076 - val_loss: 2244.8384\n",
      "Epoch 140/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1125.6167 - val_loss: 2258.1917\n",
      "Epoch 141/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1118.6372 - val_loss: 2246.7222\n",
      "Epoch 142/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1118.0337 - val_loss: 2260.0195\n",
      "Epoch 143/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1116.3824 - val_loss: 2250.0464\n",
      "Epoch 144/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1132.1324 - val_loss: 2239.3494\n",
      "Epoch 145/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1105.1704 - val_loss: 2234.5969\n",
      "Epoch 146/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1110.3733 - val_loss: 2286.7415\n",
      "Epoch 147/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1104.5012 - val_loss: 2259.1130\n",
      "Epoch 148/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1106.4723 - val_loss: 2254.6130\n",
      "Epoch 149/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1112.7446 - val_loss: 2259.6709\n",
      "Epoch 150/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1099.3651 - val_loss: 2213.1721\n",
      "Epoch 151/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1098.7203 - val_loss: 2268.6331\n",
      "Epoch 152/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1094.8896 - val_loss: 2314.3691\n",
      "Epoch 153/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1091.4304 - val_loss: 2292.6296\n",
      "Epoch 154/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1088.5719 - val_loss: 2328.3301\n",
      "Epoch 155/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1088.2809 - val_loss: 2256.8562\n",
      "Epoch 156/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1076.0128 - val_loss: 2302.4971\n",
      "Epoch 157/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1075.2220 - val_loss: 2411.1643\n",
      "Epoch 158/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1083.6648 - val_loss: 2256.3523\n",
      "Epoch 159/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1070.6217 - val_loss: 2302.8767\n",
      "Epoch 160/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1076.7655 - val_loss: 2321.1187\n",
      "Epoch 161/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1071.7113 - val_loss: 2256.6497\n",
      "Epoch 162/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1068.6141 - val_loss: 2232.0042\n",
      "Epoch 163/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1060.2424 - val_loss: 2377.0476\n",
      "Epoch 164/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1057.4767 - val_loss: 2303.4800\n",
      "Epoch 165/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1057.0211 - val_loss: 2260.2952\n",
      "Epoch 166/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1067.9679 - val_loss: 2353.2373\n",
      "Epoch 167/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1049.6414 - val_loss: 2344.4900\n",
      "Epoch 168/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1048.4698 - val_loss: 2284.8884\n",
      "Epoch 169/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1041.4590 - val_loss: 2327.9763\n",
      "Epoch 170/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1053.8826 - val_loss: 2352.2959\n",
      "Epoch 171/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1032.9756 - val_loss: 2333.2737\n",
      "Epoch 172/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1035.1984 - val_loss: 2329.1838\n",
      "Epoch 173/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1035.0657 - val_loss: 2383.4041\n",
      "Epoch 174/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1038.0387 - val_loss: 2279.3103\n",
      "Epoch 175/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1036.1456 - val_loss: 2371.3025\n",
      "Epoch 176/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1029.4719 - val_loss: 2337.2141\n",
      "Epoch 177/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1022.4485 - val_loss: 2386.6624\n",
      "Epoch 178/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1020.5505 - val_loss: 2313.9661\n",
      "Epoch 179/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1025.2395 - val_loss: 2322.1892\n",
      "Epoch 180/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1007.5770 - val_loss: 2322.6843\n",
      "Epoch 181/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1017.7215 - val_loss: 2300.6855\n",
      "Epoch 182/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1026.5576 - val_loss: 2355.9441\n",
      "Epoch 183/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1013.2670 - val_loss: 2281.1685\n",
      "Epoch 184/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1016.6490 - val_loss: 2389.0159\n",
      "Epoch 185/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1009.2186 - val_loss: 2402.9302\n",
      "Epoch 186/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1004.4672 - val_loss: 2400.8457\n",
      "Epoch 187/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1014.1657 - val_loss: 2318.8647\n",
      "Epoch 188/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 996.9988 - val_loss: 2297.4661\n",
      "Epoch 189/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 988.2775 - val_loss: 2367.5891\n",
      "Epoch 190/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 994.8674 - val_loss: 2323.0454\n",
      "Epoch 191/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 983.3413 - val_loss: 2389.7966\n",
      "Epoch 192/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 984.7597 - val_loss: 2373.9036\n",
      "Epoch 193/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 987.4541 - val_loss: 2365.4219\n",
      "Epoch 194/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1002.0615 - val_loss: 2371.4819\n",
      "Epoch 195/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 990.3494 - val_loss: 2307.6575\n",
      "Epoch 196/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 978.5013 - val_loss: 2301.3777\n",
      "Epoch 197/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 972.5432 - val_loss: 2313.8491\n",
      "Epoch 198/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 978.2153 - val_loss: 2385.5408\n",
      "Epoch 199/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 977.7817 - val_loss: 2373.0002\n",
      "Epoch 200/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 972.4225 - val_loss: 2390.7783\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2390.7783\n",
      "RMSE: 50.641709723032406\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Charger vos donnes dans un dataframe (assurez-vous que vos donnes sont correctement formates)\n",
    "# Supposons que le dataframe contienne des colonnes 'Feature1', 'Feature2', ..., 'FeatureN', et 'MeltingPoint'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Normalisez les caractristiques\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crez votre modle de rgression\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# valuez le modle\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69095fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 17027.8926 - val_loss: 11229.7852\n",
      "Epoch 2/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 7176.8633 - val_loss: 6728.8872\n",
      "Epoch 3/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5686.8125 - val_loss: 5926.5381\n",
      "Epoch 4/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5144.2607 - val_loss: 5358.7764\n",
      "Epoch 5/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4740.6797 - val_loss: 4907.2095\n",
      "Epoch 6/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4391.9795 - val_loss: 4532.0991\n",
      "Epoch 7/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4097.7891 - val_loss: 4187.6001\n",
      "Epoch 8/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3851.7812 - val_loss: 3926.6440\n",
      "Epoch 9/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3650.8665 - val_loss: 3722.9529\n",
      "Epoch 10/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3488.7339 - val_loss: 3561.3132\n",
      "Epoch 11/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3351.2078 - val_loss: 3448.6501\n",
      "Epoch 12/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 3241.0022 - val_loss: 3336.9929\n",
      "Epoch 13/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 3145.2571 - val_loss: 3247.4775\n",
      "Epoch 14/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3055.0266 - val_loss: 3156.2280\n",
      "Epoch 15/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2975.1868 - val_loss: 3081.4678\n",
      "Epoch 16/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2903.0544 - val_loss: 3007.8433\n",
      "Epoch 17/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2836.9294 - val_loss: 2940.9421\n",
      "Epoch 18/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2775.6821 - val_loss: 2888.1570\n",
      "Epoch 19/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2717.3132 - val_loss: 2827.5994\n",
      "Epoch 20/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2663.7974 - val_loss: 2796.8408\n",
      "Epoch 21/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 2616.7051 - val_loss: 2734.7295\n",
      "Epoch 22/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2568.9214 - val_loss: 2700.7952\n",
      "Epoch 23/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2525.5601 - val_loss: 2664.7734\n",
      "Epoch 24/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2486.1987 - val_loss: 2637.3831\n",
      "Epoch 25/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2449.5991 - val_loss: 2622.3442\n",
      "Epoch 26/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2416.3435 - val_loss: 2583.0173\n",
      "Epoch 27/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2382.9270 - val_loss: 2558.2078\n",
      "Epoch 28/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2353.8726 - val_loss: 2537.7561\n",
      "Epoch 29/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2325.6802 - val_loss: 2516.4519\n",
      "Epoch 30/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2301.2756 - val_loss: 2525.8335\n",
      "Epoch 31/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2278.5144 - val_loss: 2498.0535\n",
      "Epoch 32/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2258.1777 - val_loss: 2465.0171\n",
      "Epoch 33/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2239.0369 - val_loss: 2455.0039\n",
      "Epoch 34/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2220.8628 - val_loss: 2446.5803\n",
      "Epoch 35/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2200.7170 - val_loss: 2424.0872\n",
      "Epoch 36/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2184.5115 - val_loss: 2438.2920\n",
      "Epoch 37/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2169.5806 - val_loss: 2429.2024\n",
      "Epoch 38/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2153.8806 - val_loss: 2424.0872\n",
      "Epoch 39/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2144.5674 - val_loss: 2396.2466\n",
      "Epoch 40/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2128.8523 - val_loss: 2394.0874\n",
      "Epoch 41/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2118.8230 - val_loss: 2403.1255\n",
      "Epoch 42/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2102.5874 - val_loss: 2384.7524\n",
      "Epoch 43/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2092.4758 - val_loss: 2371.5610\n",
      "Epoch 44/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2081.6628 - val_loss: 2381.3809\n",
      "Epoch 45/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2070.1404 - val_loss: 2344.6042\n",
      "Epoch 46/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2060.1682 - val_loss: 2369.0168\n",
      "Epoch 47/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2047.3124 - val_loss: 2318.1062\n",
      "Epoch 48/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2039.9796 - val_loss: 2359.8779\n",
      "Epoch 49/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2028.8245 - val_loss: 2331.4529\n",
      "Epoch 50/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2020.1586 - val_loss: 2332.2222\n",
      "Epoch 51/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2008.0320 - val_loss: 2326.5562\n",
      "Epoch 52/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2003.4302 - val_loss: 2350.1199\n",
      "Epoch 53/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1994.7593 - val_loss: 2311.5505\n",
      "Epoch 54/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1987.5046 - val_loss: 2336.2798\n",
      "Epoch 55/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1979.7079 - val_loss: 2289.9607\n",
      "Epoch 56/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1972.9159 - val_loss: 2328.1819\n",
      "Epoch 57/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1961.9402 - val_loss: 2289.5896\n",
      "Epoch 58/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1957.5773 - val_loss: 2341.0774\n",
      "Epoch 59/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1949.1100 - val_loss: 2294.1260\n",
      "Epoch 60/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1940.6356 - val_loss: 2335.2976\n",
      "Epoch 61/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1937.1459 - val_loss: 2265.5522\n",
      "Epoch 62/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1930.2778 - val_loss: 2314.2974\n",
      "Epoch 63/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1924.5916 - val_loss: 2263.7512\n",
      "Epoch 64/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1917.8815 - val_loss: 2324.9373\n",
      "Epoch 65/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1912.1842 - val_loss: 2254.8230\n",
      "Epoch 66/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1905.7123 - val_loss: 2305.2971\n",
      "Epoch 67/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1900.0953 - val_loss: 2285.1846\n",
      "Epoch 68/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1895.5902 - val_loss: 2310.8726\n",
      "Epoch 69/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1887.1858 - val_loss: 2275.4878\n",
      "Epoch 70/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1881.2639 - val_loss: 2315.5366\n",
      "Epoch 71/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1877.3333 - val_loss: 2272.9753\n",
      "Epoch 72/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1874.1771 - val_loss: 2329.6257\n",
      "Epoch 73/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1870.9639 - val_loss: 2263.4641\n",
      "Epoch 74/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1863.8047 - val_loss: 2316.2542\n",
      "Epoch 75/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1860.1310 - val_loss: 2269.9302\n",
      "Epoch 76/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1853.4708 - val_loss: 2292.4817\n",
      "Epoch 77/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1850.8561 - val_loss: 2259.3020\n",
      "Epoch 78/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1845.2999 - val_loss: 2300.2783\n",
      "Epoch 79/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1844.1305 - val_loss: 2256.1338\n",
      "Epoch 80/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1839.3115 - val_loss: 2315.5625\n",
      "Epoch 81/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1832.6119 - val_loss: 2234.1863\n",
      "Epoch 82/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1831.0360 - val_loss: 2327.3872\n",
      "Epoch 83/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1825.5455 - val_loss: 2240.0779\n",
      "Epoch 84/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1821.9307 - val_loss: 2299.6633\n",
      "Epoch 85/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1816.1979 - val_loss: 2257.8745\n",
      "Epoch 86/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1813.2885 - val_loss: 2310.2087\n",
      "Epoch 87/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1811.2063 - val_loss: 2238.8206\n",
      "Epoch 88/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1807.6017 - val_loss: 2302.1453\n",
      "Epoch 89/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1801.4236 - val_loss: 2243.5845\n",
      "Epoch 90/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1799.1669 - val_loss: 2316.0657\n",
      "Epoch 91/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1795.6836 - val_loss: 2256.7214\n",
      "Epoch 92/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1792.4158 - val_loss: 2310.9092\n",
      "Epoch 93/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1787.1674 - val_loss: 2259.9751\n",
      "Epoch 94/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1781.6127 - val_loss: 2331.5955\n",
      "Epoch 95/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1781.9500 - val_loss: 2268.4526\n",
      "Epoch 96/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1779.0541 - val_loss: 2300.3032\n",
      "Epoch 97/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1772.2981 - val_loss: 2221.3464\n",
      "Epoch 98/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1774.2263 - val_loss: 2319.1558\n",
      "Epoch 99/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1767.3292 - val_loss: 2263.3193\n",
      "Epoch 100/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1764.0708 - val_loss: 2302.5574\n",
      "Epoch 101/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1761.7942 - val_loss: 2248.7871\n",
      "Epoch 102/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1757.7703 - val_loss: 2318.4785\n",
      "Epoch 103/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1754.0449 - val_loss: 2256.6433\n",
      "Epoch 104/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1752.5199 - val_loss: 2335.2063\n",
      "Epoch 105/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1746.4261 - val_loss: 2277.1960\n",
      "Epoch 106/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1747.1765 - val_loss: 2259.3816\n",
      "Epoch 107/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1742.6931 - val_loss: 2297.3740\n",
      "Epoch 108/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1738.8185 - val_loss: 2267.2993\n",
      "Epoch 109/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1735.1326 - val_loss: 2250.1582\n",
      "Epoch 110/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1735.0275 - val_loss: 2300.7986\n",
      "Epoch 111/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1729.3439 - val_loss: 2255.6838\n",
      "Epoch 112/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1729.0403 - val_loss: 2304.1409\n",
      "Epoch 113/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1725.7073 - val_loss: 2220.8391\n",
      "Epoch 114/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1724.6394 - val_loss: 2341.0925\n",
      "Epoch 115/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1719.5013 - val_loss: 2243.9458\n",
      "Epoch 116/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1716.6921 - val_loss: 2323.0024\n",
      "Epoch 117/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1713.9766 - val_loss: 2268.9973\n",
      "Epoch 118/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1709.6992 - val_loss: 2266.3875\n",
      "Epoch 119/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1708.8236 - val_loss: 2267.7480\n",
      "Epoch 120/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1702.9247 - val_loss: 2305.0920\n",
      "Epoch 121/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1705.1655 - val_loss: 2277.1360\n",
      "Epoch 122/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1698.9656 - val_loss: 2352.6812\n",
      "Epoch 123/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1698.6909 - val_loss: 2254.7334\n",
      "Epoch 124/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1694.1296 - val_loss: 2323.6060\n",
      "Epoch 125/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1695.4935 - val_loss: 2263.6223\n",
      "Epoch 126/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1691.4479 - val_loss: 2289.7759\n",
      "Epoch 127/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1687.8026 - val_loss: 2230.3950\n",
      "Epoch 128/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1683.8069 - val_loss: 2311.6465\n",
      "Epoch 129/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1684.1967 - val_loss: 2253.8171\n",
      "Epoch 130/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1683.2075 - val_loss: 2291.5664\n",
      "Epoch 131/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1679.8792 - val_loss: 2243.5964\n",
      "Epoch 132/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1679.7322 - val_loss: 2306.6362\n",
      "Epoch 133/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1674.4500 - val_loss: 2251.2507\n",
      "Epoch 134/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1674.3386 - val_loss: 2301.3267\n",
      "Epoch 135/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1673.1641 - val_loss: 2262.7915\n",
      "Epoch 136/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1667.1678 - val_loss: 2303.0979\n",
      "Epoch 137/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1668.3077 - val_loss: 2235.4521\n",
      "Epoch 138/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1663.7762 - val_loss: 2310.0417\n",
      "Epoch 139/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1665.0234 - val_loss: 2244.1177\n",
      "Epoch 140/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1660.6179 - val_loss: 2331.5535\n",
      "Epoch 141/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1658.0455 - val_loss: 2251.5859\n",
      "Epoch 142/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1657.0422 - val_loss: 2307.2581\n",
      "Epoch 143/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1652.0505 - val_loss: 2235.6863\n",
      "Epoch 144/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1652.0011 - val_loss: 2314.8167\n",
      "Epoch 145/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1650.8990 - val_loss: 2265.6108\n",
      "Epoch 146/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1652.2848 - val_loss: 2307.5586\n",
      "Epoch 147/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1647.2424 - val_loss: 2239.6262\n",
      "Epoch 148/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1646.3822 - val_loss: 2325.8962\n",
      "Epoch 149/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1643.5980 - val_loss: 2259.6487\n",
      "Epoch 150/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1642.7958 - val_loss: 2274.5090\n",
      "Epoch 151/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1641.6914 - val_loss: 2257.1077\n",
      "Epoch 152/200\n",
      "498/498 [==============================] - 3s 6ms/step - loss: 1638.8958 - val_loss: 2336.1116\n",
      "Epoch 153/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1637.3123 - val_loss: 2245.8005\n",
      "Epoch 154/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1634.5935 - val_loss: 2308.5667\n",
      "Epoch 155/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1633.4111 - val_loss: 2242.5608\n",
      "Epoch 156/200\n",
      "350/498 [====================>.........] - ETA: 0s - loss: 1625.7908"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find a model.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Entranez le modle\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# valuez le modle\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Charger vos donnes dans un dataframe (assurez-vous que vos donnes sont correctement formates)\n",
    "# Supposons que le dataframe contienne des colonnes 'Feature1', 'Feature2', ..., 'FeatureN', et 'MeltingPoint'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Normalisez les caractristiques\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crez votre modle de rgression\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# valuez le modle\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b422d6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 7784.6860 - val_loss: 4892.7949\n",
      "Epoch 2/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4280.6470 - val_loss: 3827.2000\n",
      "Epoch 3/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3404.5405 - val_loss: 3235.9014\n",
      "Epoch 4/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2987.7778 - val_loss: 2987.8345\n",
      "Epoch 5/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2763.3521 - val_loss: 2859.2288\n",
      "Epoch 6/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2602.1370 - val_loss: 2742.2678\n",
      "Epoch 7/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2485.8499 - val_loss: 2657.6582\n",
      "Epoch 8/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2392.1768 - val_loss: 2564.5901\n",
      "Epoch 9/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2315.2944 - val_loss: 2567.1421\n",
      "Epoch 10/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2251.6082 - val_loss: 2462.6060\n",
      "Epoch 11/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2215.7124 - val_loss: 2452.6646\n",
      "Epoch 12/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2167.8098 - val_loss: 2410.0249\n",
      "Epoch 13/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2116.7571 - val_loss: 2381.6973\n",
      "Epoch 14/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2083.5938 - val_loss: 2373.1545\n",
      "Epoch 15/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2072.2129 - val_loss: 2343.7605\n",
      "Epoch 16/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2032.3730 - val_loss: 2259.1704\n",
      "Epoch 17/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2005.4536 - val_loss: 2334.3823\n",
      "Epoch 18/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1995.2711 - val_loss: 2289.0476\n",
      "Epoch 19/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1972.7388 - val_loss: 2297.8213\n",
      "Epoch 20/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1961.5674 - val_loss: 2291.9609\n",
      "Epoch 21/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1942.6954 - val_loss: 2306.7346\n",
      "Epoch 22/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1919.8418 - val_loss: 2258.1609\n",
      "Epoch 23/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1908.4027 - val_loss: 2252.7029\n",
      "Epoch 24/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1898.9419 - val_loss: 2275.0305\n",
      "Epoch 25/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1884.3553 - val_loss: 2308.2971\n",
      "Epoch 26/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1864.4823 - val_loss: 2226.0417\n",
      "Epoch 27/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1846.1388 - val_loss: 2301.1909\n",
      "Epoch 28/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1840.7577 - val_loss: 2227.7261\n",
      "Epoch 29/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1836.0310 - val_loss: 2238.7673\n",
      "Epoch 30/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1814.3251 - val_loss: 2256.4548\n",
      "Epoch 31/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1808.0796 - val_loss: 2264.4375\n",
      "Epoch 32/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1801.1923 - val_loss: 2239.3191\n",
      "Epoch 33/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1795.8818 - val_loss: 2239.8457\n",
      "Epoch 34/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1774.2369 - val_loss: 2300.9119\n",
      "Epoch 35/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1770.8359 - val_loss: 2230.4521\n",
      "Epoch 36/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1763.6364 - val_loss: 2295.4221\n",
      "Epoch 37/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1753.8700 - val_loss: 2244.1348\n",
      "Epoch 38/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1742.5341 - val_loss: 2210.6887\n",
      "Epoch 39/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1739.3157 - val_loss: 2232.0527\n",
      "Epoch 40/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1728.5300 - val_loss: 2247.3479\n",
      "Epoch 41/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1721.2383 - val_loss: 2271.3384\n",
      "Epoch 42/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1713.2723 - val_loss: 2194.5757\n",
      "Epoch 43/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1695.9053 - val_loss: 2271.7686\n",
      "Epoch 44/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1700.4407 - val_loss: 2248.9771\n",
      "Epoch 45/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1691.8770 - val_loss: 2276.5298\n",
      "Epoch 46/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1686.2867 - val_loss: 2195.9578\n",
      "Epoch 47/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1676.8710 - val_loss: 2251.1406\n",
      "Epoch 48/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1680.7534 - val_loss: 2199.2551\n",
      "Epoch 49/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1663.3370 - val_loss: 2293.8374\n",
      "Epoch 50/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1659.3282 - val_loss: 2187.3269\n",
      "Epoch 51/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1658.1642 - val_loss: 2300.8574\n",
      "Epoch 52/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1649.0881 - val_loss: 2231.8765\n",
      "Epoch 53/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1635.4647 - val_loss: 2280.6248\n",
      "Epoch 54/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1642.7013 - val_loss: 2212.0708\n",
      "Epoch 55/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1634.2284 - val_loss: 2246.1719\n",
      "Epoch 56/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1632.0315 - val_loss: 2205.4170\n",
      "Epoch 57/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1617.3948 - val_loss: 2222.6467\n",
      "Epoch 58/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1613.6329 - val_loss: 2245.1506\n",
      "Epoch 59/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1612.0214 - val_loss: 2261.0396\n",
      "Epoch 60/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1613.4076 - val_loss: 2238.1013\n",
      "Epoch 61/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1603.6328 - val_loss: 2267.2036\n",
      "Epoch 62/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1598.1051 - val_loss: 2256.6467\n",
      "Epoch 63/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1592.9189 - val_loss: 2218.7424\n",
      "Epoch 64/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1585.5337 - val_loss: 2196.4385\n",
      "Epoch 65/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1583.3000 - val_loss: 2221.4185\n",
      "Epoch 66/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1583.1527 - val_loss: 2194.8799\n",
      "Epoch 67/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1573.5635 - val_loss: 2212.1731\n",
      "Epoch 68/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1562.7433 - val_loss: 2236.8071\n",
      "Epoch 69/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1564.7241 - val_loss: 2257.4512\n",
      "Epoch 70/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1560.1503 - val_loss: 2263.2122\n",
      "Epoch 71/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1559.6423 - val_loss: 2210.6594\n",
      "Epoch 72/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1545.0842 - val_loss: 2220.6074\n",
      "Epoch 73/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1545.9368 - val_loss: 2195.7173\n",
      "Epoch 74/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1547.4303 - val_loss: 2220.7493\n",
      "Epoch 75/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1533.5178 - val_loss: 2247.3647\n",
      "Epoch 76/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1530.0090 - val_loss: 2189.3218\n",
      "Epoch 77/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1529.5381 - val_loss: 2211.2620\n",
      "Epoch 78/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1529.2043 - val_loss: 2176.6089\n",
      "Epoch 79/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1522.9072 - val_loss: 2205.1152\n",
      "Epoch 80/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1521.5480 - val_loss: 2215.0369\n",
      "Epoch 81/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1518.7399 - val_loss: 2228.7534\n",
      "Epoch 82/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1507.6995 - val_loss: 2183.5762\n",
      "Epoch 83/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1517.9513 - val_loss: 2245.3452\n",
      "Epoch 84/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1511.4043 - val_loss: 2237.9900\n",
      "Epoch 85/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1515.0906 - val_loss: 2203.4094\n",
      "Epoch 86/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1498.4205 - val_loss: 2216.1777\n",
      "Epoch 87/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1494.8845 - val_loss: 2215.0923\n",
      "Epoch 88/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1488.7115 - val_loss: 2243.8669\n",
      "Epoch 89/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1486.0529 - val_loss: 2229.7375\n",
      "Epoch 90/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1478.7827 - val_loss: 2205.3521\n",
      "Epoch 91/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1481.2433 - val_loss: 2228.5854\n",
      "Epoch 92/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1482.0446 - val_loss: 2281.3413\n",
      "Epoch 93/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1474.7904 - val_loss: 2276.3767\n",
      "Epoch 94/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1471.7426 - val_loss: 2183.7302\n",
      "Epoch 95/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1467.6398 - val_loss: 2243.4092\n",
      "Epoch 96/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1461.1659 - val_loss: 2258.6257\n",
      "Epoch 97/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1457.1396 - val_loss: 2212.4490\n",
      "Epoch 98/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1457.6676 - val_loss: 2274.5254\n",
      "Epoch 99/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 1449.7181 - val_loss: 2313.0212\n",
      "Epoch 100/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1455.5439 - val_loss: 2208.7297\n",
      "Epoch 101/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1456.5359 - val_loss: 2226.5913\n",
      "Epoch 102/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1449.2173 - val_loss: 2227.5093\n",
      "Epoch 103/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1441.7094 - val_loss: 2204.9119\n",
      "Epoch 104/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1440.7538 - val_loss: 2234.2773\n",
      "Epoch 105/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1439.9636 - val_loss: 2167.0010\n",
      "Epoch 106/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1433.9312 - val_loss: 2197.0103\n",
      "Epoch 107/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1431.5573 - val_loss: 2266.5879\n",
      "Epoch 108/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1424.7708 - val_loss: 2223.8347\n",
      "Epoch 109/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1430.5897 - val_loss: 2247.8057\n",
      "Epoch 110/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1420.7186 - val_loss: 2219.9509\n",
      "Epoch 111/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1422.1067 - val_loss: 2273.5457\n",
      "Epoch 112/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1421.6829 - val_loss: 2184.3843\n",
      "Epoch 113/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1414.0828 - val_loss: 2211.0740\n",
      "Epoch 114/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1413.0629 - val_loss: 2257.1528\n",
      "Epoch 115/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1404.8763 - val_loss: 2224.7666\n",
      "Epoch 116/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1405.5304 - val_loss: 2213.0754\n",
      "Epoch 117/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1405.1306 - val_loss: 2281.5320\n",
      "Epoch 118/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1402.1954 - val_loss: 2256.8506\n",
      "Epoch 119/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1403.3035 - val_loss: 2235.0371\n",
      "Epoch 120/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1395.6914 - val_loss: 2230.3506\n",
      "Epoch 121/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1392.3573 - val_loss: 2308.9646\n",
      "Epoch 122/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1384.5708 - val_loss: 2242.7546\n",
      "Epoch 123/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1381.8363 - val_loss: 2219.3936\n",
      "Epoch 124/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1386.2054 - val_loss: 2230.1250\n",
      "Epoch 125/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1388.5146 - val_loss: 2373.1907\n",
      "Epoch 126/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1378.3871 - val_loss: 2247.4902\n",
      "Epoch 127/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1369.1078 - val_loss: 2204.7500\n",
      "Epoch 128/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1375.1385 - val_loss: 2206.0374\n",
      "Epoch 129/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1370.1749 - val_loss: 2232.1094\n",
      "Epoch 130/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1371.1881 - val_loss: 2224.0173\n",
      "Epoch 131/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1365.6191 - val_loss: 2320.7197\n",
      "Epoch 132/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1363.6986 - val_loss: 2220.6753\n",
      "Epoch 133/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1361.5925 - val_loss: 2227.3171\n",
      "Epoch 134/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1358.0959 - val_loss: 2211.0371\n",
      "Epoch 135/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1360.8684 - val_loss: 2174.5144\n",
      "Epoch 136/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1357.3124 - val_loss: 2278.3855\n",
      "Epoch 137/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1342.9432 - val_loss: 2285.4348\n",
      "Epoch 138/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1345.7155 - val_loss: 2246.6079\n",
      "Epoch 139/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1367.1647 - val_loss: 2207.4197\n",
      "Epoch 140/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1349.1342 - val_loss: 2260.8604\n",
      "Epoch 141/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1343.3171 - val_loss: 2233.3125\n",
      "Epoch 142/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1337.4397 - val_loss: 2242.8120\n",
      "Epoch 143/200\n",
      "498/498 [==============================] - 4s 9ms/step - loss: 1340.1462 - val_loss: 2306.2749\n",
      "Epoch 144/200\n",
      "498/498 [==============================] - 3s 7ms/step - loss: 1341.3862 - val_loss: 2242.4851\n",
      "Epoch 145/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1340.6312 - val_loss: 2206.2634\n",
      "Epoch 146/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1332.5250 - val_loss: 2272.3086\n",
      "Epoch 147/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1318.7817 - val_loss: 2235.2729\n",
      "Epoch 148/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1322.4497 - val_loss: 2226.2627\n",
      "Epoch 149/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1322.5707 - val_loss: 2274.9788\n",
      "Epoch 150/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1321.2009 - val_loss: 2266.4128\n",
      "Epoch 151/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1319.2466 - val_loss: 2227.2498\n",
      "Epoch 152/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1320.7421 - val_loss: 2242.4473\n",
      "Epoch 153/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1315.2892 - val_loss: 2262.3108\n",
      "Epoch 154/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1311.8955 - val_loss: 2264.1040\n",
      "Epoch 155/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1304.6057 - val_loss: 2272.3599\n",
      "Epoch 156/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1300.9468 - val_loss: 2220.6189\n",
      "Epoch 157/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1303.1967 - val_loss: 2226.7600\n",
      "Epoch 158/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1306.0160 - val_loss: 2328.5508\n",
      "Epoch 159/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1304.5120 - val_loss: 2200.2947\n",
      "Epoch 160/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1299.0040 - val_loss: 2271.5327\n",
      "Epoch 161/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1298.6130 - val_loss: 2278.7512\n",
      "Epoch 162/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1287.5599 - val_loss: 2345.8462\n",
      "Epoch 163/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1287.5204 - val_loss: 2282.0940\n",
      "Epoch 164/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1289.7632 - val_loss: 2282.2451\n",
      "Epoch 165/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1284.0736 - val_loss: 2293.2375\n",
      "Epoch 166/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1288.3073 - val_loss: 2302.0571\n",
      "Epoch 167/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1287.5903 - val_loss: 2272.6221\n",
      "Epoch 168/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1276.1510 - val_loss: 2305.4863\n",
      "Epoch 169/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1279.9877 - val_loss: 2236.8003\n",
      "Epoch 170/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1276.5225 - val_loss: 2268.6663\n",
      "Epoch 171/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1276.1951 - val_loss: 2281.3362\n",
      "Epoch 172/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1269.6195 - val_loss: 2253.3699\n",
      "Epoch 173/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1272.0621 - val_loss: 2257.4919\n",
      "Epoch 174/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1266.3823 - val_loss: 2247.8196\n",
      "Epoch 175/200\n",
      "498/498 [==============================] - 3s 6ms/step - loss: 1266.9521 - val_loss: 2273.6353\n",
      "Epoch 176/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1259.1208 - val_loss: 2289.6885\n",
      "Epoch 177/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1258.8571 - val_loss: 2296.3682\n",
      "Epoch 178/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1266.9618 - val_loss: 2240.1404\n",
      "Epoch 179/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1256.0369 - val_loss: 2233.3359\n",
      "Epoch 180/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1250.1589 - val_loss: 2249.2161\n",
      "Epoch 181/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1257.2234 - val_loss: 2223.8630\n",
      "Epoch 182/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1250.8668 - val_loss: 2294.1250\n",
      "Epoch 183/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1244.0767 - val_loss: 2264.7327\n",
      "Epoch 184/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1250.4647 - val_loss: 2281.4553\n",
      "Epoch 185/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1243.0706 - val_loss: 2286.1865\n",
      "Epoch 186/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1250.7041 - val_loss: 2302.1123\n",
      "Epoch 187/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1238.0144 - val_loss: 2273.0955\n",
      "Epoch 188/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1232.2156 - val_loss: 2333.9854\n",
      "Epoch 189/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1230.7319 - val_loss: 2306.6855\n",
      "Epoch 190/200\n",
      "498/498 [==============================] - 3s 6ms/step - loss: 1244.7260 - val_loss: 2308.0940\n",
      "Epoch 191/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 1235.4434 - val_loss: 2256.9954\n",
      "Epoch 192/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1234.2103 - val_loss: 2434.9905\n",
      "Epoch 193/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1230.9650 - val_loss: 2334.4077\n",
      "Epoch 194/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1220.7167 - val_loss: 2262.8677\n",
      "Epoch 195/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1226.7626 - val_loss: 2260.7222\n",
      "Epoch 196/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1221.7269 - val_loss: 2308.6072\n",
      "Epoch 197/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1215.8573 - val_loss: 2419.6245\n",
      "Epoch 198/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1217.8488 - val_loss: 2306.6406\n",
      "Epoch 199/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1221.1522 - val_loss: 2242.1296\n",
      "Epoch 200/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1211.7362 - val_loss: 2304.5630\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2304.5630\n",
      "RMSE: 50.641709723032406\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Charger vos donnes dans un dataframe (assurez-vous que vos donnes sont correctement formates)\n",
    "# Supposons que le dataframe contienne des colonnes 'Feature1', 'Feature2', ..., 'FeatureN', et 'MeltingPoint'\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Normalisez les caractristiques\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Crez votre modle de rgression\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# valuez le modle\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5d71ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "498/498 [==============================] - 9s 4ms/step - loss: 17344.8105 - val_loss: 14597.9932\n",
      "Epoch 2/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 11209.3828 - val_loss: 8882.9463\n",
      "Epoch 3/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 7644.0801 - val_loss: 7214.0903\n",
      "Epoch 4/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 6697.2354 - val_loss: 6744.3379\n",
      "Epoch 5/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 6337.0806 - val_loss: 6485.7778\n",
      "Epoch 6/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 6109.7759 - val_loss: 6300.6050\n",
      "Epoch 7/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5938.1636 - val_loss: 6150.3853\n",
      "Epoch 8/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5798.7925 - val_loss: 6023.8232\n",
      "Epoch 9/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5680.5073 - val_loss: 5916.4761\n",
      "Epoch 10/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5578.2886 - val_loss: 5822.3320\n",
      "Epoch 11/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5488.1060 - val_loss: 5736.2686\n",
      "Epoch 12/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5406.7856 - val_loss: 5659.9170\n",
      "Epoch 13/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5333.1729 - val_loss: 5588.5659\n",
      "Epoch 14/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5265.6938 - val_loss: 5523.7866\n",
      "Epoch 15/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5202.8467 - val_loss: 5461.8472\n",
      "Epoch 16/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5143.6313 - val_loss: 5404.0522\n",
      "Epoch 17/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5086.7666 - val_loss: 5349.0986\n",
      "Epoch 18/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 5033.4531 - val_loss: 5297.5894\n",
      "Epoch 19/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4983.3882 - val_loss: 5247.6030\n",
      "Epoch 20/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4935.9424 - val_loss: 5200.6040\n",
      "Epoch 21/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4890.9307 - val_loss: 5156.7617\n",
      "Epoch 22/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4848.1714 - val_loss: 5113.3662\n",
      "Epoch 23/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4806.4692 - val_loss: 5069.9668\n",
      "Epoch 24/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4766.7310 - val_loss: 5030.7944\n",
      "Epoch 25/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4729.0591 - val_loss: 4992.4019\n",
      "Epoch 26/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4692.8564 - val_loss: 4955.2188\n",
      "Epoch 27/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4658.3169 - val_loss: 4919.3730\n",
      "Epoch 28/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4625.2026 - val_loss: 4885.7319\n",
      "Epoch 29/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4593.4404 - val_loss: 4852.5635\n",
      "Epoch 30/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4562.7930 - val_loss: 4821.4087\n",
      "Epoch 31/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4533.4321 - val_loss: 4790.4102\n",
      "Epoch 32/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4505.1357 - val_loss: 4760.7754\n",
      "Epoch 33/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4477.8013 - val_loss: 4732.3887\n",
      "Epoch 34/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4451.4409 - val_loss: 4704.5820\n",
      "Epoch 35/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4425.8657 - val_loss: 4677.4419\n",
      "Epoch 36/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4401.0273 - val_loss: 4650.8457\n",
      "Epoch 37/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4377.0786 - val_loss: 4625.3896\n",
      "Epoch 38/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4353.5776 - val_loss: 4600.4985\n",
      "Epoch 39/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4330.6274 - val_loss: 4576.7178\n",
      "Epoch 40/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4308.0361 - val_loss: 4552.0664\n",
      "Epoch 41/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 4285.3569 - val_loss: 4527.7896\n",
      "Epoch 42/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4262.9106 - val_loss: 4503.7466\n",
      "Epoch 43/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4241.3486 - val_loss: 4479.9697\n",
      "Epoch 44/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 4220.8604 - val_loss: 4458.2319\n",
      "Epoch 45/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4201.1636 - val_loss: 4437.5288\n",
      "Epoch 46/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4181.9897 - val_loss: 4417.2954\n",
      "Epoch 47/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 4163.6133 - val_loss: 4396.6836\n",
      "Epoch 48/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4145.4355 - val_loss: 4376.7583\n",
      "Epoch 49/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4127.6519 - val_loss: 4356.9839\n",
      "Epoch 50/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4110.5342 - val_loss: 4339.1978\n",
      "Epoch 51/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4093.8379 - val_loss: 4320.9902\n",
      "Epoch 52/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 4077.3843 - val_loss: 4303.0918\n",
      "Epoch 53/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4061.3442 - val_loss: 4285.9209\n",
      "Epoch 54/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4045.6406 - val_loss: 4268.8179\n",
      "Epoch 55/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4030.3325 - val_loss: 4252.0200\n",
      "Epoch 56/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 4015.2986 - val_loss: 4235.6392\n",
      "Epoch 57/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 4000.5715 - val_loss: 4219.5288\n",
      "Epoch 58/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 3986.1318 - val_loss: 4203.6045\n",
      "Epoch 59/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3972.0237 - val_loss: 4188.1025\n",
      "Epoch 60/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3958.1704 - val_loss: 4173.0571\n",
      "Epoch 61/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3944.5935 - val_loss: 4158.3032\n",
      "Epoch 62/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 3931.2664 - val_loss: 4143.6704\n",
      "Epoch 63/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3918.2180 - val_loss: 4129.6079\n",
      "Epoch 64/200\n",
      "498/498 [==============================] - 3s 5ms/step - loss: 3905.4553 - val_loss: 4115.4565\n",
      "Epoch 65/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 3892.9075 - val_loss: 4101.7656\n",
      "Epoch 66/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 3880.5825 - val_loss: 4088.4043\n",
      "Epoch 67/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3868.4722 - val_loss: 4075.3542\n",
      "Epoch 68/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3856.6365 - val_loss: 4062.4924\n",
      "Epoch 69/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3844.8926 - val_loss: 4050.1333\n",
      "Epoch 70/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3833.4302 - val_loss: 4037.8650\n",
      "Epoch 71/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3822.1265 - val_loss: 4025.4788\n",
      "Epoch 72/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3811.0144 - val_loss: 4013.3687\n",
      "Epoch 73/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3800.1152 - val_loss: 4001.4861\n",
      "Epoch 74/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3789.3481 - val_loss: 3989.9348\n",
      "Epoch 75/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3778.7983 - val_loss: 3978.5994\n",
      "Epoch 76/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3768.4170 - val_loss: 3967.5317\n",
      "Epoch 77/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3758.2380 - val_loss: 3956.5825\n",
      "Epoch 78/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3748.1919 - val_loss: 3945.8491\n",
      "Epoch 79/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3738.3374 - val_loss: 3935.1406\n",
      "Epoch 80/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3728.5977 - val_loss: 3924.6099\n",
      "Epoch 81/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3718.9673 - val_loss: 3914.6379\n",
      "Epoch 82/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3709.5825 - val_loss: 3904.1699\n",
      "Epoch 83/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3700.2974 - val_loss: 3894.0828\n",
      "Epoch 84/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3691.1550 - val_loss: 3884.3833\n",
      "Epoch 85/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3682.1357 - val_loss: 3874.7258\n",
      "Epoch 86/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3673.2622 - val_loss: 3864.8828\n",
      "Epoch 87/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3664.3628 - val_loss: 3855.1248\n",
      "Epoch 88/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3655.6880 - val_loss: 3845.8826\n",
      "Epoch 89/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3647.0559 - val_loss: 3836.5701\n",
      "Epoch 90/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3638.6602 - val_loss: 3827.5566\n",
      "Epoch 91/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3630.3879 - val_loss: 3818.7339\n",
      "Epoch 92/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3622.2437 - val_loss: 3809.9607\n",
      "Epoch 93/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3614.2356 - val_loss: 3801.4888\n",
      "Epoch 94/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3606.2803 - val_loss: 3793.0735\n",
      "Epoch 95/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3598.4814 - val_loss: 3784.7043\n",
      "Epoch 96/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3590.8193 - val_loss: 3776.7395\n",
      "Epoch 97/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3583.1912 - val_loss: 3768.9983\n",
      "Epoch 98/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3575.7834 - val_loss: 3760.7576\n",
      "Epoch 99/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3568.4292 - val_loss: 3752.9771\n",
      "Epoch 100/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3561.1980 - val_loss: 3745.3955\n",
      "Epoch 101/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3554.0706 - val_loss: 3737.8062\n",
      "Epoch 102/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3546.9771 - val_loss: 3730.2285\n",
      "Epoch 103/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3540.0071 - val_loss: 3722.9805\n",
      "Epoch 104/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3533.1394 - val_loss: 3715.6523\n",
      "Epoch 105/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3526.3794 - val_loss: 3708.4834\n",
      "Epoch 106/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3519.6812 - val_loss: 3701.4050\n",
      "Epoch 107/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3513.0601 - val_loss: 3694.4976\n",
      "Epoch 108/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3506.5300 - val_loss: 3687.6489\n",
      "Epoch 109/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3500.1362 - val_loss: 3680.7397\n",
      "Epoch 110/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3493.7639 - val_loss: 3674.0962\n",
      "Epoch 111/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3487.4985 - val_loss: 3667.4333\n",
      "Epoch 112/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3481.3000 - val_loss: 3660.9453\n",
      "Epoch 113/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3475.2158 - val_loss: 3654.4749\n",
      "Epoch 114/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3469.1602 - val_loss: 3648.0771\n",
      "Epoch 115/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3463.2200 - val_loss: 3641.8000\n",
      "Epoch 116/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3457.2922 - val_loss: 3635.5618\n",
      "Epoch 117/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3451.4709 - val_loss: 3629.5842\n",
      "Epoch 118/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3445.7000 - val_loss: 3623.5029\n",
      "Epoch 119/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3440.0122 - val_loss: 3617.6582\n",
      "Epoch 120/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3434.3997 - val_loss: 3611.7778\n",
      "Epoch 121/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3428.8494 - val_loss: 3605.7908\n",
      "Epoch 122/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3423.3999 - val_loss: 3600.0408\n",
      "Epoch 123/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3417.8987 - val_loss: 3594.4656\n",
      "Epoch 124/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3412.4922 - val_loss: 3589.0286\n",
      "Epoch 125/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3407.2490 - val_loss: 3583.5200\n",
      "Epoch 126/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3401.9763 - val_loss: 3578.0791\n",
      "Epoch 127/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3396.8020 - val_loss: 3572.5146\n",
      "Epoch 128/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3391.6809 - val_loss: 3567.2019\n",
      "Epoch 129/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3386.5623 - val_loss: 3562.0981\n",
      "Epoch 130/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3381.6340 - val_loss: 3556.7852\n",
      "Epoch 131/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3376.6704 - val_loss: 3551.4465\n",
      "Epoch 132/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3371.8142 - val_loss: 3546.3804\n",
      "Epoch 133/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3366.8855 - val_loss: 3541.6279\n",
      "Epoch 134/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3362.2488 - val_loss: 3536.5740\n",
      "Epoch 135/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3357.4634 - val_loss: 3531.4397\n",
      "Epoch 136/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3352.8245 - val_loss: 3526.6206\n",
      "Epoch 137/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3348.2190 - val_loss: 3521.7859\n",
      "Epoch 138/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3343.6392 - val_loss: 3517.0784\n",
      "Epoch 139/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3339.0977 - val_loss: 3512.3906\n",
      "Epoch 140/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3334.6211 - val_loss: 3507.7708\n",
      "Epoch 141/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3330.2104 - val_loss: 3503.2310\n",
      "Epoch 142/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3325.8169 - val_loss: 3498.7002\n",
      "Epoch 143/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3321.4851 - val_loss: 3494.2327\n",
      "Epoch 144/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3317.1597 - val_loss: 3489.6924\n",
      "Epoch 145/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3312.8379 - val_loss: 3485.3340\n",
      "Epoch 146/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3308.6824 - val_loss: 3480.8735\n",
      "Epoch 147/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3304.4993 - val_loss: 3476.5229\n",
      "Epoch 148/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3300.3186 - val_loss: 3472.2070\n",
      "Epoch 149/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3296.2229 - val_loss: 3467.8774\n",
      "Epoch 150/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3292.1589 - val_loss: 3463.6450\n",
      "Epoch 151/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3288.1763 - val_loss: 3459.5474\n",
      "Epoch 152/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3284.1338 - val_loss: 3455.3865\n",
      "Epoch 153/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3280.1707 - val_loss: 3451.2798\n",
      "Epoch 154/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3276.2690 - val_loss: 3447.2871\n",
      "Epoch 155/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3272.3555 - val_loss: 3443.3647\n",
      "Epoch 156/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3268.5315 - val_loss: 3439.3833\n",
      "Epoch 157/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3264.7358 - val_loss: 3435.3892\n",
      "Epoch 158/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3260.9319 - val_loss: 3431.4316\n",
      "Epoch 159/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3257.1814 - val_loss: 3427.6458\n",
      "Epoch 160/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3253.4751 - val_loss: 3423.7883\n",
      "Epoch 161/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3249.7925 - val_loss: 3420.0068\n",
      "Epoch 162/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3246.1792 - val_loss: 3416.3052\n",
      "Epoch 163/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3242.5432 - val_loss: 3412.6011\n",
      "Epoch 164/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3238.9614 - val_loss: 3408.9158\n",
      "Epoch 165/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3235.3899 - val_loss: 3405.2031\n",
      "Epoch 166/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3231.8662 - val_loss: 3401.5935\n",
      "Epoch 167/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3228.3662 - val_loss: 3398.0469\n",
      "Epoch 168/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3224.8767 - val_loss: 3394.4727\n",
      "Epoch 169/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3221.4265 - val_loss: 3390.9658\n",
      "Epoch 170/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3217.9832 - val_loss: 3387.5623\n",
      "Epoch 171/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3214.6021 - val_loss: 3384.0037\n",
      "Epoch 172/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3211.2163 - val_loss: 3380.5745\n",
      "Epoch 173/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3207.8958 - val_loss: 3377.1667\n",
      "Epoch 174/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3204.5610 - val_loss: 3373.7373\n",
      "Epoch 175/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3201.2695 - val_loss: 3370.3948\n",
      "Epoch 176/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3198.0005 - val_loss: 3367.1001\n",
      "Epoch 177/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3194.7507 - val_loss: 3363.7090\n",
      "Epoch 178/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3191.5295 - val_loss: 3360.5105\n",
      "Epoch 179/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3188.3689 - val_loss: 3357.2871\n",
      "Epoch 180/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3185.1638 - val_loss: 3353.9448\n",
      "Epoch 181/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3182.0193 - val_loss: 3350.8013\n",
      "Epoch 182/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3178.8857 - val_loss: 3347.6562\n",
      "Epoch 183/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3175.7886 - val_loss: 3344.4226\n",
      "Epoch 184/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3172.7246 - val_loss: 3341.2886\n",
      "Epoch 185/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3169.6653 - val_loss: 3338.1702\n",
      "Epoch 186/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3166.5886 - val_loss: 3335.0271\n",
      "Epoch 187/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3163.5986 - val_loss: 3331.9712\n",
      "Epoch 188/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3160.6067 - val_loss: 3328.9902\n",
      "Epoch 189/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3157.6353 - val_loss: 3325.9761\n",
      "Epoch 190/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3154.6643 - val_loss: 3322.9609\n",
      "Epoch 191/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 3151.7390 - val_loss: 3319.9963\n",
      "Epoch 192/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3148.8352 - val_loss: 3317.0627\n",
      "Epoch 193/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3145.9268 - val_loss: 3314.0996\n",
      "Epoch 194/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3143.0557 - val_loss: 3311.2344\n",
      "Epoch 195/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3140.1931 - val_loss: 3308.3176\n",
      "Epoch 196/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 3137.3430 - val_loss: 3305.4001\n",
      "Epoch 197/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 3134.5034 - val_loss: 3302.5510\n",
      "Epoch 198/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3131.7051 - val_loss: 3299.7485\n",
      "Epoch 199/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 3128.9045 - val_loss: 3296.9360\n",
      "Epoch 200/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3126.1089 - val_loss: 3294.2053\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 3294.2053\n",
      "RMSE: 57.39516810904578\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad\n",
    "\n",
    "# Choisissez l'optimiseur que vous souhaitez utiliser\n",
    "# optimizer = SGD(learning_rate=0.01)  # Stochastic Gradient Descent\n",
    "# optimizer = RMSprop(learning_rate=0.01)  # RMSprop\n",
    "optimizer = Adagrad(learning_rate=0.001)  # Adagrad\n",
    "\n",
    "# Crez votre modle de rgression\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle avec l'optimiseur choisi\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "rmse = np.sqrt(loss)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a5678609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "498/498 [==============================] - 3s 4ms/step - loss: 4822.3608 - val_loss: 4228.8545\n",
      "Epoch 2/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3574.8777 - val_loss: 3575.9824\n",
      "Epoch 3/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 3137.8369 - val_loss: 3237.6948\n",
      "Epoch 4/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2929.7625 - val_loss: 3075.7568\n",
      "Epoch 5/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2792.6121 - val_loss: 2968.1606\n",
      "Epoch 6/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2692.3730 - val_loss: 2873.6423\n",
      "Epoch 7/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 2611.6765 - val_loss: 2826.1545\n",
      "Epoch 8/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2541.3591 - val_loss: 2778.3828\n",
      "Epoch 9/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2486.9338 - val_loss: 2719.4036\n",
      "Epoch 10/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2438.6946 - val_loss: 2675.3250\n",
      "Epoch 11/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2395.5674 - val_loss: 2641.4353\n",
      "Epoch 12/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2359.1226 - val_loss: 2610.5029\n",
      "Epoch 13/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2327.3428 - val_loss: 2584.8777\n",
      "Epoch 14/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2296.8733 - val_loss: 2559.2476\n",
      "Epoch 15/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 2273.1074 - val_loss: 2545.6536\n",
      "Epoch 16/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2247.1016 - val_loss: 2562.8052\n",
      "Epoch 17/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2227.0815 - val_loss: 2510.4270\n",
      "Epoch 18/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2206.8308 - val_loss: 2494.4338\n",
      "Epoch 19/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2187.8093 - val_loss: 2495.4082\n",
      "Epoch 20/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2172.6975 - val_loss: 2475.9890\n",
      "Epoch 21/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 2155.9915 - val_loss: 2473.0862\n",
      "Epoch 22/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 2142.6938 - val_loss: 2451.1008\n",
      "Epoch 23/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2128.1392 - val_loss: 2446.7324\n",
      "Epoch 24/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2116.2424 - val_loss: 2428.6538\n",
      "Epoch 25/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2102.2283 - val_loss: 2449.5217\n",
      "Epoch 26/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2090.5820 - val_loss: 2420.7415\n",
      "Epoch 27/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2080.2656 - val_loss: 2403.6245\n",
      "Epoch 28/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2068.8481 - val_loss: 2401.7029\n",
      "Epoch 29/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2058.8240 - val_loss: 2396.5127\n",
      "Epoch 30/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 2049.0405 - val_loss: 2390.3777\n",
      "Epoch 31/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2038.4249 - val_loss: 2408.2478\n",
      "Epoch 32/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2031.2964 - val_loss: 2386.1663\n",
      "Epoch 33/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2021.2933 - val_loss: 2373.6626\n",
      "Epoch 34/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2014.0994 - val_loss: 2380.5574\n",
      "Epoch 35/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2006.1372 - val_loss: 2378.1450\n",
      "Epoch 36/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1998.8038 - val_loss: 2364.5618\n",
      "Epoch 37/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1989.7690 - val_loss: 2361.3623\n",
      "Epoch 38/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1984.6967 - val_loss: 2356.7847\n",
      "Epoch 39/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1980.0675 - val_loss: 2352.7217\n",
      "Epoch 40/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1972.1367 - val_loss: 2370.4514\n",
      "Epoch 41/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1966.2468 - val_loss: 2378.1572\n",
      "Epoch 42/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1960.7859 - val_loss: 2350.5078\n",
      "Epoch 43/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1953.9469 - val_loss: 2346.2935\n",
      "Epoch 44/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1949.1650 - val_loss: 2348.0918\n",
      "Epoch 45/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1944.7203 - val_loss: 2340.9263\n",
      "Epoch 46/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1939.0729 - val_loss: 2352.3223\n",
      "Epoch 47/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1932.6136 - val_loss: 2338.1055\n",
      "Epoch 48/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1929.2832 - val_loss: 2334.5422\n",
      "Epoch 49/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1923.7788 - val_loss: 2327.6709\n",
      "Epoch 50/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1916.9003 - val_loss: 2334.2520\n",
      "Epoch 51/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1914.4138 - val_loss: 2325.9229\n",
      "Epoch 52/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1907.5443 - val_loss: 2335.5732\n",
      "Epoch 53/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1905.6528 - val_loss: 2323.8560\n",
      "Epoch 54/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1900.7990 - val_loss: 2325.8489\n",
      "Epoch 55/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1896.4163 - val_loss: 2322.4185\n",
      "Epoch 56/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1890.7240 - val_loss: 2322.2837\n",
      "Epoch 57/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1887.9102 - val_loss: 2330.3601\n",
      "Epoch 58/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1883.7531 - val_loss: 2325.3289\n",
      "Epoch 59/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1879.4412 - val_loss: 2316.8059\n",
      "Epoch 60/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1876.6486 - val_loss: 2318.1772\n",
      "Epoch 61/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1872.8036 - val_loss: 2320.2576\n",
      "Epoch 62/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1869.9434 - val_loss: 2321.3933\n",
      "Epoch 63/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1866.2618 - val_loss: 2314.1399\n",
      "Epoch 64/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1861.5675 - val_loss: 2326.9839\n",
      "Epoch 65/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1859.4421 - val_loss: 2316.5652\n",
      "Epoch 66/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 1855.3300 - val_loss: 2313.0474\n",
      "Epoch 67/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1852.4928 - val_loss: 2316.7422\n",
      "Epoch 68/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1850.7312 - val_loss: 2313.9890\n",
      "Epoch 69/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1847.5707 - val_loss: 2314.0771\n",
      "Epoch 70/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1845.1121 - val_loss: 2314.4478\n",
      "Epoch 71/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1842.0984 - val_loss: 2312.5144\n",
      "Epoch 72/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1836.7571 - val_loss: 2313.8022\n",
      "Epoch 73/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1834.0229 - val_loss: 2305.7239\n",
      "Epoch 74/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1833.7181 - val_loss: 2311.3071\n",
      "Epoch 75/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1828.6903 - val_loss: 2315.7583\n",
      "Epoch 76/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1827.2975 - val_loss: 2314.9863\n",
      "Epoch 77/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 1824.0675 - val_loss: 2302.3953\n",
      "Epoch 78/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1821.4006 - val_loss: 2306.8257\n",
      "Epoch 79/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1818.8763 - val_loss: 2319.5422\n",
      "Epoch 80/200\n",
      "498/498 [==============================] - 5s 10ms/step - loss: 1816.5474 - val_loss: 2304.9712\n",
      "Epoch 81/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1813.6172 - val_loss: 2310.2490\n",
      "Epoch 82/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1812.1260 - val_loss: 2305.7363\n",
      "Epoch 83/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1809.6862 - val_loss: 2302.2163\n",
      "Epoch 84/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1806.2527 - val_loss: 2306.2192\n",
      "Epoch 85/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1802.7365 - val_loss: 2309.9175\n",
      "Epoch 86/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1801.7502 - val_loss: 2306.9780\n",
      "Epoch 87/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1798.8031 - val_loss: 2306.1289\n",
      "Epoch 88/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1797.2795 - val_loss: 2312.1155\n",
      "Epoch 89/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1795.5406 - val_loss: 2317.8938\n",
      "Epoch 90/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1791.8796 - val_loss: 2313.2334\n",
      "Epoch 91/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1789.8499 - val_loss: 2298.4307\n",
      "Epoch 92/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1787.5776 - val_loss: 2310.2383\n",
      "Epoch 93/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1786.6086 - val_loss: 2304.6165\n",
      "Epoch 94/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1784.2056 - val_loss: 2304.1504\n",
      "Epoch 95/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1782.2218 - val_loss: 2301.5454\n",
      "Epoch 96/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1780.0027 - val_loss: 2299.8887\n",
      "Epoch 97/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1777.2311 - val_loss: 2300.6572\n",
      "Epoch 98/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1774.9384 - val_loss: 2295.5608\n",
      "Epoch 99/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1774.1002 - val_loss: 2297.5779\n",
      "Epoch 100/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1771.2102 - val_loss: 2296.1641\n",
      "Epoch 101/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1769.8658 - val_loss: 2298.1697\n",
      "Epoch 102/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1767.2339 - val_loss: 2299.5098\n",
      "Epoch 103/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1764.8280 - val_loss: 2294.8950\n",
      "Epoch 104/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1762.7413 - val_loss: 2294.9314\n",
      "Epoch 105/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1760.9845 - val_loss: 2306.5537\n",
      "Epoch 106/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1759.6309 - val_loss: 2294.9849\n",
      "Epoch 107/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1757.9634 - val_loss: 2301.5417\n",
      "Epoch 108/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1754.4570 - val_loss: 2299.4465\n",
      "Epoch 109/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1754.2628 - val_loss: 2302.8401\n",
      "Epoch 110/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1751.7607 - val_loss: 2301.3113\n",
      "Epoch 111/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1750.5199 - val_loss: 2298.9712\n",
      "Epoch 112/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1746.8740 - val_loss: 2310.2334\n",
      "Epoch 113/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1746.9778 - val_loss: 2297.5254\n",
      "Epoch 114/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 1743.8251 - val_loss: 2301.6687\n",
      "Epoch 115/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1742.9550 - val_loss: 2302.4675\n",
      "Epoch 116/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1740.4384 - val_loss: 2295.4631\n",
      "Epoch 117/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1737.9135 - val_loss: 2299.1189\n",
      "Epoch 118/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 1736.2532 - val_loss: 2300.5083\n",
      "Epoch 119/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1736.4238 - val_loss: 2296.5388\n",
      "Epoch 120/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1734.1964 - val_loss: 2296.3950\n",
      "Epoch 121/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1731.8879 - val_loss: 2301.4670\n",
      "Epoch 122/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1731.0104 - val_loss: 2296.1108\n",
      "Epoch 123/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1729.6562 - val_loss: 2294.4534\n",
      "Epoch 124/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1728.3362 - val_loss: 2292.3206\n",
      "Epoch 125/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1726.9882 - val_loss: 2287.9382\n",
      "Epoch 126/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1724.4081 - val_loss: 2291.0110\n",
      "Epoch 127/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1724.2050 - val_loss: 2290.7898\n",
      "Epoch 128/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1722.3190 - val_loss: 2296.8557\n",
      "Epoch 129/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1721.3179 - val_loss: 2291.0740\n",
      "Epoch 130/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1719.4196 - val_loss: 2289.4155\n",
      "Epoch 131/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1718.0433 - val_loss: 2298.9846\n",
      "Epoch 132/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1716.2213 - val_loss: 2290.4653\n",
      "Epoch 133/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1714.2161 - val_loss: 2294.9250\n",
      "Epoch 134/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 1713.6023 - val_loss: 2289.8137\n",
      "Epoch 135/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 1711.4275 - val_loss: 2293.4773\n",
      "Epoch 136/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1710.7386 - val_loss: 2283.4548\n",
      "Epoch 137/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1709.6063 - val_loss: 2290.1577\n",
      "Epoch 138/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1708.5804 - val_loss: 2286.6367\n",
      "Epoch 139/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1706.4932 - val_loss: 2289.9363\n",
      "Epoch 140/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1704.2277 - val_loss: 2281.9053\n",
      "Epoch 141/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1702.7605 - val_loss: 2284.7004\n",
      "Epoch 142/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1701.7209 - val_loss: 2286.9497\n",
      "Epoch 143/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1700.5382 - val_loss: 2286.3679\n",
      "Epoch 144/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1699.6350 - val_loss: 2280.6028\n",
      "Epoch 145/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1697.6215 - val_loss: 2287.2759\n",
      "Epoch 146/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1696.8795 - val_loss: 2283.8206\n",
      "Epoch 147/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1695.4631 - val_loss: 2281.3430\n",
      "Epoch 148/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1693.8651 - val_loss: 2281.8250\n",
      "Epoch 149/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1692.5385 - val_loss: 2278.1755\n",
      "Epoch 150/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1691.5232 - val_loss: 2280.5671\n",
      "Epoch 151/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1690.2694 - val_loss: 2283.3376\n",
      "Epoch 152/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1690.2902 - val_loss: 2280.3337\n",
      "Epoch 153/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1687.3885 - val_loss: 2282.5581\n",
      "Epoch 154/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 1687.1868 - val_loss: 2285.3894\n",
      "Epoch 155/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1684.4110 - val_loss: 2286.1289\n",
      "Epoch 156/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1684.8177 - val_loss: 2279.0359\n",
      "Epoch 157/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1682.3251 - val_loss: 2293.0005\n",
      "Epoch 158/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1682.7024 - val_loss: 2282.7271\n",
      "Epoch 159/200\n",
      "498/498 [==============================] - 2s 3ms/step - loss: 1680.6233 - val_loss: 2279.5183\n",
      "Epoch 160/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1680.3582 - val_loss: 2283.5771\n",
      "Epoch 161/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1678.8723 - val_loss: 2289.8206\n",
      "Epoch 162/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1677.9258 - val_loss: 2285.8103\n",
      "Epoch 163/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1676.5382 - val_loss: 2278.9277\n",
      "Epoch 164/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1675.1738 - val_loss: 2284.3418\n",
      "Epoch 165/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1673.2845 - val_loss: 2280.7317\n",
      "Epoch 166/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1672.9031 - val_loss: 2281.6672\n",
      "Epoch 167/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1671.7281 - val_loss: 2279.5503\n",
      "Epoch 168/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1669.8217 - val_loss: 2278.0024\n",
      "Epoch 169/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1669.2484 - val_loss: 2286.2632\n",
      "Epoch 170/200\n",
      "498/498 [==============================] - 3s 6ms/step - loss: 1668.9008 - val_loss: 2280.0090\n",
      "Epoch 171/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1667.0420 - val_loss: 2280.2705\n",
      "Epoch 172/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1665.9454 - val_loss: 2279.6653\n",
      "Epoch 173/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1664.9434 - val_loss: 2284.0020\n",
      "Epoch 174/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1664.5295 - val_loss: 2286.9504\n",
      "Epoch 175/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1662.0312 - val_loss: 2285.5332\n",
      "Epoch 176/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1662.1320 - val_loss: 2278.7671\n",
      "Epoch 177/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1660.8584 - val_loss: 2282.9048\n",
      "Epoch 178/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1659.9485 - val_loss: 2284.4001\n",
      "Epoch 179/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1659.1650 - val_loss: 2275.8188\n",
      "Epoch 180/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1658.6434 - val_loss: 2278.7781\n",
      "Epoch 181/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1656.0217 - val_loss: 2288.4980\n",
      "Epoch 182/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1656.5947 - val_loss: 2280.5215\n",
      "Epoch 183/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1655.0405 - val_loss: 2277.4741\n",
      "Epoch 184/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1654.2499 - val_loss: 2286.8247\n",
      "Epoch 185/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1653.5629 - val_loss: 2279.2661\n",
      "Epoch 186/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1652.2142 - val_loss: 2282.2095\n",
      "Epoch 187/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1651.5646 - val_loss: 2284.1038\n",
      "Epoch 188/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1650.1080 - val_loss: 2288.7886\n",
      "Epoch 189/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1648.8123 - val_loss: 2282.1370\n",
      "Epoch 190/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1648.5562 - val_loss: 2286.5129\n",
      "Epoch 191/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1647.6415 - val_loss: 2283.9104\n",
      "Epoch 192/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1646.7031 - val_loss: 2280.1860\n",
      "Epoch 193/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1644.4646 - val_loss: 2283.7083\n",
      "Epoch 194/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1644.3275 - val_loss: 2278.2634\n",
      "Epoch 195/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1642.8644 - val_loss: 2277.0593\n",
      "Epoch 196/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1641.6133 - val_loss: 2281.2178\n",
      "Epoch 197/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1642.3762 - val_loss: 2284.1680\n",
      "Epoch 198/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1640.9169 - val_loss: 2280.1707\n",
      "Epoch 199/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1640.0784 - val_loss: 2282.0554\n",
      "Epoch 200/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1638.2245 - val_loss: 2288.7734\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2288.7734\n",
      "RMSE: 47.84112705089628\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad\n",
    "\n",
    "# Choisissez l'optimiseur que vous souhaitez utiliser\n",
    "# optimizer = SGD(learning_rate=0.01)  # Stochastic Gradient Descent\n",
    "# optimizer = RMSprop(learning_rate=0.01)  # RMSprop\n",
    "optimizer = Adagrad(learning_rate=0.01)  # Adagrad\n",
    "\n",
    "# Crez votre modle de rgression\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle avec l'optimiseur choisi\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "rmse = np.sqrt(loss)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00a93e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "498/498 [==============================] - 3s 4ms/step - loss: 4047.8333 - val_loss: 2893.4678\n",
      "Epoch 2/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2930.9836 - val_loss: 3036.1775\n",
      "Epoch 3/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2645.3828 - val_loss: 2802.2395\n",
      "Epoch 4/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2464.6372 - val_loss: 2605.3848\n",
      "Epoch 5/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2353.8926 - val_loss: 3243.0496\n",
      "Epoch 6/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2267.4326 - val_loss: 3120.6077\n",
      "Epoch 7/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2230.1331 - val_loss: 2296.7722\n",
      "Epoch 8/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2132.8857 - val_loss: 2272.1550\n",
      "Epoch 9/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2100.4126 - val_loss: 2276.1553\n",
      "Epoch 10/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2038.7722 - val_loss: 2396.9014\n",
      "Epoch 11/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 2017.3972 - val_loss: 2784.9697\n",
      "Epoch 12/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1966.4160 - val_loss: 2207.8936\n",
      "Epoch 13/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1940.4747 - val_loss: 2409.3123\n",
      "Epoch 14/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1918.7091 - val_loss: 2326.2749\n",
      "Epoch 15/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1899.4586 - val_loss: 2281.7900\n",
      "Epoch 16/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1866.3344 - val_loss: 2267.2134\n",
      "Epoch 17/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1847.8685 - val_loss: 2205.0000\n",
      "Epoch 18/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1828.8193 - val_loss: 2153.0874\n",
      "Epoch 19/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1810.4093 - val_loss: 2320.8647\n",
      "Epoch 20/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1789.8290 - val_loss: 2268.2229\n",
      "Epoch 21/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1753.2156 - val_loss: 2254.1809\n",
      "Epoch 22/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1753.5089 - val_loss: 2261.4387\n",
      "Epoch 23/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1731.9238 - val_loss: 2374.2451\n",
      "Epoch 24/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1701.4396 - val_loss: 2231.8088\n",
      "Epoch 25/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1691.6439 - val_loss: 2401.0535\n",
      "Epoch 26/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1679.8632 - val_loss: 2346.8862\n",
      "Epoch 27/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1690.2397 - val_loss: 2544.4688\n",
      "Epoch 28/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1671.1171 - val_loss: 2159.3896\n",
      "Epoch 29/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1667.0099 - val_loss: 2188.3931\n",
      "Epoch 30/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1645.3000 - val_loss: 2250.2329\n",
      "Epoch 31/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1645.8411 - val_loss: 2255.8132\n",
      "Epoch 32/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1631.5551 - val_loss: 2159.0000\n",
      "Epoch 33/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1632.2700 - val_loss: 2161.4358\n",
      "Epoch 34/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1618.3108 - val_loss: 2224.8152\n",
      "Epoch 35/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1600.3378 - val_loss: 2181.4695\n",
      "Epoch 36/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1588.2900 - val_loss: 2274.6484\n",
      "Epoch 37/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1586.7761 - val_loss: 2190.3677\n",
      "Epoch 38/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1586.4006 - val_loss: 2200.4385\n",
      "Epoch 39/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1584.1553 - val_loss: 2238.9612\n",
      "Epoch 40/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1558.3960 - val_loss: 2463.5903\n",
      "Epoch 41/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1551.7437 - val_loss: 2396.2810\n",
      "Epoch 42/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1542.4773 - val_loss: 2371.1265\n",
      "Epoch 43/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1538.1082 - val_loss: 2470.7954\n",
      "Epoch 44/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1525.5391 - val_loss: 2241.1401\n",
      "Epoch 45/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1520.4003 - val_loss: 2327.9509\n",
      "Epoch 46/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1517.9243 - val_loss: 2192.0388\n",
      "Epoch 47/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1505.0295 - val_loss: 2319.3784\n",
      "Epoch 48/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1490.7382 - val_loss: 2404.0588\n",
      "Epoch 49/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1500.4954 - val_loss: 2255.9087\n",
      "Epoch 50/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1493.8313 - val_loss: 2176.2615\n",
      "Epoch 51/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1485.5211 - val_loss: 2173.3877\n",
      "Epoch 52/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1471.6309 - val_loss: 2214.4570\n",
      "Epoch 53/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1469.1639 - val_loss: 2158.1492\n",
      "Epoch 54/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1453.9634 - val_loss: 2226.5007\n",
      "Epoch 55/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1455.1771 - val_loss: 2251.4934\n",
      "Epoch 56/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1450.5947 - val_loss: 2309.6042\n",
      "Epoch 57/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1439.9962 - val_loss: 2242.4248\n",
      "Epoch 58/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1429.7656 - val_loss: 2184.7061\n",
      "Epoch 59/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1433.7634 - val_loss: 2292.3486\n",
      "Epoch 60/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1441.9402 - val_loss: 2239.0879\n",
      "Epoch 61/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1422.2861 - val_loss: 2236.7812\n",
      "Epoch 62/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1422.8162 - val_loss: 3061.3682\n",
      "Epoch 63/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1412.5634 - val_loss: 2264.7544\n",
      "Epoch 64/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1403.6416 - val_loss: 2240.5344\n",
      "Epoch 65/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1398.1846 - val_loss: 2322.0869\n",
      "Epoch 66/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1402.6063 - val_loss: 2223.2795\n",
      "Epoch 67/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1393.2825 - val_loss: 2308.8655\n",
      "Epoch 68/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1392.9796 - val_loss: 2193.0889\n",
      "Epoch 69/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1389.9067 - val_loss: 2274.5186\n",
      "Epoch 70/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1386.5309 - val_loss: 2394.8157\n",
      "Epoch 71/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1387.2638 - val_loss: 2256.8938\n",
      "Epoch 72/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1370.9746 - val_loss: 2206.2656\n",
      "Epoch 73/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1391.8345 - val_loss: 2210.8877\n",
      "Epoch 74/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1358.1833 - val_loss: 2224.4238\n",
      "Epoch 75/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1363.5824 - val_loss: 2203.6206\n",
      "Epoch 76/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1355.9042 - val_loss: 2250.1130\n",
      "Epoch 77/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1350.5166 - val_loss: 2356.2273\n",
      "Epoch 78/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1343.5537 - val_loss: 2238.5947\n",
      "Epoch 79/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1339.3990 - val_loss: 2212.5747\n",
      "Epoch 80/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1340.6954 - val_loss: 2932.7190\n",
      "Epoch 81/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1332.2423 - val_loss: 2231.0737\n",
      "Epoch 82/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1330.6260 - val_loss: 2400.2471\n",
      "Epoch 83/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1332.8719 - val_loss: 2324.1433\n",
      "Epoch 84/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1328.8159 - val_loss: 2206.1108\n",
      "Epoch 85/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1321.1024 - val_loss: 2257.4512\n",
      "Epoch 86/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1340.4569 - val_loss: 2243.2378\n",
      "Epoch 87/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1317.5317 - val_loss: 2243.6143\n",
      "Epoch 88/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1305.5010 - val_loss: 2311.5867\n",
      "Epoch 89/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1311.8541 - val_loss: 2355.3750\n",
      "Epoch 90/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1318.1433 - val_loss: 2365.4255\n",
      "Epoch 91/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1304.1125 - val_loss: 2266.0828\n",
      "Epoch 92/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1315.3604 - val_loss: 2208.9414\n",
      "Epoch 93/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1286.1010 - val_loss: 2406.1887\n",
      "Epoch 94/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1303.3285 - val_loss: 2270.0156\n",
      "Epoch 95/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1294.8151 - val_loss: 2287.9768\n",
      "Epoch 96/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1292.8893 - val_loss: 2394.0698\n",
      "Epoch 97/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1277.4596 - val_loss: 2385.0298\n",
      "Epoch 98/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1300.3105 - val_loss: 2409.4460\n",
      "Epoch 99/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1282.3044 - val_loss: 2366.1758\n",
      "Epoch 100/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1270.9340 - val_loss: 2268.1143\n",
      "Epoch 101/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1275.9580 - val_loss: 2462.6423\n",
      "Epoch 102/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1274.0237 - val_loss: 2281.0703\n",
      "Epoch 103/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1269.9049 - val_loss: 2285.3735\n",
      "Epoch 104/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1263.6538 - val_loss: 2243.0181\n",
      "Epoch 105/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1272.6333 - val_loss: 2265.1104\n",
      "Epoch 106/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1268.7264 - val_loss: 2308.8086\n",
      "Epoch 107/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1264.7578 - val_loss: 2279.4644\n",
      "Epoch 108/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1257.8284 - val_loss: 2255.7136\n",
      "Epoch 109/200\n",
      "498/498 [==============================] - 2s 5ms/step - loss: 1250.6221 - val_loss: 2272.3745\n",
      "Epoch 110/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1254.6854 - val_loss: 2341.9805\n",
      "Epoch 111/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1261.4969 - val_loss: 2728.3914\n",
      "Epoch 112/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1242.3453 - val_loss: 2315.1760\n",
      "Epoch 113/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1252.6981 - val_loss: 2279.9915\n",
      "Epoch 114/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1235.4280 - val_loss: 2320.8086\n",
      "Epoch 115/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1291.2952 - val_loss: 2294.4880\n",
      "Epoch 116/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1254.6837 - val_loss: 2352.1257\n",
      "Epoch 117/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1242.9574 - val_loss: 2277.2314\n",
      "Epoch 118/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1232.3525 - val_loss: 2314.1846\n",
      "Epoch 119/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1230.0771 - val_loss: 2813.9458\n",
      "Epoch 120/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1234.5582 - val_loss: 2412.9709\n",
      "Epoch 121/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1230.0782 - val_loss: 2351.9863\n",
      "Epoch 122/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1246.7079 - val_loss: 2470.0342\n",
      "Epoch 123/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1232.4189 - val_loss: 2333.2239\n",
      "Epoch 124/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1235.5165 - val_loss: 2375.5449\n",
      "Epoch 125/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1235.1951 - val_loss: 2586.4089\n",
      "Epoch 126/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1225.0654 - val_loss: 2257.2319\n",
      "Epoch 127/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1250.3857 - val_loss: 2297.3064\n",
      "Epoch 128/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1227.2511 - val_loss: 2360.0300\n",
      "Epoch 129/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1224.9353 - val_loss: 2363.1477\n",
      "Epoch 130/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1215.8593 - val_loss: 2410.3950\n",
      "Epoch 131/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1214.3154 - val_loss: 2331.8550\n",
      "Epoch 132/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1217.8601 - val_loss: 2660.7805\n",
      "Epoch 133/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1216.6309 - val_loss: 2443.5361\n",
      "Epoch 134/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1220.7235 - val_loss: 2347.9663\n",
      "Epoch 135/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1219.2704 - val_loss: 2371.3848\n",
      "Epoch 136/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1207.1370 - val_loss: 2331.9658\n",
      "Epoch 137/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1200.7014 - val_loss: 2374.0601\n",
      "Epoch 138/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1195.8569 - val_loss: 2363.2773\n",
      "Epoch 139/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1190.3201 - val_loss: 2401.1885\n",
      "Epoch 140/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1186.4298 - val_loss: 2558.0444\n",
      "Epoch 141/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1195.0864 - val_loss: 2683.0532\n",
      "Epoch 142/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1199.6022 - val_loss: 2426.8889\n",
      "Epoch 143/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1192.4261 - val_loss: 2318.4280\n",
      "Epoch 144/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1183.2968 - val_loss: 3218.5994\n",
      "Epoch 145/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1204.1213 - val_loss: 2309.4409\n",
      "Epoch 146/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1195.6609 - val_loss: 2280.0981\n",
      "Epoch 147/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1189.9575 - val_loss: 2376.4343\n",
      "Epoch 148/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1195.8146 - val_loss: 2336.7720\n",
      "Epoch 149/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1186.3384 - val_loss: 2337.9014\n",
      "Epoch 150/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1176.9147 - val_loss: 2326.3965\n",
      "Epoch 151/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1191.7686 - val_loss: 2350.4878\n",
      "Epoch 152/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1183.3082 - val_loss: 2323.0234\n",
      "Epoch 153/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1168.0402 - val_loss: 2403.4060\n",
      "Epoch 154/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1174.4856 - val_loss: 2336.6282\n",
      "Epoch 155/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1168.7188 - val_loss: 2604.4639\n",
      "Epoch 156/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1163.9675 - val_loss: 2414.0671\n",
      "Epoch 157/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1170.2216 - val_loss: 2343.7778\n",
      "Epoch 158/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1190.2695 - val_loss: 2387.7661\n",
      "Epoch 159/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1159.3812 - val_loss: 2361.4395\n",
      "Epoch 160/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1170.7161 - val_loss: 2435.6106\n",
      "Epoch 161/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1175.5088 - val_loss: 2658.8430\n",
      "Epoch 162/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1157.2538 - val_loss: 2489.5571\n",
      "Epoch 163/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1170.5947 - val_loss: 2509.8577\n",
      "Epoch 164/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1158.9210 - val_loss: 2348.3806\n",
      "Epoch 165/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1156.3384 - val_loss: 2362.6816\n",
      "Epoch 166/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1150.3285 - val_loss: 2409.7251\n",
      "Epoch 167/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1152.5813 - val_loss: 2344.8018\n",
      "Epoch 168/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1163.1809 - val_loss: 2405.3848\n",
      "Epoch 169/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1166.6906 - val_loss: 2407.6128\n",
      "Epoch 170/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1158.8655 - val_loss: 2681.6487\n",
      "Epoch 171/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1144.0613 - val_loss: 2342.7649\n",
      "Epoch 172/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1143.9882 - val_loss: 2949.8301\n",
      "Epoch 173/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1170.8489 - val_loss: 2443.8999\n",
      "Epoch 174/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1151.8363 - val_loss: 2379.1433\n",
      "Epoch 175/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1143.1788 - val_loss: 2645.6003\n",
      "Epoch 176/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1135.4554 - val_loss: 2416.8123\n",
      "Epoch 177/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1135.1415 - val_loss: 2429.1021\n",
      "Epoch 178/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1159.2771 - val_loss: 2496.6956\n",
      "Epoch 179/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1142.7755 - val_loss: 2344.0679\n",
      "Epoch 180/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1145.5962 - val_loss: 2501.8489\n",
      "Epoch 181/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1135.9490 - val_loss: 2472.6589\n",
      "Epoch 182/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1134.7780 - val_loss: 2521.6355\n",
      "Epoch 183/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1144.0391 - val_loss: 2434.1882\n",
      "Epoch 184/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1138.2606 - val_loss: 2695.9836\n",
      "Epoch 185/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1139.8800 - val_loss: 2500.1621\n",
      "Epoch 186/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1150.7197 - val_loss: 2591.1387\n",
      "Epoch 187/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1138.0413 - val_loss: 2411.2354\n",
      "Epoch 188/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1129.5118 - val_loss: 2454.5371\n",
      "Epoch 189/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1355.7484 - val_loss: 2369.9280\n",
      "Epoch 190/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1183.0343 - val_loss: 2411.8123\n",
      "Epoch 191/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1145.8468 - val_loss: 2450.0527\n",
      "Epoch 192/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1131.7983 - val_loss: 2497.7043\n",
      "Epoch 193/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1130.8394 - val_loss: 2583.2329\n",
      "Epoch 194/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1122.9736 - val_loss: 2443.8389\n",
      "Epoch 195/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1131.7750 - val_loss: 2377.4097\n",
      "Epoch 196/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1121.2784 - val_loss: 2675.1724\n",
      "Epoch 197/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1116.3416 - val_loss: 2405.2468\n",
      "Epoch 198/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1123.4559 - val_loss: 2415.5769\n",
      "Epoch 199/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1138.6564 - val_loss: 2703.8933\n",
      "Epoch 200/200\n",
      "498/498 [==============================] - 2s 4ms/step - loss: 1109.8303 - val_loss: 2423.0740\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 2423.0740\n",
      "RMSE: 49.22472929950326\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad\n",
    "\n",
    "# Choisissez l'optimiseur que vous souhaitez utiliser\n",
    "# optimizer = SGD(learning_rate=0.01)  # Stochastic Gradient Descent\n",
    "optimizer = RMSprop(learning_rate=0.01)  # RMSprop\n",
    "\n",
    "\n",
    "# Crez votre modle de rgression\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle avec l'optimiseur choisi\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=200, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "rmse = np.sqrt(loss)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "343546fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 18793.9727 - val_loss: 18446.8555\n",
      "Epoch 2/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 17299.2285 - val_loss: 16579.7676\n",
      "Epoch 3/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 15211.9199 - val_loss: 14344.5596\n",
      "Epoch 4/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 12976.0957 - val_loss: 12191.9766\n",
      "Epoch 5/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 10973.5898 - val_loss: 10441.6230\n",
      "Epoch 6/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 9438.0830 - val_loss: 9209.6846\n",
      "Epoch 7/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 8394.2773 - val_loss: 8422.1631\n",
      "Epoch 8/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 7733.0425 - val_loss: 7933.2891\n",
      "Epoch 9/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 7311.0508 - val_loss: 7615.2847\n",
      "Epoch 10/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 7028.6470 - val_loss: 7386.9097\n",
      "Epoch 11/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 6819.6094 - val_loss: 7209.1821\n",
      "Epoch 12/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 6656.3921 - val_loss: 7061.3486\n",
      "Epoch 13/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 6521.6323 - val_loss: 6934.9165\n",
      "Epoch 14/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 6406.6948 - val_loss: 6825.0200\n",
      "Epoch 15/1000\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 6307.6348 - val_loss: 6726.8325\n",
      "Epoch 16/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 6220.0522 - val_loss: 6637.7275\n",
      "Epoch 17/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 6142.2734 - val_loss: 6559.1650\n",
      "Epoch 18/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 6072.3477 - val_loss: 6487.0869\n",
      "Epoch 19/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 6008.6221 - val_loss: 6421.2046\n",
      "Epoch 20/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5950.7544 - val_loss: 6358.3516\n",
      "Epoch 21/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5897.2656 - val_loss: 6301.0332\n",
      "Epoch 22/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5848.1113 - val_loss: 6248.9614\n",
      "Epoch 23/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5802.5869 - val_loss: 6199.9302\n",
      "Epoch 24/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5760.1426 - val_loss: 6154.1245\n",
      "Epoch 25/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5720.1499 - val_loss: 6109.3027\n",
      "Epoch 26/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5682.3364 - val_loss: 6068.3081\n",
      "Epoch 27/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5647.2935 - val_loss: 6029.9946\n",
      "Epoch 28/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5613.7500 - val_loss: 5992.7588\n",
      "Epoch 29/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 5581.6567 - val_loss: 5958.6240\n",
      "Epoch 30/1000\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 5551.5767 - val_loss: 5924.8818\n",
      "Epoch 31/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5522.1616 - val_loss: 5893.7529\n",
      "Epoch 32/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5494.5361 - val_loss: 5862.7441\n",
      "Epoch 33/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5467.7251 - val_loss: 5833.2573\n",
      "Epoch 34/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5441.9175 - val_loss: 5805.0210\n",
      "Epoch 35/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5417.1421 - val_loss: 5776.8975\n",
      "Epoch 36/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5392.8740 - val_loss: 5749.3174\n",
      "Epoch 37/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 5369.1509 - val_loss: 5723.1729\n",
      "Epoch 38/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5346.2334 - val_loss: 5698.1040\n",
      "Epoch 39/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5323.7896 - val_loss: 5673.5503\n",
      "Epoch 40/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5302.2241 - val_loss: 5649.3340\n",
      "Epoch 41/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 5281.1797 - val_loss: 5625.7759\n",
      "Epoch 42/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 5260.6816 - val_loss: 5602.5063\n",
      "Epoch 43/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5240.5396 - val_loss: 5580.0283\n",
      "Epoch 44/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5221.0195 - val_loss: 5557.9087\n",
      "Epoch 45/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5201.7725 - val_loss: 5536.6455\n",
      "Epoch 46/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5182.6436 - val_loss: 5515.0923\n",
      "Epoch 47/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5164.0161 - val_loss: 5494.2217\n",
      "Epoch 48/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5145.5615 - val_loss: 5474.0068\n",
      "Epoch 49/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5127.3364 - val_loss: 5453.5454\n",
      "Epoch 50/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5109.3037 - val_loss: 5433.1304\n",
      "Epoch 51/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 5091.2954 - val_loss: 5413.1104\n",
      "Epoch 52/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5073.6074 - val_loss: 5393.1323\n",
      "Epoch 53/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5056.1699 - val_loss: 5373.6040\n",
      "Epoch 54/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5038.6587 - val_loss: 5354.1099\n",
      "Epoch 55/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 5021.6025 - val_loss: 5334.9971\n",
      "Epoch 56/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 5004.5356 - val_loss: 5316.6113\n",
      "Epoch 57/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 4987.9644 - val_loss: 5297.9478\n",
      "Epoch 58/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4971.5869 - val_loss: 5279.5781\n",
      "Epoch 59/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4955.3564 - val_loss: 5261.2837\n",
      "Epoch 60/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4939.3081 - val_loss: 5243.6406\n",
      "Epoch 61/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4923.4717 - val_loss: 5225.9575\n",
      "Epoch 62/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4907.7153 - val_loss: 5209.0703\n",
      "Epoch 63/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4892.2178 - val_loss: 5192.1230\n",
      "Epoch 64/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4876.8237 - val_loss: 5175.2261\n",
      "Epoch 65/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4861.5615 - val_loss: 5158.4146\n",
      "Epoch 66/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4846.3169 - val_loss: 5141.8521\n",
      "Epoch 67/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4831.3257 - val_loss: 5125.2231\n",
      "Epoch 68/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4816.3584 - val_loss: 5108.6318\n",
      "Epoch 69/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 4801.3545 - val_loss: 5091.6533\n",
      "Epoch 70/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4786.3447 - val_loss: 5074.6689\n",
      "Epoch 71/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4771.2612 - val_loss: 5057.5635\n",
      "Epoch 72/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4756.2251 - val_loss: 5040.8911\n",
      "Epoch 73/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4741.1021 - val_loss: 5024.3223\n",
      "Epoch 74/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 4725.8608 - val_loss: 5006.8599\n",
      "Epoch 75/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 4710.2607 - val_loss: 4989.9468\n",
      "Epoch 76/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4695.1016 - val_loss: 4972.8618\n",
      "Epoch 77/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4680.0103 - val_loss: 4955.7666\n",
      "Epoch 78/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4665.1558 - val_loss: 4939.5815\n",
      "Epoch 79/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4650.6201 - val_loss: 4923.6772\n",
      "Epoch 80/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4636.4380 - val_loss: 4908.4248\n",
      "Epoch 81/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4622.0645 - val_loss: 4892.8477\n",
      "Epoch 82/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4608.0693 - val_loss: 4877.5098\n",
      "Epoch 83/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 4594.0259 - val_loss: 4862.3711\n",
      "Epoch 84/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4580.0200 - val_loss: 4847.4028\n",
      "Epoch 85/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4566.2144 - val_loss: 4832.1807\n",
      "Epoch 86/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4552.2207 - val_loss: 4816.9897\n",
      "Epoch 87/1000\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 4538.2686 - val_loss: 4801.9570\n",
      "Epoch 88/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4524.1006 - val_loss: 4786.7900\n",
      "Epoch 89/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4510.4077 - val_loss: 4771.9502\n",
      "Epoch 90/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4496.8994 - val_loss: 4757.8076\n",
      "Epoch 91/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4483.7202 - val_loss: 4743.3687\n",
      "Epoch 92/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4470.5024 - val_loss: 4729.2964\n",
      "Epoch 93/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4457.3892 - val_loss: 4715.4614\n",
      "Epoch 94/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 4444.3696 - val_loss: 4701.5786\n",
      "Epoch 95/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4431.4771 - val_loss: 4687.6655\n",
      "Epoch 96/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4418.6582 - val_loss: 4674.0669\n",
      "Epoch 97/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4406.0063 - val_loss: 4660.5552\n",
      "Epoch 98/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4393.3306 - val_loss: 4646.8389\n",
      "Epoch 99/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4380.8130 - val_loss: 4633.7959\n",
      "Epoch 100/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4368.5737 - val_loss: 4620.8311\n",
      "Epoch 101/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4356.3052 - val_loss: 4607.9927\n",
      "Epoch 102/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4344.2554 - val_loss: 4595.1338\n",
      "Epoch 103/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4332.1392 - val_loss: 4582.2695\n",
      "Epoch 104/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4320.1494 - val_loss: 4569.9302\n",
      "Epoch 105/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4308.2363 - val_loss: 4557.0449\n",
      "Epoch 106/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 4296.3955 - val_loss: 4544.5288\n",
      "Epoch 107/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4284.6860 - val_loss: 4532.0063\n",
      "Epoch 108/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4272.7290 - val_loss: 4519.6738\n",
      "Epoch 109/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4261.0410 - val_loss: 4507.4224\n",
      "Epoch 110/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4249.2310 - val_loss: 4495.3877\n",
      "Epoch 111/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4237.7651 - val_loss: 4483.1143\n",
      "Epoch 112/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4226.3901 - val_loss: 4471.3359\n",
      "Epoch 113/1000\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 4215.1914 - val_loss: 4459.6484\n",
      "Epoch 114/1000\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 4203.9741 - val_loss: 4448.0020\n",
      "Epoch 115/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4192.8530 - val_loss: 4436.5327\n",
      "Epoch 116/1000\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 4181.9546 - val_loss: 4425.3994\n",
      "Epoch 117/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4171.0532 - val_loss: 4414.5244\n",
      "Epoch 118/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 4160.3105 - val_loss: 4403.6143\n",
      "Epoch 119/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4149.6904 - val_loss: 4392.8540\n",
      "Epoch 120/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4139.0205 - val_loss: 4381.7627\n",
      "Epoch 121/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4128.5254 - val_loss: 4370.5352\n",
      "Epoch 122/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4117.9321 - val_loss: 4360.1367\n",
      "Epoch 123/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4107.3999 - val_loss: 4349.4233\n",
      "Epoch 124/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4096.8550 - val_loss: 4338.8032\n",
      "Epoch 125/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4086.5349 - val_loss: 4328.5933\n",
      "Epoch 126/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4076.3528 - val_loss: 4318.6982\n",
      "Epoch 127/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4066.3159 - val_loss: 4308.7036\n",
      "Epoch 128/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 4056.3977 - val_loss: 4299.2046\n",
      "Epoch 129/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4046.6577 - val_loss: 4289.3579\n",
      "Epoch 130/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 4036.9019 - val_loss: 4279.8379\n",
      "Epoch 131/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 4027.2571 - val_loss: 4270.4287\n",
      "Epoch 132/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4017.6855 - val_loss: 4260.9365\n",
      "Epoch 133/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 4008.2310 - val_loss: 4251.8979\n",
      "Epoch 134/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3998.8743 - val_loss: 4242.3384\n",
      "Epoch 135/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3989.5674 - val_loss: 4233.4800\n",
      "Epoch 136/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3980.3840 - val_loss: 4224.7583\n",
      "Epoch 137/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3971.2358 - val_loss: 4216.1011\n",
      "Epoch 138/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3962.2600 - val_loss: 4207.7192\n",
      "Epoch 139/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3953.3369 - val_loss: 4199.1333\n",
      "Epoch 140/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3944.4812 - val_loss: 4191.0239\n",
      "Epoch 141/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3935.7898 - val_loss: 4182.6064\n",
      "Epoch 142/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3927.0896 - val_loss: 4174.4126\n",
      "Epoch 143/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3918.4978 - val_loss: 4166.4102\n",
      "Epoch 144/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3910.0056 - val_loss: 4158.3218\n",
      "Epoch 145/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3901.6187 - val_loss: 4150.4204\n",
      "Epoch 146/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3893.2646 - val_loss: 4142.5952\n",
      "Epoch 147/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3884.9675 - val_loss: 4134.7729\n",
      "Epoch 148/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3876.7271 - val_loss: 4127.1650\n",
      "Epoch 149/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3868.5020 - val_loss: 4119.7451\n",
      "Epoch 150/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3860.4751 - val_loss: 4112.3120\n",
      "Epoch 151/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3852.5552 - val_loss: 4104.9683\n",
      "Epoch 152/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3844.6609 - val_loss: 4097.7949\n",
      "Epoch 153/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3836.8621 - val_loss: 4090.7073\n",
      "Epoch 154/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3829.2202 - val_loss: 4083.5188\n",
      "Epoch 155/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3821.5393 - val_loss: 4076.6082\n",
      "Epoch 156/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3813.9978 - val_loss: 4069.7202\n",
      "Epoch 157/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3806.5088 - val_loss: 4063.1509\n",
      "Epoch 158/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3799.1968 - val_loss: 4056.4512\n",
      "Epoch 159/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3791.8540 - val_loss: 4049.9031\n",
      "Epoch 160/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3784.5940 - val_loss: 4043.4453\n",
      "Epoch 161/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3777.4390 - val_loss: 4036.7786\n",
      "Epoch 162/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3770.3459 - val_loss: 4030.2190\n",
      "Epoch 163/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3763.3857 - val_loss: 4024.0505\n",
      "Epoch 164/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3756.4163 - val_loss: 4017.8218\n",
      "Epoch 165/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3749.4312 - val_loss: 4011.5071\n",
      "Epoch 166/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3742.6975 - val_loss: 4005.4341\n",
      "Epoch 167/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3735.8860 - val_loss: 3999.5481\n",
      "Epoch 168/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3729.2534 - val_loss: 3993.4497\n",
      "Epoch 169/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3722.6638 - val_loss: 3987.5872\n",
      "Epoch 170/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3716.1350 - val_loss: 3981.7012\n",
      "Epoch 171/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3709.6921 - val_loss: 3976.0454\n",
      "Epoch 172/1000\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 3703.2671 - val_loss: 3970.4929\n",
      "Epoch 173/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3696.9126 - val_loss: 3964.7429\n",
      "Epoch 174/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3690.6755 - val_loss: 3959.2383\n",
      "Epoch 175/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3684.4592 - val_loss: 3953.7388\n",
      "Epoch 176/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3678.2341 - val_loss: 3948.1633\n",
      "Epoch 177/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3672.1191 - val_loss: 3942.7722\n",
      "Epoch 178/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3666.0027 - val_loss: 3937.3201\n",
      "Epoch 179/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3659.9658 - val_loss: 3931.8660\n",
      "Epoch 180/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3654.0334 - val_loss: 3926.6506\n",
      "Epoch 181/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3648.1172 - val_loss: 3921.4780\n",
      "Epoch 182/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3642.2903 - val_loss: 3916.4390\n",
      "Epoch 183/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3636.4902 - val_loss: 3911.3611\n",
      "Epoch 184/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3630.7419 - val_loss: 3906.3171\n",
      "Epoch 185/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3625.0579 - val_loss: 3901.3223\n",
      "Epoch 186/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3619.4514 - val_loss: 3896.4463\n",
      "Epoch 187/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3613.8528 - val_loss: 3891.3862\n",
      "Epoch 188/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3608.3254 - val_loss: 3886.5039\n",
      "Epoch 189/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3602.8459 - val_loss: 3881.7563\n",
      "Epoch 190/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3597.3916 - val_loss: 3876.9417\n",
      "Epoch 191/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3592.0142 - val_loss: 3871.9895\n",
      "Epoch 192/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3586.6877 - val_loss: 3867.2659\n",
      "Epoch 193/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3581.4277 - val_loss: 3862.5498\n",
      "Epoch 194/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3576.1790 - val_loss: 3857.8584\n",
      "Epoch 195/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3570.9863 - val_loss: 3853.2085\n",
      "Epoch 196/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3565.8547 - val_loss: 3848.6379\n",
      "Epoch 197/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3560.7869 - val_loss: 3844.1519\n",
      "Epoch 198/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3555.6292 - val_loss: 3839.7725\n",
      "Epoch 199/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3550.6782 - val_loss: 3835.4006\n",
      "Epoch 200/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3545.7122 - val_loss: 3831.0295\n",
      "Epoch 201/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3540.8103 - val_loss: 3826.5364\n",
      "Epoch 202/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3535.9158 - val_loss: 3822.1975\n",
      "Epoch 203/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3531.0330 - val_loss: 3817.8301\n",
      "Epoch 204/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3526.2808 - val_loss: 3813.5322\n",
      "Epoch 205/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3521.4353 - val_loss: 3809.2527\n",
      "Epoch 206/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3516.6936 - val_loss: 3805.1382\n",
      "Epoch 207/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3512.0740 - val_loss: 3801.0569\n",
      "Epoch 208/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3507.3948 - val_loss: 3797.0552\n",
      "Epoch 209/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3502.7876 - val_loss: 3792.9021\n",
      "Epoch 210/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3498.1772 - val_loss: 3788.8875\n",
      "Epoch 211/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3493.6794 - val_loss: 3784.8167\n",
      "Epoch 212/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 3489.1467 - val_loss: 3780.8333\n",
      "Epoch 213/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3484.6816 - val_loss: 3776.8318\n",
      "Epoch 214/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3480.1890 - val_loss: 3773.0076\n",
      "Epoch 215/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3475.7761 - val_loss: 3769.0967\n",
      "Epoch 216/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3471.4019 - val_loss: 3765.3169\n",
      "Epoch 217/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3467.0996 - val_loss: 3761.4836\n",
      "Epoch 218/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3462.7493 - val_loss: 3757.7161\n",
      "Epoch 219/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3458.4907 - val_loss: 3754.0120\n",
      "Epoch 220/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3454.2966 - val_loss: 3750.3071\n",
      "Epoch 221/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3450.0332 - val_loss: 3746.5210\n",
      "Epoch 222/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3445.8313 - val_loss: 3742.9062\n",
      "Epoch 223/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3441.7170 - val_loss: 3739.2437\n",
      "Epoch 224/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3437.5925 - val_loss: 3735.5281\n",
      "Epoch 225/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3433.4673 - val_loss: 3731.8538\n",
      "Epoch 226/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3429.4075 - val_loss: 3728.3152\n",
      "Epoch 227/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3425.3831 - val_loss: 3724.7363\n",
      "Epoch 228/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3421.3516 - val_loss: 3721.2554\n",
      "Epoch 229/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3417.3892 - val_loss: 3717.7808\n",
      "Epoch 230/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3413.4402 - val_loss: 3714.2561\n",
      "Epoch 231/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3409.4973 - val_loss: 3710.8059\n",
      "Epoch 232/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3405.5703 - val_loss: 3707.3826\n",
      "Epoch 233/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3401.7100 - val_loss: 3703.9556\n",
      "Epoch 234/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3397.8662 - val_loss: 3700.6487\n",
      "Epoch 235/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3394.0762 - val_loss: 3697.2954\n",
      "Epoch 236/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3390.2744 - val_loss: 3693.9629\n",
      "Epoch 237/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3386.4910 - val_loss: 3690.5366\n",
      "Epoch 238/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3382.7068 - val_loss: 3687.2573\n",
      "Epoch 239/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3379.0464 - val_loss: 3683.9683\n",
      "Epoch 240/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3375.2942 - val_loss: 3680.7205\n",
      "Epoch 241/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3371.6443 - val_loss: 3677.3684\n",
      "Epoch 242/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3368.0142 - val_loss: 3673.5723\n",
      "Epoch 243/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3364.4163 - val_loss: 3670.3989\n",
      "Epoch 244/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3360.8347 - val_loss: 3667.2925\n",
      "Epoch 245/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3357.2546 - val_loss: 3664.1558\n",
      "Epoch 246/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3353.7107 - val_loss: 3660.9021\n",
      "Epoch 247/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3350.1833 - val_loss: 3657.8416\n",
      "Epoch 248/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3346.6038 - val_loss: 3654.6492\n",
      "Epoch 249/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3343.2151 - val_loss: 3651.5098\n",
      "Epoch 250/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3339.7598 - val_loss: 3648.4119\n",
      "Epoch 251/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3336.3020 - val_loss: 3645.2412\n",
      "Epoch 252/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3332.8254 - val_loss: 3642.2720\n",
      "Epoch 253/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3329.4148 - val_loss: 3639.1624\n",
      "Epoch 254/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3326.0649 - val_loss: 3636.2100\n",
      "Epoch 255/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3322.6328 - val_loss: 3633.2163\n",
      "Epoch 256/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3319.2795 - val_loss: 3630.1746\n",
      "Epoch 257/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3315.9546 - val_loss: 3627.1919\n",
      "Epoch 258/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3312.6455 - val_loss: 3624.2349\n",
      "Epoch 259/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3309.3591 - val_loss: 3621.2639\n",
      "Epoch 260/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3306.0901 - val_loss: 3618.3503\n",
      "Epoch 261/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3302.8052 - val_loss: 3615.4685\n",
      "Epoch 262/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3299.5815 - val_loss: 3612.6064\n",
      "Epoch 263/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3296.4067 - val_loss: 3609.7671\n",
      "Epoch 264/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3293.2004 - val_loss: 3606.8503\n",
      "Epoch 265/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3289.9290 - val_loss: 3603.9543\n",
      "Epoch 266/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3286.8389 - val_loss: 3601.1506\n",
      "Epoch 267/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3283.6638 - val_loss: 3598.2527\n",
      "Epoch 268/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3280.5198 - val_loss: 3595.4880\n",
      "Epoch 269/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3277.4102 - val_loss: 3592.6548\n",
      "Epoch 270/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3274.3230 - val_loss: 3589.8381\n",
      "Epoch 271/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3271.2717 - val_loss: 3587.0708\n",
      "Epoch 272/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3268.1968 - val_loss: 3584.3093\n",
      "Epoch 273/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3265.1453 - val_loss: 3581.5745\n",
      "Epoch 274/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3262.0679 - val_loss: 3578.7988\n",
      "Epoch 275/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3259.0225 - val_loss: 3576.1770\n",
      "Epoch 276/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3256.0508 - val_loss: 3573.4502\n",
      "Epoch 277/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3253.1057 - val_loss: 3570.7781\n",
      "Epoch 278/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3250.1172 - val_loss: 3568.0459\n",
      "Epoch 279/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3247.1948 - val_loss: 3565.4080\n",
      "Epoch 280/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3244.2546 - val_loss: 3562.5212\n",
      "Epoch 281/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3241.3889 - val_loss: 3559.8887\n",
      "Epoch 282/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3238.4653 - val_loss: 3557.2380\n",
      "Epoch 283/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3235.5730 - val_loss: 3554.5605\n",
      "Epoch 284/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3232.6665 - val_loss: 3551.9888\n",
      "Epoch 285/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3229.8367 - val_loss: 3549.3555\n",
      "Epoch 286/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3226.9722 - val_loss: 3546.7109\n",
      "Epoch 287/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3224.1890 - val_loss: 3543.9988\n",
      "Epoch 288/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3221.3572 - val_loss: 3541.4656\n",
      "Epoch 289/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3218.5847 - val_loss: 3538.8997\n",
      "Epoch 290/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3215.7507 - val_loss: 3536.2822\n",
      "Epoch 291/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3213.0134 - val_loss: 3533.7815\n",
      "Epoch 292/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3210.2710 - val_loss: 3531.2644\n",
      "Epoch 293/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3207.5466 - val_loss: 3528.7634\n",
      "Epoch 294/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3204.8140 - val_loss: 3526.3025\n",
      "Epoch 295/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3202.0789 - val_loss: 3523.8428\n",
      "Epoch 296/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3199.3757 - val_loss: 3521.3823\n",
      "Epoch 297/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3196.7158 - val_loss: 3518.9216\n",
      "Epoch 298/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3194.0469 - val_loss: 3516.4194\n",
      "Epoch 299/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3191.3813 - val_loss: 3514.0095\n",
      "Epoch 300/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3188.7004 - val_loss: 3511.3840\n",
      "Epoch 301/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3186.0803 - val_loss: 3508.9768\n",
      "Epoch 302/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3183.4714 - val_loss: 3506.5771\n",
      "Epoch 303/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3180.8457 - val_loss: 3504.1914\n",
      "Epoch 304/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3178.2410 - val_loss: 3501.7793\n",
      "Epoch 305/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3175.6870 - val_loss: 3499.3679\n",
      "Epoch 306/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3173.0925 - val_loss: 3497.0261\n",
      "Epoch 307/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3170.5371 - val_loss: 3494.6396\n",
      "Epoch 308/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3168.0098 - val_loss: 3492.2385\n",
      "Epoch 309/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3165.4424 - val_loss: 3489.9802\n",
      "Epoch 310/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3162.9495 - val_loss: 3487.5032\n",
      "Epoch 311/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3160.4053 - val_loss: 3485.1255\n",
      "Epoch 312/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3157.9104 - val_loss: 3482.7756\n",
      "Epoch 313/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3155.3835 - val_loss: 3480.3489\n",
      "Epoch 314/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3152.9365 - val_loss: 3477.9504\n",
      "Epoch 315/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3150.4446 - val_loss: 3475.6260\n",
      "Epoch 316/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3148.0024 - val_loss: 3473.3066\n",
      "Epoch 317/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3145.5803 - val_loss: 3470.9492\n",
      "Epoch 318/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3143.1213 - val_loss: 3468.5837\n",
      "Epoch 319/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3140.6877 - val_loss: 3466.2986\n",
      "Epoch 320/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3138.3049 - val_loss: 3463.9714\n",
      "Epoch 321/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3135.8945 - val_loss: 3461.6382\n",
      "Epoch 322/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3133.4702 - val_loss: 3459.4421\n",
      "Epoch 323/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3131.1321 - val_loss: 3457.1631\n",
      "Epoch 324/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3128.7046 - val_loss: 3454.8833\n",
      "Epoch 325/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3126.3748 - val_loss: 3452.6292\n",
      "Epoch 326/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3124.0303 - val_loss: 3450.3931\n",
      "Epoch 327/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3121.6902 - val_loss: 3448.1855\n",
      "Epoch 328/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3119.3975 - val_loss: 3445.9893\n",
      "Epoch 329/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3117.0540 - val_loss: 3443.7244\n",
      "Epoch 330/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3114.7485 - val_loss: 3441.5305\n",
      "Epoch 331/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3112.4648 - val_loss: 3439.3628\n",
      "Epoch 332/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3110.1750 - val_loss: 3437.1960\n",
      "Epoch 333/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3107.9248 - val_loss: 3433.9167\n",
      "Epoch 334/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3105.6548 - val_loss: 3431.6594\n",
      "Epoch 335/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3103.4307 - val_loss: 3429.5024\n",
      "Epoch 336/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3101.1528 - val_loss: 3427.4392\n",
      "Epoch 337/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3098.9009 - val_loss: 3425.2776\n",
      "Epoch 338/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3096.6816 - val_loss: 3423.1611\n",
      "Epoch 339/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3094.4668 - val_loss: 3421.0808\n",
      "Epoch 340/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3092.3088 - val_loss: 3418.9368\n",
      "Epoch 341/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3090.0918 - val_loss: 3416.8723\n",
      "Epoch 342/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3087.9116 - val_loss: 3414.8054\n",
      "Epoch 343/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3085.6819 - val_loss: 3412.7156\n",
      "Epoch 344/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3083.5510 - val_loss: 3410.4841\n",
      "Epoch 345/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3081.4094 - val_loss: 3408.4399\n",
      "Epoch 346/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3079.2600 - val_loss: 3406.4546\n",
      "Epoch 347/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3077.0842 - val_loss: 3404.5190\n",
      "Epoch 348/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3075.0100 - val_loss: 3402.5295\n",
      "Epoch 349/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3072.8928 - val_loss: 3400.5295\n",
      "Epoch 350/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3070.7998 - val_loss: 3398.5244\n",
      "Epoch 351/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3068.6809 - val_loss: 3396.6035\n",
      "Epoch 352/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3066.6162 - val_loss: 3394.5762\n",
      "Epoch 353/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3064.5286 - val_loss: 3392.5964\n",
      "Epoch 354/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3062.4570 - val_loss: 3390.6118\n",
      "Epoch 355/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3060.3901 - val_loss: 3388.6714\n",
      "Epoch 356/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3058.3306 - val_loss: 3386.7246\n",
      "Epoch 357/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3056.3071 - val_loss: 3384.7107\n",
      "Epoch 358/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3054.2715 - val_loss: 3382.7271\n",
      "Epoch 359/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3052.2275 - val_loss: 3380.8210\n",
      "Epoch 360/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3050.2014 - val_loss: 3378.8572\n",
      "Epoch 361/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3048.1545 - val_loss: 3376.9417\n",
      "Epoch 362/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3046.1809 - val_loss: 3374.9663\n",
      "Epoch 363/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3044.1440 - val_loss: 3373.1858\n",
      "Epoch 364/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3042.1868 - val_loss: 3371.2844\n",
      "Epoch 365/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3040.2300 - val_loss: 3369.4316\n",
      "Epoch 366/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3038.2200 - val_loss: 3367.5740\n",
      "Epoch 367/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3036.2737 - val_loss: 3365.7095\n",
      "Epoch 368/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3034.3013 - val_loss: 3363.8022\n",
      "Epoch 369/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3032.3521 - val_loss: 3361.9473\n",
      "Epoch 370/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3030.4128 - val_loss: 3360.1096\n",
      "Epoch 371/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3028.5012 - val_loss: 3358.2512\n",
      "Epoch 372/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3026.5273 - val_loss: 3356.3320\n",
      "Epoch 373/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3024.6387 - val_loss: 3354.4851\n",
      "Epoch 374/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3022.7056 - val_loss: 3352.6262\n",
      "Epoch 375/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3020.7859 - val_loss: 3350.7646\n",
      "Epoch 376/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3018.8699 - val_loss: 3348.8757\n",
      "Epoch 377/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3016.9854 - val_loss: 3347.1008\n",
      "Epoch 378/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3015.1169 - val_loss: 3345.3193\n",
      "Epoch 379/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3013.2737 - val_loss: 3343.4573\n",
      "Epoch 380/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3011.3799 - val_loss: 3341.7166\n",
      "Epoch 381/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3009.4910 - val_loss: 3339.9551\n",
      "Epoch 382/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3007.6643 - val_loss: 3338.2288\n",
      "Epoch 383/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 3005.7915 - val_loss: 3336.4404\n",
      "Epoch 384/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3003.9214 - val_loss: 3334.6519\n",
      "Epoch 385/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3002.0938 - val_loss: 3332.8042\n",
      "Epoch 386/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 3000.2661 - val_loss: 3331.0378\n",
      "Epoch 387/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2998.4514 - val_loss: 3329.3093\n",
      "Epoch 388/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2996.6196 - val_loss: 3327.4995\n",
      "Epoch 389/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2994.8237 - val_loss: 3325.7778\n",
      "Epoch 390/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2992.9851 - val_loss: 3324.0278\n",
      "Epoch 391/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2991.1453 - val_loss: 3322.3823\n",
      "Epoch 392/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2989.3621 - val_loss: 3320.6267\n",
      "Epoch 393/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2987.5947 - val_loss: 3318.8887\n",
      "Epoch 394/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2985.8057 - val_loss: 3317.0906\n",
      "Epoch 395/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2984.0374 - val_loss: 3315.3184\n",
      "Epoch 396/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2982.2568 - val_loss: 3313.6174\n",
      "Epoch 397/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2980.4634 - val_loss: 3311.8220\n",
      "Epoch 398/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2978.7307 - val_loss: 3310.0361\n",
      "Epoch 399/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2976.9548 - val_loss: 3308.2761\n",
      "Epoch 400/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2975.1975 - val_loss: 3306.5166\n",
      "Epoch 401/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2973.4529 - val_loss: 3304.7646\n",
      "Epoch 402/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2971.7151 - val_loss: 3303.0608\n",
      "Epoch 403/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2970.0002 - val_loss: 3301.2952\n",
      "Epoch 404/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2968.2402 - val_loss: 3299.6011\n",
      "Epoch 405/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2966.5200 - val_loss: 3297.9099\n",
      "Epoch 406/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2964.8276 - val_loss: 3296.2214\n",
      "Epoch 407/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2963.1304 - val_loss: 3294.5061\n",
      "Epoch 408/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2961.4446 - val_loss: 3292.8792\n",
      "Epoch 409/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2959.7039 - val_loss: 3291.2312\n",
      "Epoch 410/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2958.0129 - val_loss: 3289.5281\n",
      "Epoch 411/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2956.3267 - val_loss: 3287.8486\n",
      "Epoch 412/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2954.6526 - val_loss: 3286.2473\n",
      "Epoch 413/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2952.9705 - val_loss: 3284.6921\n",
      "Epoch 414/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2951.2981 - val_loss: 3283.0891\n",
      "Epoch 415/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2949.6694 - val_loss: 3281.4241\n",
      "Epoch 416/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2947.9846 - val_loss: 3279.7700\n",
      "Epoch 417/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2946.2952 - val_loss: 3278.1423\n",
      "Epoch 418/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2944.6226 - val_loss: 3276.6074\n",
      "Epoch 419/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2943.0403 - val_loss: 3274.9521\n",
      "Epoch 420/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2941.3706 - val_loss: 3273.3137\n",
      "Epoch 421/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2939.7197 - val_loss: 3271.6160\n",
      "Epoch 422/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2938.1362 - val_loss: 3269.9963\n",
      "Epoch 423/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2936.4688 - val_loss: 3268.4028\n",
      "Epoch 424/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2934.8406 - val_loss: 3266.7000\n",
      "Epoch 425/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2933.1929 - val_loss: 3265.0479\n",
      "Epoch 426/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2931.5959 - val_loss: 3263.5095\n",
      "Epoch 427/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2930.0042 - val_loss: 3261.9683\n",
      "Epoch 428/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2928.3882 - val_loss: 3260.3608\n",
      "Epoch 429/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2926.8025 - val_loss: 3258.7161\n",
      "Epoch 430/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2925.2532 - val_loss: 3257.2141\n",
      "Epoch 431/1000\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 2923.6228 - val_loss: 3255.5947\n",
      "Epoch 432/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2922.0229 - val_loss: 3253.9780\n",
      "Epoch 433/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2920.4751 - val_loss: 3252.4104\n",
      "Epoch 434/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2918.8679 - val_loss: 3250.8206\n",
      "Epoch 435/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2917.2986 - val_loss: 3249.1875\n",
      "Epoch 436/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2915.7300 - val_loss: 3247.5903\n",
      "Epoch 437/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2914.1694 - val_loss: 3245.9541\n",
      "Epoch 438/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2912.6238 - val_loss: 3244.3037\n",
      "Epoch 439/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2911.0715 - val_loss: 3242.7375\n",
      "Epoch 440/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2909.5154 - val_loss: 3241.1047\n",
      "Epoch 441/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2908.0049 - val_loss: 3239.4546\n",
      "Epoch 442/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2906.4492 - val_loss: 3237.8574\n",
      "Epoch 443/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2904.9500 - val_loss: 3236.2834\n",
      "Epoch 444/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2903.3867 - val_loss: 3234.7136\n",
      "Epoch 445/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2901.8860 - val_loss: 3233.0847\n",
      "Epoch 446/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2900.3625 - val_loss: 3231.4714\n",
      "Epoch 447/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2898.8271 - val_loss: 3229.9568\n",
      "Epoch 448/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2897.3508 - val_loss: 3228.3828\n",
      "Epoch 449/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2895.8633 - val_loss: 3226.7854\n",
      "Epoch 450/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2894.2869 - val_loss: 3225.2039\n",
      "Epoch 451/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2892.8391 - val_loss: 3223.6426\n",
      "Epoch 452/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2891.3025 - val_loss: 3222.0198\n",
      "Epoch 453/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2889.8528 - val_loss: 3220.5168\n",
      "Epoch 454/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2888.3818 - val_loss: 3219.0369\n",
      "Epoch 455/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2886.9243 - val_loss: 3217.4817\n",
      "Epoch 456/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2885.3984 - val_loss: 3216.0208\n",
      "Epoch 457/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2883.9326 - val_loss: 3214.4934\n",
      "Epoch 458/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2882.4651 - val_loss: 3212.9478\n",
      "Epoch 459/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2881.0339 - val_loss: 3211.4109\n",
      "Epoch 460/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2879.5652 - val_loss: 3209.9541\n",
      "Epoch 461/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2878.1375 - val_loss: 3208.4871\n",
      "Epoch 462/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2876.6807 - val_loss: 3207.0527\n",
      "Epoch 463/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2875.2566 - val_loss: 3205.5696\n",
      "Epoch 464/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2873.7891 - val_loss: 3204.0903\n",
      "Epoch 465/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2872.3582 - val_loss: 3202.6787\n",
      "Epoch 466/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2870.9421 - val_loss: 3201.2439\n",
      "Epoch 467/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2869.5344 - val_loss: 3199.7737\n",
      "Epoch 468/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2868.0535 - val_loss: 3198.2527\n",
      "Epoch 469/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2866.6768 - val_loss: 3196.8040\n",
      "Epoch 470/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2865.2085 - val_loss: 3195.2354\n",
      "Epoch 471/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2863.8267 - val_loss: 3193.8672\n",
      "Epoch 472/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2862.3792 - val_loss: 3192.5027\n",
      "Epoch 473/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2860.9875 - val_loss: 3191.0852\n",
      "Epoch 474/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2859.5886 - val_loss: 3189.6790\n",
      "Epoch 475/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2858.1702 - val_loss: 3188.2285\n",
      "Epoch 476/1000\n",
      "125/125 [==============================] - 0s 4ms/step - loss: 2856.7727 - val_loss: 3186.7786\n",
      "Epoch 477/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2855.3828 - val_loss: 3185.3245\n",
      "Epoch 478/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2853.9827 - val_loss: 3184.0476\n",
      "Epoch 479/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2852.6282 - val_loss: 3182.6458\n",
      "Epoch 480/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2851.2373 - val_loss: 3181.1802\n",
      "Epoch 481/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2849.8616 - val_loss: 3179.7344\n",
      "Epoch 482/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2848.4800 - val_loss: 3178.3550\n",
      "Epoch 483/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2847.1494 - val_loss: 3176.9587\n",
      "Epoch 484/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2845.7563 - val_loss: 3175.6304\n",
      "Epoch 485/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2844.3928 - val_loss: 3174.2166\n",
      "Epoch 486/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2843.0681 - val_loss: 3172.8110\n",
      "Epoch 487/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2841.6956 - val_loss: 3171.4612\n",
      "Epoch 488/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2840.3076 - val_loss: 3170.1536\n",
      "Epoch 489/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2838.9590 - val_loss: 3168.7959\n",
      "Epoch 490/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2837.6521 - val_loss: 3167.4343\n",
      "Epoch 491/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2836.2781 - val_loss: 3166.0854\n",
      "Epoch 492/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2834.9336 - val_loss: 3164.7312\n",
      "Epoch 493/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2833.6394 - val_loss: 3163.3940\n",
      "Epoch 494/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2832.2800 - val_loss: 3162.0955\n",
      "Epoch 495/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2830.9409 - val_loss: 3160.7104\n",
      "Epoch 496/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2829.6538 - val_loss: 3159.3979\n",
      "Epoch 497/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2828.2944 - val_loss: 3158.1704\n",
      "Epoch 498/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2827.0000 - val_loss: 3156.8789\n",
      "Epoch 499/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2825.7021 - val_loss: 3155.5527\n",
      "Epoch 500/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2824.4062 - val_loss: 3154.2493\n",
      "Epoch 501/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2823.0684 - val_loss: 3153.0620\n",
      "Epoch 502/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2821.7949 - val_loss: 3151.8069\n",
      "Epoch 503/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2820.4626 - val_loss: 3150.4207\n",
      "Epoch 504/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2819.1714 - val_loss: 3149.2361\n",
      "Epoch 505/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2817.9097 - val_loss: 3147.9849\n",
      "Epoch 506/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2816.6077 - val_loss: 3146.6804\n",
      "Epoch 507/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2815.3293 - val_loss: 3145.3586\n",
      "Epoch 508/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2814.0129 - val_loss: 3144.1077\n",
      "Epoch 509/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2812.7832 - val_loss: 3142.9285\n",
      "Epoch 510/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2811.4734 - val_loss: 3141.7241\n",
      "Epoch 511/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2810.1899 - val_loss: 3140.3372\n",
      "Epoch 512/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2808.9180 - val_loss: 3139.0400\n",
      "Epoch 513/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2807.6558 - val_loss: 3137.8418\n",
      "Epoch 514/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2806.3843 - val_loss: 3136.6443\n",
      "Epoch 515/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2805.1362 - val_loss: 3135.4194\n",
      "Epoch 516/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2803.8838 - val_loss: 3134.2549\n",
      "Epoch 517/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2802.6082 - val_loss: 3132.9878\n",
      "Epoch 518/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2801.3845 - val_loss: 3131.8267\n",
      "Epoch 519/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2800.1294 - val_loss: 3130.6455\n",
      "Epoch 520/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2798.8381 - val_loss: 3129.5371\n",
      "Epoch 521/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2797.6328 - val_loss: 3128.3428\n",
      "Epoch 522/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2796.3789 - val_loss: 3127.1333\n",
      "Epoch 523/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2795.1326 - val_loss: 3125.8882\n",
      "Epoch 524/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2793.8901 - val_loss: 3124.6987\n",
      "Epoch 525/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2792.6543 - val_loss: 3123.4724\n",
      "Epoch 526/1000\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 2791.4021 - val_loss: 3122.3884\n",
      "Epoch 527/1000\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 2790.2209 - val_loss: 3121.2134\n",
      "Epoch 528/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2788.9917 - val_loss: 3120.0208\n",
      "Epoch 529/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2787.7654 - val_loss: 3118.7673\n",
      "Epoch 530/1000\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 2786.5591 - val_loss: 3117.5562\n",
      "Epoch 531/1000\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2785.3525 - val_loss: 3116.4695\n",
      "Epoch 532/1000\n",
      "125/125 [==============================] - 2s 12ms/step - loss: 2784.1267 - val_loss: 3115.3374\n",
      "Epoch 533/1000\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 2782.9180 - val_loss: 3114.2673\n",
      "Epoch 534/1000\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 2781.7212 - val_loss: 3113.0750\n",
      "Epoch 535/1000\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 2780.4878 - val_loss: 3111.9663\n",
      "Epoch 536/1000\n",
      "125/125 [==============================] - 1s 10ms/step - loss: 2779.2930 - val_loss: 3110.8193\n",
      "Epoch 537/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2778.0938 - val_loss: 3109.7461\n",
      "Epoch 538/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2776.8630 - val_loss: 3108.5669\n",
      "Epoch 539/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2775.7183 - val_loss: 3107.4531\n",
      "Epoch 540/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2774.5291 - val_loss: 3106.3438\n",
      "Epoch 541/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2773.3621 - val_loss: 3105.2673\n",
      "Epoch 542/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2772.1809 - val_loss: 3104.1663\n",
      "Epoch 543/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2770.9832 - val_loss: 3103.0449\n",
      "Epoch 544/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2769.8306 - val_loss: 3102.1252\n",
      "Epoch 545/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2768.6379 - val_loss: 3100.9790\n",
      "Epoch 546/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2767.4839 - val_loss: 3099.8977\n",
      "Epoch 547/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2766.2925 - val_loss: 3098.8572\n",
      "Epoch 548/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2765.1599 - val_loss: 3097.6938\n",
      "Epoch 549/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2763.9961 - val_loss: 3096.5732\n",
      "Epoch 550/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2762.8174 - val_loss: 3095.4836\n",
      "Epoch 551/1000\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 2761.6511 - val_loss: 3094.3916\n",
      "Epoch 552/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2760.5105 - val_loss: 3093.2805\n",
      "Epoch 553/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2759.3445 - val_loss: 3092.1687\n",
      "Epoch 554/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2758.2104 - val_loss: 3091.1216\n",
      "Epoch 555/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2757.0647 - val_loss: 3090.1475\n",
      "Epoch 556/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2755.9370 - val_loss: 3089.1433\n",
      "Epoch 557/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2754.8059 - val_loss: 3088.1184\n",
      "Epoch 558/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2753.6628 - val_loss: 3087.1045\n",
      "Epoch 559/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2752.5366 - val_loss: 3086.1165\n",
      "Epoch 560/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2751.3953 - val_loss: 3085.1157\n",
      "Epoch 561/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2750.2993 - val_loss: 3084.1541\n",
      "Epoch 562/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2749.1467 - val_loss: 3083.1387\n",
      "Epoch 563/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2748.0205 - val_loss: 3082.1191\n",
      "Epoch 564/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2746.9414 - val_loss: 3081.1648\n",
      "Epoch 565/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2745.8154 - val_loss: 3080.2598\n",
      "Epoch 566/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2744.7119 - val_loss: 3079.3081\n",
      "Epoch 567/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2743.5925 - val_loss: 3078.3237\n",
      "Epoch 568/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2742.4915 - val_loss: 3077.2578\n",
      "Epoch 569/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2741.3398 - val_loss: 3076.2708\n",
      "Epoch 570/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2740.2251 - val_loss: 3075.2871\n",
      "Epoch 571/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2739.1646 - val_loss: 3074.3057\n",
      "Epoch 572/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2738.0291 - val_loss: 3073.3289\n",
      "Epoch 573/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2736.9758 - val_loss: 3072.3999\n",
      "Epoch 574/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2735.8694 - val_loss: 3071.4382\n",
      "Epoch 575/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2734.7888 - val_loss: 3070.4431\n",
      "Epoch 576/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2733.7078 - val_loss: 3069.5095\n",
      "Epoch 577/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2732.5867 - val_loss: 3068.5432\n",
      "Epoch 578/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2731.5371 - val_loss: 3067.6392\n",
      "Epoch 579/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2730.4248 - val_loss: 3066.6775\n",
      "Epoch 580/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2729.3516 - val_loss: 3065.7117\n",
      "Epoch 581/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2728.2830 - val_loss: 3064.8171\n",
      "Epoch 582/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2727.2166 - val_loss: 3063.9355\n",
      "Epoch 583/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2726.1157 - val_loss: 3063.0464\n",
      "Epoch 584/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2725.0842 - val_loss: 3062.0894\n",
      "Epoch 585/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2724.0066 - val_loss: 3061.2234\n",
      "Epoch 586/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2722.9004 - val_loss: 3060.2629\n",
      "Epoch 587/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2721.9075 - val_loss: 3059.3953\n",
      "Epoch 588/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2720.8179 - val_loss: 3058.5188\n",
      "Epoch 589/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2719.7693 - val_loss: 3057.6687\n",
      "Epoch 590/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2718.6860 - val_loss: 3056.6729\n",
      "Epoch 591/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2717.6770 - val_loss: 3055.8030\n",
      "Epoch 592/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2716.6284 - val_loss: 3054.9639\n",
      "Epoch 593/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2715.5605 - val_loss: 3054.0410\n",
      "Epoch 594/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2714.5259 - val_loss: 3053.1990\n",
      "Epoch 595/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2713.4893 - val_loss: 3052.4021\n",
      "Epoch 596/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2712.4390 - val_loss: 3051.5991\n",
      "Epoch 597/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2711.3767 - val_loss: 3050.6130\n",
      "Epoch 598/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2710.3843 - val_loss: 3049.8232\n",
      "Epoch 599/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2709.3606 - val_loss: 3048.9883\n",
      "Epoch 600/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2708.3232 - val_loss: 3048.2261\n",
      "Epoch 601/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2707.2898 - val_loss: 3047.4243\n",
      "Epoch 602/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2706.2598 - val_loss: 3046.6038\n",
      "Epoch 603/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2705.2632 - val_loss: 3045.7485\n",
      "Epoch 604/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2704.2173 - val_loss: 3044.8889\n",
      "Epoch 605/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2703.2393 - val_loss: 3044.0425\n",
      "Epoch 606/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2702.1921 - val_loss: 3043.2871\n",
      "Epoch 607/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2701.2126 - val_loss: 3042.5337\n",
      "Epoch 608/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2700.1653 - val_loss: 3041.7034\n",
      "Epoch 609/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2699.1733 - val_loss: 3040.8379\n",
      "Epoch 610/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2698.1738 - val_loss: 3040.0793\n",
      "Epoch 611/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2697.1426 - val_loss: 3039.3223\n",
      "Epoch 612/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2696.1741 - val_loss: 3038.6133\n",
      "Epoch 613/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2695.1836 - val_loss: 3037.7559\n",
      "Epoch 614/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2694.1812 - val_loss: 3036.9294\n",
      "Epoch 615/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2693.1934 - val_loss: 3036.2534\n",
      "Epoch 616/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2692.2065 - val_loss: 3035.4170\n",
      "Epoch 617/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2691.2004 - val_loss: 3034.6360\n",
      "Epoch 618/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2690.2322 - val_loss: 3033.8801\n",
      "Epoch 619/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2689.2034 - val_loss: 3033.2393\n",
      "Epoch 620/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2688.2812 - val_loss: 3032.4285\n",
      "Epoch 621/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2687.2791 - val_loss: 3031.7288\n",
      "Epoch 622/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2686.3167 - val_loss: 3030.9216\n",
      "Epoch 623/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2685.3408 - val_loss: 3030.2278\n",
      "Epoch 624/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2684.3875 - val_loss: 3029.4834\n",
      "Epoch 625/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2683.4026 - val_loss: 3028.7380\n",
      "Epoch 626/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2682.4124 - val_loss: 3028.0652\n",
      "Epoch 627/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2681.4536 - val_loss: 3027.3188\n",
      "Epoch 628/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2680.4890 - val_loss: 3026.5835\n",
      "Epoch 629/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2679.5149 - val_loss: 3025.8083\n",
      "Epoch 630/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2678.5735 - val_loss: 3025.0557\n",
      "Epoch 631/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2677.5969 - val_loss: 3024.3108\n",
      "Epoch 632/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2676.6687 - val_loss: 3023.5667\n",
      "Epoch 633/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2675.6899 - val_loss: 3022.9023\n",
      "Epoch 634/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2674.7317 - val_loss: 3022.0271\n",
      "Epoch 635/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2673.7371 - val_loss: 3021.2971\n",
      "Epoch 636/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2672.7964 - val_loss: 3020.4141\n",
      "Epoch 637/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2671.8799 - val_loss: 3019.6919\n",
      "Epoch 638/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2670.9294 - val_loss: 3018.4370\n",
      "Epoch 639/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2669.9622 - val_loss: 3017.6885\n",
      "Epoch 640/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2669.0361 - val_loss: 3017.0034\n",
      "Epoch 641/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2668.0811 - val_loss: 3016.2888\n",
      "Epoch 642/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2667.1401 - val_loss: 3015.6162\n",
      "Epoch 643/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2666.2163 - val_loss: 3014.9680\n",
      "Epoch 644/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2665.2644 - val_loss: 3014.2871\n",
      "Epoch 645/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2664.3513 - val_loss: 3013.5874\n",
      "Epoch 646/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2663.4268 - val_loss: 3012.9175\n",
      "Epoch 647/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2662.4810 - val_loss: 3012.2405\n",
      "Epoch 648/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2661.5566 - val_loss: 3011.6709\n",
      "Epoch 649/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2660.6631 - val_loss: 3010.9395\n",
      "Epoch 650/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2659.7463 - val_loss: 3010.2817\n",
      "Epoch 651/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2658.8359 - val_loss: 3009.5613\n",
      "Epoch 652/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2657.8870 - val_loss: 3008.8760\n",
      "Epoch 653/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2657.0027 - val_loss: 3008.2273\n",
      "Epoch 654/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2656.1042 - val_loss: 3007.5967\n",
      "Epoch 655/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2655.1716 - val_loss: 3006.9009\n",
      "Epoch 656/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2654.2363 - val_loss: 3006.1697\n",
      "Epoch 657/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2653.3521 - val_loss: 3005.4690\n",
      "Epoch 658/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2652.4514 - val_loss: 3004.7825\n",
      "Epoch 659/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2651.5178 - val_loss: 3004.1108\n",
      "Epoch 660/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2650.6433 - val_loss: 3003.4348\n",
      "Epoch 661/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2649.7305 - val_loss: 3002.7710\n",
      "Epoch 662/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2648.8328 - val_loss: 3002.1389\n",
      "Epoch 663/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2647.9399 - val_loss: 3001.4568\n",
      "Epoch 664/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2647.0254 - val_loss: 3000.8081\n",
      "Epoch 665/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2646.1453 - val_loss: 3000.1323\n",
      "Epoch 666/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2645.2522 - val_loss: 2999.4395\n",
      "Epoch 667/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2644.3655 - val_loss: 2998.7151\n",
      "Epoch 668/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2643.4968 - val_loss: 2998.0916\n",
      "Epoch 669/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2642.5879 - val_loss: 2997.4250\n",
      "Epoch 670/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2641.6902 - val_loss: 2996.8225\n",
      "Epoch 671/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2640.8267 - val_loss: 2996.1272\n",
      "Epoch 672/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2639.9451 - val_loss: 2995.5117\n",
      "Epoch 673/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2639.0881 - val_loss: 2994.9497\n",
      "Epoch 674/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2638.1697 - val_loss: 2994.2886\n",
      "Epoch 675/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2637.3005 - val_loss: 2993.6697\n",
      "Epoch 676/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2636.4260 - val_loss: 2993.0659\n",
      "Epoch 677/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2635.5427 - val_loss: 2992.4944\n",
      "Epoch 678/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2634.6687 - val_loss: 2991.8254\n",
      "Epoch 679/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2633.8074 - val_loss: 2991.2715\n",
      "Epoch 680/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2632.9436 - val_loss: 2990.6272\n",
      "Epoch 681/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2632.0420 - val_loss: 2989.9900\n",
      "Epoch 682/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2631.2246 - val_loss: 2989.3877\n",
      "Epoch 683/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2630.3733 - val_loss: 2988.7996\n",
      "Epoch 684/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2629.5195 - val_loss: 2988.1926\n",
      "Epoch 685/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2628.6301 - val_loss: 2987.5491\n",
      "Epoch 686/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2627.7510 - val_loss: 2986.9045\n",
      "Epoch 687/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2626.8967 - val_loss: 2986.3367\n",
      "Epoch 688/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2626.0227 - val_loss: 2985.8059\n",
      "Epoch 689/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2625.1826 - val_loss: 2985.1172\n",
      "Epoch 690/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2624.3350 - val_loss: 2984.5732\n",
      "Epoch 691/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2623.4521 - val_loss: 2983.9551\n",
      "Epoch 692/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2622.6182 - val_loss: 2983.3252\n",
      "Epoch 693/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2621.7554 - val_loss: 2982.7427\n",
      "Epoch 694/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2620.9236 - val_loss: 2982.0994\n",
      "Epoch 695/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2620.0933 - val_loss: 2981.4941\n",
      "Epoch 696/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2619.2229 - val_loss: 2980.8977\n",
      "Epoch 697/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2618.3787 - val_loss: 2980.3662\n",
      "Epoch 698/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2617.5427 - val_loss: 2979.7722\n",
      "Epoch 699/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2616.7051 - val_loss: 2979.1267\n",
      "Epoch 700/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2615.8555 - val_loss: 2978.4866\n",
      "Epoch 701/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2615.0288 - val_loss: 2977.9460\n",
      "Epoch 702/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2614.1741 - val_loss: 2977.4338\n",
      "Epoch 703/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2613.3926 - val_loss: 2976.8579\n",
      "Epoch 704/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2612.5374 - val_loss: 2976.3157\n",
      "Epoch 705/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2611.6880 - val_loss: 2975.6978\n",
      "Epoch 706/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2610.8667 - val_loss: 2975.1128\n",
      "Epoch 707/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2610.0234 - val_loss: 2974.5471\n",
      "Epoch 708/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2609.2197 - val_loss: 2973.9805\n",
      "Epoch 709/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2608.3801 - val_loss: 2973.4546\n",
      "Epoch 710/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2607.5791 - val_loss: 2972.8679\n",
      "Epoch 711/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2606.7520 - val_loss: 2972.3792\n",
      "Epoch 712/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2605.9546 - val_loss: 2971.8352\n",
      "Epoch 713/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2605.1267 - val_loss: 2971.3276\n",
      "Epoch 714/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2604.2937 - val_loss: 2970.8650\n",
      "Epoch 715/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2603.4998 - val_loss: 2970.2979\n",
      "Epoch 716/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2602.6836 - val_loss: 2969.6780\n",
      "Epoch 717/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2601.8330 - val_loss: 2969.1299\n",
      "Epoch 718/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2601.0515 - val_loss: 2968.6138\n",
      "Epoch 719/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2600.2566 - val_loss: 2968.0781\n",
      "Epoch 720/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2599.4155 - val_loss: 2967.5129\n",
      "Epoch 721/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2598.6250 - val_loss: 2967.0024\n",
      "Epoch 722/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2597.8066 - val_loss: 2966.5234\n",
      "Epoch 723/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2597.0339 - val_loss: 2965.9482\n",
      "Epoch 724/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2596.2441 - val_loss: 2965.3853\n",
      "Epoch 725/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2595.4448 - val_loss: 2964.8765\n",
      "Epoch 726/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2594.6604 - val_loss: 2964.3313\n",
      "Epoch 727/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2593.8386 - val_loss: 2963.8237\n",
      "Epoch 728/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2593.0442 - val_loss: 2963.3342\n",
      "Epoch 729/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2592.2432 - val_loss: 2962.7957\n",
      "Epoch 730/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2591.4814 - val_loss: 2962.2949\n",
      "Epoch 731/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2590.6687 - val_loss: 2961.7576\n",
      "Epoch 732/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2589.8694 - val_loss: 2961.1812\n",
      "Epoch 733/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2589.1023 - val_loss: 2960.6736\n",
      "Epoch 734/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2588.3145 - val_loss: 2960.0913\n",
      "Epoch 735/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2587.5176 - val_loss: 2959.5862\n",
      "Epoch 736/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2586.7339 - val_loss: 2959.1077\n",
      "Epoch 737/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2585.9485 - val_loss: 2958.5916\n",
      "Epoch 738/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2585.2039 - val_loss: 2958.0984\n",
      "Epoch 739/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2584.4248 - val_loss: 2957.6755\n",
      "Epoch 740/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2583.6155 - val_loss: 2957.1577\n",
      "Epoch 741/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2582.8337 - val_loss: 2956.6606\n",
      "Epoch 742/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2582.1011 - val_loss: 2956.1125\n",
      "Epoch 743/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2581.3188 - val_loss: 2955.6013\n",
      "Epoch 744/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2580.5183 - val_loss: 2955.1672\n",
      "Epoch 745/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2579.7866 - val_loss: 2954.6680\n",
      "Epoch 746/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2579.0024 - val_loss: 2954.1501\n",
      "Epoch 747/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2578.2661 - val_loss: 2953.7090\n",
      "Epoch 748/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2577.4893 - val_loss: 2953.2041\n",
      "Epoch 749/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2576.7227 - val_loss: 2952.7327\n",
      "Epoch 750/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2575.9761 - val_loss: 2952.1775\n",
      "Epoch 751/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2575.1851 - val_loss: 2951.6863\n",
      "Epoch 752/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2574.4253 - val_loss: 2951.2344\n",
      "Epoch 753/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2573.6912 - val_loss: 2950.7581\n",
      "Epoch 754/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2572.9214 - val_loss: 2950.2874\n",
      "Epoch 755/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2572.1772 - val_loss: 2949.8652\n",
      "Epoch 756/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2571.4094 - val_loss: 2949.4426\n",
      "Epoch 757/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2570.6787 - val_loss: 2948.9153\n",
      "Epoch 758/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2569.9470 - val_loss: 2948.4612\n",
      "Epoch 759/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2569.1409 - val_loss: 2947.9897\n",
      "Epoch 760/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2568.4460 - val_loss: 2947.5095\n",
      "Epoch 761/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2567.6794 - val_loss: 2947.0300\n",
      "Epoch 762/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2566.9414 - val_loss: 2946.6265\n",
      "Epoch 763/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2566.1851 - val_loss: 2946.1631\n",
      "Epoch 764/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2565.4465 - val_loss: 2945.7307\n",
      "Epoch 765/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2564.7002 - val_loss: 2945.2688\n",
      "Epoch 766/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2563.9512 - val_loss: 2944.7622\n",
      "Epoch 767/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2563.2056 - val_loss: 2944.3718\n",
      "Epoch 768/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2562.5039 - val_loss: 2944.0364\n",
      "Epoch 769/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2561.7439 - val_loss: 2943.5532\n",
      "Epoch 770/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2561.0598 - val_loss: 2943.0876\n",
      "Epoch 771/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2560.2974 - val_loss: 2942.6182\n",
      "Epoch 772/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2559.5869 - val_loss: 2942.1816\n",
      "Epoch 773/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2558.8674 - val_loss: 2941.7026\n",
      "Epoch 774/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2558.0942 - val_loss: 2941.2278\n",
      "Epoch 775/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2557.3809 - val_loss: 2940.7922\n",
      "Epoch 776/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2556.6577 - val_loss: 2940.3838\n",
      "Epoch 777/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2555.9219 - val_loss: 2939.9575\n",
      "Epoch 778/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2555.2075 - val_loss: 2939.4714\n",
      "Epoch 779/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2554.5049 - val_loss: 2939.0122\n",
      "Epoch 780/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2553.7656 - val_loss: 2938.5498\n",
      "Epoch 781/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2553.0801 - val_loss: 2938.1108\n",
      "Epoch 782/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2552.3582 - val_loss: 2937.6445\n",
      "Epoch 783/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2551.6094 - val_loss: 2937.1694\n",
      "Epoch 784/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2550.9253 - val_loss: 2936.7600\n",
      "Epoch 785/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2550.1936 - val_loss: 2936.2878\n",
      "Epoch 786/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2549.4866 - val_loss: 2935.8728\n",
      "Epoch 787/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2548.7991 - val_loss: 2935.4224\n",
      "Epoch 788/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2548.0847 - val_loss: 2934.9829\n",
      "Epoch 789/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2547.3628 - val_loss: 2934.5332\n",
      "Epoch 790/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2546.6487 - val_loss: 2934.0833\n",
      "Epoch 791/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2545.9631 - val_loss: 2933.6665\n",
      "Epoch 792/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2545.2734 - val_loss: 2933.1184\n",
      "Epoch 793/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2544.5559 - val_loss: 2932.7043\n",
      "Epoch 794/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2543.8535 - val_loss: 2932.2439\n",
      "Epoch 795/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2543.1338 - val_loss: 2931.8271\n",
      "Epoch 796/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2542.4783 - val_loss: 2931.4336\n",
      "Epoch 797/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2541.7756 - val_loss: 2930.9961\n",
      "Epoch 798/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2541.0530 - val_loss: 2930.5432\n",
      "Epoch 799/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2540.3721 - val_loss: 2930.0791\n",
      "Epoch 800/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2539.7058 - val_loss: 2929.6782\n",
      "Epoch 801/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2538.9697 - val_loss: 2929.2351\n",
      "Epoch 802/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2538.2573 - val_loss: 2928.8225\n",
      "Epoch 803/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2537.5969 - val_loss: 2928.4050\n",
      "Epoch 804/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2536.9211 - val_loss: 2927.9558\n",
      "Epoch 805/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2536.2363 - val_loss: 2927.4607\n",
      "Epoch 806/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2535.5430 - val_loss: 2927.0393\n",
      "Epoch 807/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2534.8086 - val_loss: 2926.5486\n",
      "Epoch 808/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2534.1414 - val_loss: 2926.1479\n",
      "Epoch 809/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2533.4978 - val_loss: 2925.6909\n",
      "Epoch 810/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2532.8020 - val_loss: 2925.2437\n",
      "Epoch 811/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2532.1289 - val_loss: 2924.7988\n",
      "Epoch 812/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2531.4565 - val_loss: 2924.3879\n",
      "Epoch 813/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2530.7651 - val_loss: 2923.9392\n",
      "Epoch 814/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2530.0693 - val_loss: 2923.5535\n",
      "Epoch 815/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2529.4175 - val_loss: 2923.1414\n",
      "Epoch 816/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2528.7185 - val_loss: 2922.7498\n",
      "Epoch 817/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2528.0425 - val_loss: 2922.3462\n",
      "Epoch 818/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2527.3650 - val_loss: 2921.9695\n",
      "Epoch 819/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2526.7012 - val_loss: 2921.5029\n",
      "Epoch 820/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2526.0330 - val_loss: 2921.0793\n",
      "Epoch 821/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2525.3491 - val_loss: 2920.6697\n",
      "Epoch 822/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2524.6904 - val_loss: 2920.2258\n",
      "Epoch 823/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2524.0164 - val_loss: 2919.8240\n",
      "Epoch 824/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2523.3672 - val_loss: 2919.4104\n",
      "Epoch 825/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2522.6904 - val_loss: 2919.0085\n",
      "Epoch 826/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2522.0232 - val_loss: 2918.6006\n",
      "Epoch 827/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2521.3823 - val_loss: 2918.2339\n",
      "Epoch 828/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2520.7073 - val_loss: 2917.7847\n",
      "Epoch 829/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2520.0327 - val_loss: 2917.4314\n",
      "Epoch 830/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2519.3918 - val_loss: 2917.0493\n",
      "Epoch 831/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2518.6958 - val_loss: 2916.6821\n",
      "Epoch 832/1000\n",
      "125/125 [==============================] - 1s 9ms/step - loss: 2518.0471 - val_loss: 2916.3020\n",
      "Epoch 833/1000\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 2517.4119 - val_loss: 2915.9014\n",
      "Epoch 834/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2516.7346 - val_loss: 2915.4993\n",
      "Epoch 835/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2516.0674 - val_loss: 2915.1074\n",
      "Epoch 836/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2515.4316 - val_loss: 2914.7251\n",
      "Epoch 837/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2514.7720 - val_loss: 2914.3953\n",
      "Epoch 838/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2514.1318 - val_loss: 2914.0115\n",
      "Epoch 839/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2513.4895 - val_loss: 2913.6226\n",
      "Epoch 840/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2512.8459 - val_loss: 2913.2014\n",
      "Epoch 841/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2512.1714 - val_loss: 2912.8025\n",
      "Epoch 842/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2511.5269 - val_loss: 2912.3948\n",
      "Epoch 843/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2510.8652 - val_loss: 2912.0217\n",
      "Epoch 844/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2510.2341 - val_loss: 2911.5732\n",
      "Epoch 845/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2509.5764 - val_loss: 2911.2009\n",
      "Epoch 846/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2508.9375 - val_loss: 2910.7974\n",
      "Epoch 847/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2508.3113 - val_loss: 2910.4121\n",
      "Epoch 848/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2507.6633 - val_loss: 2910.0505\n",
      "Epoch 849/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2507.0054 - val_loss: 2909.6948\n",
      "Epoch 850/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2506.3411 - val_loss: 2909.2927\n",
      "Epoch 851/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2505.7200 - val_loss: 2908.9050\n",
      "Epoch 852/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2505.0945 - val_loss: 2908.5271\n",
      "Epoch 853/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2504.4568 - val_loss: 2908.2012\n",
      "Epoch 854/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2503.8308 - val_loss: 2907.8101\n",
      "Epoch 855/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2503.1995 - val_loss: 2907.3894\n",
      "Epoch 856/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2502.5886 - val_loss: 2906.9934\n",
      "Epoch 857/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2501.9316 - val_loss: 2906.5688\n",
      "Epoch 858/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2501.2998 - val_loss: 2906.1650\n",
      "Epoch 859/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2500.6714 - val_loss: 2905.8315\n",
      "Epoch 860/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2500.0413 - val_loss: 2905.4893\n",
      "Epoch 861/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2499.4421 - val_loss: 2905.1453\n",
      "Epoch 862/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2498.7898 - val_loss: 2904.7246\n",
      "Epoch 863/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2498.1602 - val_loss: 2904.3647\n",
      "Epoch 864/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2497.5474 - val_loss: 2904.0081\n",
      "Epoch 865/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2496.9133 - val_loss: 2903.6462\n",
      "Epoch 866/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2496.2778 - val_loss: 2903.2871\n",
      "Epoch 867/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2495.6545 - val_loss: 2902.9102\n",
      "Epoch 868/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2495.0623 - val_loss: 2902.5447\n",
      "Epoch 869/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2494.4487 - val_loss: 2902.1885\n",
      "Epoch 870/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2493.8145 - val_loss: 2901.8145\n",
      "Epoch 871/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2493.1982 - val_loss: 2901.3733\n",
      "Epoch 872/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2492.5740 - val_loss: 2901.0632\n",
      "Epoch 873/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2491.9846 - val_loss: 2900.7576\n",
      "Epoch 874/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2491.3689 - val_loss: 2900.4243\n",
      "Epoch 875/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2490.7522 - val_loss: 2900.0732\n",
      "Epoch 876/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2490.1216 - val_loss: 2899.7144\n",
      "Epoch 877/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2489.5325 - val_loss: 2899.3591\n",
      "Epoch 878/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2488.9365 - val_loss: 2898.9600\n",
      "Epoch 879/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2488.3308 - val_loss: 2898.6199\n",
      "Epoch 880/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2487.7190 - val_loss: 2898.0737\n",
      "Epoch 881/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2487.1292 - val_loss: 2897.6833\n",
      "Epoch 882/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2486.5156 - val_loss: 2897.3350\n",
      "Epoch 883/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2485.9297 - val_loss: 2897.0081\n",
      "Epoch 884/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2485.3262 - val_loss: 2896.7102\n",
      "Epoch 885/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2484.7241 - val_loss: 2896.3525\n",
      "Epoch 886/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2484.1121 - val_loss: 2895.9802\n",
      "Epoch 887/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2483.5361 - val_loss: 2895.6543\n",
      "Epoch 888/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2482.8997 - val_loss: 2895.2913\n",
      "Epoch 889/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2482.3291 - val_loss: 2894.9529\n",
      "Epoch 890/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2481.7214 - val_loss: 2894.6353\n",
      "Epoch 891/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2481.1069 - val_loss: 2894.2932\n",
      "Epoch 892/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2480.5300 - val_loss: 2893.9749\n",
      "Epoch 893/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2479.9363 - val_loss: 2893.5684\n",
      "Epoch 894/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2479.3269 - val_loss: 2893.2151\n",
      "Epoch 895/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2478.7512 - val_loss: 2892.8755\n",
      "Epoch 896/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2478.1338 - val_loss: 2892.5076\n",
      "Epoch 897/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2477.5662 - val_loss: 2892.1855\n",
      "Epoch 898/1000\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 2476.9741 - val_loss: 2891.8376\n",
      "Epoch 899/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2476.3779 - val_loss: 2891.5237\n",
      "Epoch 900/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2475.7781 - val_loss: 2891.2175\n",
      "Epoch 901/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2475.2046 - val_loss: 2890.8958\n",
      "Epoch 902/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2474.6047 - val_loss: 2890.6013\n",
      "Epoch 903/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2474.0378 - val_loss: 2890.2917\n",
      "Epoch 904/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2473.4634 - val_loss: 2890.0056\n",
      "Epoch 905/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2472.8518 - val_loss: 2889.6829\n",
      "Epoch 906/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2472.2964 - val_loss: 2889.2871\n",
      "Epoch 907/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2471.7146 - val_loss: 2889.0144\n",
      "Epoch 908/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2471.1597 - val_loss: 2888.6948\n",
      "Epoch 909/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2470.5566 - val_loss: 2888.3552\n",
      "Epoch 910/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2469.9573 - val_loss: 2888.0488\n",
      "Epoch 911/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2469.4160 - val_loss: 2887.6992\n",
      "Epoch 912/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2468.8032 - val_loss: 2887.3884\n",
      "Epoch 913/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2468.2922 - val_loss: 2887.0784\n",
      "Epoch 914/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2467.6843 - val_loss: 2886.7375\n",
      "Epoch 915/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2467.1228 - val_loss: 2886.4583\n",
      "Epoch 916/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2466.5427 - val_loss: 2886.1975\n",
      "Epoch 917/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2465.9709 - val_loss: 2885.8916\n",
      "Epoch 918/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2465.3938 - val_loss: 2885.5752\n",
      "Epoch 919/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2464.8176 - val_loss: 2885.2268\n",
      "Epoch 920/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2464.2903 - val_loss: 2884.9863\n",
      "Epoch 921/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2463.7114 - val_loss: 2884.7393\n",
      "Epoch 922/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2463.1443 - val_loss: 2884.4109\n",
      "Epoch 923/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2462.5505 - val_loss: 2884.0674\n",
      "Epoch 924/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2462.0129 - val_loss: 2883.7527\n",
      "Epoch 925/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2461.4502 - val_loss: 2883.3979\n",
      "Epoch 926/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2460.8884 - val_loss: 2883.0867\n",
      "Epoch 927/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2460.3315 - val_loss: 2882.8250\n",
      "Epoch 928/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2459.7627 - val_loss: 2882.5151\n",
      "Epoch 929/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2459.2136 - val_loss: 2882.2234\n",
      "Epoch 930/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2458.6753 - val_loss: 2881.8811\n",
      "Epoch 931/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2458.1138 - val_loss: 2881.5562\n",
      "Epoch 932/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2457.5342 - val_loss: 2881.2148\n",
      "Epoch 933/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2457.0178 - val_loss: 2880.9224\n",
      "Epoch 934/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2456.4617 - val_loss: 2880.6389\n",
      "Epoch 935/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2455.9092 - val_loss: 2880.3342\n",
      "Epoch 936/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2455.3403 - val_loss: 2880.0322\n",
      "Epoch 937/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2454.8049 - val_loss: 2879.7107\n",
      "Epoch 938/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2454.2463 - val_loss: 2879.4165\n",
      "Epoch 939/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2453.6904 - val_loss: 2879.0967\n",
      "Epoch 940/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2453.1489 - val_loss: 2878.7595\n",
      "Epoch 941/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2452.5979 - val_loss: 2878.4922\n",
      "Epoch 942/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2452.0635 - val_loss: 2878.2014\n",
      "Epoch 943/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2451.5422 - val_loss: 2877.9180\n",
      "Epoch 944/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2450.9817 - val_loss: 2877.6094\n",
      "Epoch 945/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2450.4351 - val_loss: 2877.3926\n",
      "Epoch 946/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2449.8860 - val_loss: 2876.8757\n",
      "Epoch 947/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2449.3665 - val_loss: 2876.5867\n",
      "Epoch 948/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2448.8247 - val_loss: 2876.2622\n",
      "Epoch 949/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2448.2734 - val_loss: 2875.9438\n",
      "Epoch 950/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2447.7727 - val_loss: 2875.6455\n",
      "Epoch 951/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2447.1924 - val_loss: 2875.3569\n",
      "Epoch 952/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2446.6887 - val_loss: 2875.0232\n",
      "Epoch 953/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2446.1455 - val_loss: 2874.7290\n",
      "Epoch 954/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2445.5999 - val_loss: 2874.4580\n",
      "Epoch 955/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2445.0481 - val_loss: 2874.1965\n",
      "Epoch 956/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2444.5474 - val_loss: 2873.9011\n",
      "Epoch 957/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2443.9822 - val_loss: 2873.5933\n",
      "Epoch 958/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2443.4934 - val_loss: 2873.3115\n",
      "Epoch 959/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2442.9507 - val_loss: 2873.0681\n",
      "Epoch 960/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2442.4456 - val_loss: 2872.8037\n",
      "Epoch 961/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2441.9158 - val_loss: 2872.5603\n",
      "Epoch 962/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2441.3645 - val_loss: 2872.3037\n",
      "Epoch 963/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2440.8708 - val_loss: 2871.9604\n",
      "Epoch 964/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2440.3301 - val_loss: 2871.6926\n",
      "Epoch 965/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2439.7961 - val_loss: 2871.3503\n",
      "Epoch 966/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2439.3008 - val_loss: 2871.0632\n",
      "Epoch 967/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2438.7917 - val_loss: 2870.7354\n",
      "Epoch 968/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2438.2229 - val_loss: 2870.5183\n",
      "Epoch 969/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2437.7205 - val_loss: 2870.2432\n",
      "Epoch 970/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2437.2139 - val_loss: 2869.9165\n",
      "Epoch 971/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2436.6748 - val_loss: 2869.7324\n",
      "Epoch 972/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2436.1663 - val_loss: 2869.4434\n",
      "Epoch 973/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2435.6379 - val_loss: 2869.1760\n",
      "Epoch 974/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2435.1218 - val_loss: 2868.9446\n",
      "Epoch 975/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2434.5928 - val_loss: 2868.6499\n",
      "Epoch 976/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2434.0845 - val_loss: 2868.3521\n",
      "Epoch 977/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2433.5232 - val_loss: 2868.0403\n",
      "Epoch 978/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2433.0344 - val_loss: 2867.7661\n",
      "Epoch 979/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2432.5576 - val_loss: 2867.5510\n",
      "Epoch 980/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2432.0095 - val_loss: 2867.2600\n",
      "Epoch 981/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2431.5212 - val_loss: 2867.0073\n",
      "Epoch 982/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2431.0076 - val_loss: 2866.7324\n",
      "Epoch 983/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2430.4709 - val_loss: 2866.4187\n",
      "Epoch 984/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2429.9661 - val_loss: 2866.2144\n",
      "Epoch 985/1000\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 2429.4778 - val_loss: 2865.9834\n",
      "Epoch 986/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2428.9648 - val_loss: 2865.7043\n",
      "Epoch 987/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2428.4370 - val_loss: 2865.4182\n",
      "Epoch 988/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2427.9324 - val_loss: 2865.0911\n",
      "Epoch 989/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2427.4402 - val_loss: 2864.8228\n",
      "Epoch 990/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2426.9294 - val_loss: 2864.5544\n",
      "Epoch 991/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2426.4062 - val_loss: 2864.2849\n",
      "Epoch 992/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2425.9033 - val_loss: 2863.9751\n",
      "Epoch 993/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2425.3835 - val_loss: 2863.7397\n",
      "Epoch 994/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2424.9067 - val_loss: 2863.4790\n",
      "Epoch 995/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2424.4116 - val_loss: 2863.1736\n",
      "Epoch 996/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2423.9004 - val_loss: 2862.8777\n",
      "Epoch 997/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2423.4021 - val_loss: 2862.5688\n",
      "Epoch 998/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2422.8743 - val_loss: 2862.3188\n",
      "Epoch 999/1000\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 2422.3752 - val_loss: 2862.0649\n",
      "Epoch 1000/1000\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 2421.8508 - val_loss: 2861.7749\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 2861.7751\n",
      "RMSE: 53.495561932597504\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad\n",
    "\n",
    "# Choisissez l'optimiseur que vous souhaitez utiliser\n",
    "# optimizer = SGD(learning_rate=0.01)  # Stochastic Gradient Descent\n",
    "# optimizer = RMSprop(learning_rate=0.01)  # RMSprop\n",
    "optimizer = Adagrad(learning_rate=0.001)  # Adagrad\n",
    "\n",
    "# Crez votre modle de rgression\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle avec l'optimiseur choisi\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=128, validation_data=(X_test, y_test))\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "rmse = np.sqrt(loss)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c568c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "32/32 [==============================] - 2s 27ms/step - loss: 8590.3066 - val_loss: 4618.2314\n",
      "Epoch 2/1000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 4071.4854 - val_loss: 3669.0354\n",
      "Epoch 3/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 3122.9165 - val_loss: 2990.2864\n",
      "Epoch 4/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 2532.9058 - val_loss: 2891.0901\n",
      "Epoch 5/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 2352.3887 - val_loss: 2558.8389\n",
      "Epoch 6/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 2268.2222 - val_loss: 2531.5515\n",
      "Epoch 7/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 2133.4099 - val_loss: 2284.1504\n",
      "Epoch 8/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 2109.4089 - val_loss: 2220.5923\n",
      "Epoch 9/1000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 2021.8350 - val_loss: 2301.0081\n",
      "Epoch 10/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1953.9247 - val_loss: 2423.0151\n",
      "Epoch 11/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1907.5590 - val_loss: 2212.8516\n",
      "Epoch 12/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1861.9753 - val_loss: 2360.8574\n",
      "Epoch 13/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1877.5508 - val_loss: 2586.3794\n",
      "Epoch 14/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1835.0388 - val_loss: 2346.3274\n",
      "Epoch 15/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 1817.2488 - val_loss: 2340.5281\n",
      "Epoch 16/1000\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 1791.7109 - val_loss: 2216.9856\n",
      "Epoch 17/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1774.1725 - val_loss: 2444.4236\n",
      "Epoch 18/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1876.2017 - val_loss: 2231.8411\n",
      "Epoch 19/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1678.5389 - val_loss: 2100.6746\n",
      "Epoch 20/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1647.4023 - val_loss: 2316.8242\n",
      "Epoch 21/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1664.7549 - val_loss: 2154.6797\n",
      "Epoch 22/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1662.7795 - val_loss: 2263.6140\n",
      "Epoch 23/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 1606.0028 - val_loss: 2556.9363\n",
      "Epoch 24/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1651.7406 - val_loss: 2313.7876\n",
      "Epoch 25/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1601.2474 - val_loss: 2138.2329\n",
      "Epoch 26/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1546.3569 - val_loss: 2279.5864\n",
      "Epoch 27/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1545.2617 - val_loss: 2128.9888\n",
      "Epoch 28/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1540.0010 - val_loss: 2250.8948\n",
      "Epoch 29/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1556.6198 - val_loss: 2306.0339\n",
      "Epoch 30/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1481.6744 - val_loss: 2087.5696\n",
      "Epoch 31/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1430.5653 - val_loss: 2460.1733\n",
      "Epoch 32/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1490.8872 - val_loss: 2150.6973\n",
      "Epoch 33/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1436.8843 - val_loss: 2136.8276\n",
      "Epoch 34/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1444.1304 - val_loss: 2110.1790\n",
      "Epoch 35/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1485.8783 - val_loss: 2154.1538\n",
      "Epoch 36/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 1394.9641 - val_loss: 2215.8494\n",
      "Epoch 37/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1477.2194 - val_loss: 2870.8879\n",
      "Epoch 38/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1455.7859 - val_loss: 2096.6675\n",
      "Epoch 39/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1380.3622 - val_loss: 2100.5518\n",
      "Epoch 40/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1306.7637 - val_loss: 2082.3140\n",
      "Epoch 41/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 1283.6315 - val_loss: 2267.8538\n",
      "Epoch 42/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1348.2607 - val_loss: 2347.7725\n",
      "Epoch 43/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1367.5627 - val_loss: 2162.4944\n",
      "Epoch 44/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1316.4633 - val_loss: 2099.1523\n",
      "Epoch 45/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1273.2367 - val_loss: 2116.3220\n",
      "Epoch 46/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1309.5604 - val_loss: 2112.9238\n",
      "Epoch 47/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1267.7045 - val_loss: 2064.2869\n",
      "Epoch 48/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1224.1909 - val_loss: 2310.6890\n",
      "Epoch 49/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1276.6737 - val_loss: 2148.8445\n",
      "Epoch 50/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 1308.0752 - val_loss: 2244.7910\n",
      "Epoch 51/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 1228.9972 - val_loss: 2132.0452\n",
      "Epoch 52/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1186.3053 - val_loss: 2080.5525\n",
      "Epoch 53/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1146.2626 - val_loss: 2159.1824\n",
      "Epoch 54/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1121.5620 - val_loss: 2112.6355\n",
      "Epoch 55/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1153.7937 - val_loss: 2433.2231\n",
      "Epoch 56/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1209.6703 - val_loss: 2154.1309\n",
      "Epoch 57/1000\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 1187.2384 - val_loss: 2153.5820\n",
      "Epoch 58/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1148.4077 - val_loss: 2152.3501\n",
      "Epoch 59/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1103.5016 - val_loss: 2155.1777\n",
      "Epoch 60/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1159.7953 - val_loss: 2159.4426\n",
      "Epoch 61/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 1126.6707 - val_loss: 2125.7600\n",
      "Epoch 62/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 1100.9841 - val_loss: 2092.0981\n",
      "Epoch 63/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 1047.6573 - val_loss: 2110.9695\n",
      "Epoch 64/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 1033.6086 - val_loss: 2120.7312\n",
      "Epoch 65/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 1095.9762 - val_loss: 2076.9421\n",
      "Epoch 66/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 1059.9622 - val_loss: 2149.7080\n",
      "Epoch 67/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 1014.8084 - val_loss: 2357.3933\n",
      "Epoch 68/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 1063.6505 - val_loss: 2148.8101\n",
      "Epoch 69/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 1025.5188 - val_loss: 2209.5991\n",
      "Epoch 70/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 999.6080 - val_loss: 2132.2310\n",
      "Epoch 71/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 976.5373 - val_loss: 2125.2100\n",
      "Epoch 72/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 969.8395 - val_loss: 2153.4954\n",
      "Epoch 73/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 954.4497 - val_loss: 2151.6526\n",
      "Epoch 74/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 925.3439 - val_loss: 2087.0898\n",
      "Epoch 75/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 955.9934 - val_loss: 2180.5967\n",
      "Epoch 76/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 897.3789 - val_loss: 2202.2837\n",
      "Epoch 77/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 944.8573 - val_loss: 2210.2690\n",
      "Epoch 78/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 973.0229 - val_loss: 2142.8521\n",
      "Epoch 79/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 952.6216 - val_loss: 2408.2324\n",
      "Epoch 80/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 996.8424 - val_loss: 2117.5061\n",
      "Epoch 81/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 943.2195 - val_loss: 2137.2629\n",
      "Epoch 82/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 879.0443 - val_loss: 2165.9631\n",
      "Epoch 83/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 871.4881 - val_loss: 2367.6838\n",
      "Epoch 84/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 963.0990 - val_loss: 2255.5015\n",
      "Epoch 85/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 886.7878 - val_loss: 2260.0278\n",
      "Epoch 86/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 925.0004 - val_loss: 2415.5837\n",
      "Epoch 87/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 934.1496 - val_loss: 2121.9097\n",
      "Epoch 88/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 925.8521 - val_loss: 2204.5503\n",
      "Epoch 89/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 878.8705 - val_loss: 2167.1907\n",
      "Epoch 90/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 839.3431 - val_loss: 2178.7725\n",
      "Epoch 91/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 816.4831 - val_loss: 2210.7729\n",
      "Epoch 92/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 798.2502 - val_loss: 2284.8354\n",
      "Epoch 93/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 860.2122 - val_loss: 2154.5315\n",
      "Epoch 94/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 899.9183 - val_loss: 2157.4382\n",
      "Epoch 95/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 802.9670 - val_loss: 2231.0142\n",
      "Epoch 96/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 781.1943 - val_loss: 2176.3062\n",
      "Epoch 97/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 788.2813 - val_loss: 2171.4216\n",
      "Epoch 98/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 761.6877 - val_loss: 2190.7520\n",
      "Epoch 99/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 763.4293 - val_loss: 2254.2515\n",
      "Epoch 100/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 765.6887 - val_loss: 2178.0034\n",
      "Epoch 101/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 782.7458 - val_loss: 2187.3186\n",
      "Epoch 102/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 770.3534 - val_loss: 2232.2959\n",
      "Epoch 103/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 722.2722 - val_loss: 2214.6643\n",
      "Epoch 104/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 704.0137 - val_loss: 2205.5532\n",
      "Epoch 105/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 693.4258 - val_loss: 2226.5330\n",
      "Epoch 106/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 703.6263 - val_loss: 2232.9473\n",
      "Epoch 107/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 698.5948 - val_loss: 2330.3096\n",
      "Epoch 108/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 724.7527 - val_loss: 2165.6689\n",
      "Epoch 109/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 698.3214 - val_loss: 2279.7725\n",
      "Epoch 110/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 718.0392 - val_loss: 2192.9221\n",
      "Epoch 111/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 747.3649 - val_loss: 2276.6458\n",
      "Epoch 112/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 700.2085 - val_loss: 2143.8687\n",
      "Epoch 113/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 672.4011 - val_loss: 2241.9302\n",
      "Epoch 114/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 683.2959 - val_loss: 2343.4106\n",
      "Epoch 115/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 718.6034 - val_loss: 2247.7483\n",
      "Epoch 116/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 697.6326 - val_loss: 2439.8911\n",
      "Epoch 117/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 708.7132 - val_loss: 2217.0745\n",
      "Epoch 118/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 661.7880 - val_loss: 2264.3074\n",
      "Epoch 119/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 689.2797 - val_loss: 2248.3647\n",
      "Epoch 120/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 716.8390 - val_loss: 2209.3445\n",
      "Epoch 121/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 648.4058 - val_loss: 2220.0325\n",
      "Epoch 122/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 640.8810 - val_loss: 2312.5300\n",
      "Epoch 123/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 634.3861 - val_loss: 2270.6340\n",
      "Epoch 124/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 622.3924 - val_loss: 2214.4592\n",
      "Epoch 125/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 611.8923 - val_loss: 2224.3569\n",
      "Epoch 126/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 632.4998 - val_loss: 2264.2185\n",
      "Epoch 127/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 634.9359 - val_loss: 2289.0244\n",
      "Epoch 128/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 607.1146 - val_loss: 2318.5242\n",
      "Epoch 129/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 645.9082 - val_loss: 2340.7356\n",
      "Epoch 130/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 659.1971 - val_loss: 2246.0811\n",
      "Epoch 131/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 643.8049 - val_loss: 2291.0583\n",
      "Epoch 132/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 600.2485 - val_loss: 2279.8943\n",
      "Epoch 133/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 632.0882 - val_loss: 2266.9482\n",
      "Epoch 134/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 641.1967 - val_loss: 2341.6667\n",
      "Epoch 135/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 598.1204 - val_loss: 2295.3157\n",
      "Epoch 136/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 582.8784 - val_loss: 2301.3699\n",
      "Epoch 137/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 596.4000 - val_loss: 2491.0618\n",
      "Epoch 138/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 644.2941 - val_loss: 2378.9126\n",
      "Epoch 139/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 601.4303 - val_loss: 2302.2402\n",
      "Epoch 140/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 588.0659 - val_loss: 2330.8684\n",
      "Epoch 141/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 601.1398 - val_loss: 2314.6191\n",
      "Epoch 142/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 582.8640 - val_loss: 2261.9604\n",
      "Epoch 143/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 561.4979 - val_loss: 2271.3745\n",
      "Epoch 144/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 561.6933 - val_loss: 2309.5886\n",
      "Epoch 145/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 558.8371 - val_loss: 2270.8291\n",
      "Epoch 146/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 561.5417 - val_loss: 2335.2070\n",
      "Epoch 147/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 555.2128 - val_loss: 2403.8103\n",
      "Epoch 148/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 580.3814 - val_loss: 2320.6064\n",
      "Epoch 149/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 554.8581 - val_loss: 2303.6118\n",
      "Epoch 150/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 531.4996 - val_loss: 2344.1162\n",
      "Epoch 151/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 554.8994 - val_loss: 2248.6919\n",
      "Epoch 152/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 544.7531 - val_loss: 2282.0889\n",
      "Epoch 153/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 544.4018 - val_loss: 2275.5405\n",
      "Epoch 154/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 537.7570 - val_loss: 2375.4844\n",
      "Epoch 155/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 612.4352 - val_loss: 2313.8318\n",
      "Epoch 156/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 610.7744 - val_loss: 2225.4302\n",
      "Epoch 157/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 540.7358 - val_loss: 2317.9753\n",
      "Epoch 158/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 557.0186 - val_loss: 2268.6514\n",
      "Epoch 159/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 541.6688 - val_loss: 2287.6038\n",
      "Epoch 160/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 509.3344 - val_loss: 2295.4612\n",
      "Epoch 161/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 546.4624 - val_loss: 2356.8857\n",
      "Epoch 162/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 539.0300 - val_loss: 2339.7947\n",
      "Epoch 163/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 535.4574 - val_loss: 2281.1865\n",
      "Epoch 164/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 532.7726 - val_loss: 2453.1885\n",
      "Epoch 165/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 572.4072 - val_loss: 2328.0342\n",
      "Epoch 166/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 552.0137 - val_loss: 2324.7078\n",
      "Epoch 167/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 517.5407 - val_loss: 2266.0525\n",
      "Epoch 168/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 504.2392 - val_loss: 2357.3042\n",
      "Epoch 169/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 696.5795 - val_loss: 2366.8992\n",
      "Epoch 170/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 599.2632 - val_loss: 2312.0159\n",
      "Epoch 171/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 559.7699 - val_loss: 2332.9561\n",
      "Epoch 172/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 535.0065 - val_loss: 2324.1616\n",
      "Epoch 173/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 534.9003 - val_loss: 2364.9219\n",
      "Epoch 174/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 512.0192 - val_loss: 2308.6790\n",
      "Epoch 175/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 492.2822 - val_loss: 2288.9204\n",
      "Epoch 176/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 482.1279 - val_loss: 2351.7490\n",
      "Epoch 177/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 486.4443 - val_loss: 2312.9404\n",
      "Epoch 178/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 487.1666 - val_loss: 2644.5505\n",
      "Epoch 179/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 545.2281 - val_loss: 2321.2830\n",
      "Epoch 180/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 505.3887 - val_loss: 2314.8179\n",
      "Epoch 181/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 488.7038 - val_loss: 2335.2280\n",
      "Epoch 182/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 484.9707 - val_loss: 2403.8589\n",
      "Epoch 183/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 488.5050 - val_loss: 2352.3335\n",
      "Epoch 184/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 518.6812 - val_loss: 2414.3528\n",
      "Epoch 185/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 526.9988 - val_loss: 2310.2981\n",
      "Epoch 186/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 499.1473 - val_loss: 2361.1274\n",
      "Epoch 187/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 462.6068 - val_loss: 2344.8240\n",
      "Epoch 188/1000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 496.3893 - val_loss: 2305.6084\n",
      "Epoch 189/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 513.0818 - val_loss: 2347.0510\n",
      "Epoch 190/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 486.4384 - val_loss: 2252.4644\n",
      "Epoch 191/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 462.4349 - val_loss: 2259.8926\n",
      "Epoch 192/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 438.8248 - val_loss: 2336.4128\n",
      "Epoch 193/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 460.3913 - val_loss: 2291.4431\n",
      "Epoch 194/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 464.4671 - val_loss: 2341.7495\n",
      "Epoch 195/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 453.3826 - val_loss: 2339.9834\n",
      "Epoch 196/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 473.8393 - val_loss: 2369.8618\n",
      "Epoch 197/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 469.2369 - val_loss: 2302.5913\n",
      "Epoch 198/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 462.8542 - val_loss: 2314.4199\n",
      "Epoch 199/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 476.2262 - val_loss: 2484.8728\n",
      "Epoch 200/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 558.5182 - val_loss: 2323.0010\n",
      "Epoch 201/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 531.6476 - val_loss: 2376.0513\n",
      "Epoch 202/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 544.8397 - val_loss: 2429.1772\n",
      "Epoch 203/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 550.0933 - val_loss: 2335.3325\n",
      "Epoch 204/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 530.9874 - val_loss: 2406.0547\n",
      "Epoch 205/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 550.9984 - val_loss: 2362.0740\n",
      "Epoch 206/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 538.2212 - val_loss: 2523.1155\n",
      "Epoch 207/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 566.9749 - val_loss: 2399.2505\n",
      "Epoch 208/1000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 528.1218 - val_loss: 2382.2490\n",
      "Epoch 209/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 479.6395 - val_loss: 2386.1477\n",
      "Epoch 210/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 483.3337 - val_loss: 2360.5342\n",
      "Epoch 211/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 466.7269 - val_loss: 2374.2068\n",
      "Epoch 212/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 521.2525 - val_loss: 2442.4106\n",
      "Epoch 213/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 495.9773 - val_loss: 2389.1621\n",
      "Epoch 214/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 483.9036 - val_loss: 2405.2888\n",
      "Epoch 215/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 501.9038 - val_loss: 2331.7368\n",
      "Epoch 216/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 442.7864 - val_loss: 2371.2759\n",
      "Epoch 217/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 440.1131 - val_loss: 2357.1821\n",
      "Epoch 218/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 449.8226 - val_loss: 2417.2888\n",
      "Epoch 219/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 444.4971 - val_loss: 2384.5420\n",
      "Epoch 220/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 463.5451 - val_loss: 2286.1365\n",
      "Epoch 221/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 443.6275 - val_loss: 2293.2349\n",
      "Epoch 222/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 455.9119 - val_loss: 2351.5071\n",
      "Epoch 223/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 454.4796 - val_loss: 2393.2727\n",
      "Epoch 224/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 445.9110 - val_loss: 2392.5862\n",
      "Epoch 225/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 446.2524 - val_loss: 2346.2512\n",
      "Epoch 226/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 456.8069 - val_loss: 2338.3127\n",
      "Epoch 227/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 434.7360 - val_loss: 2333.3130\n",
      "Epoch 228/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 447.4142 - val_loss: 2347.7092\n",
      "Epoch 229/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 482.7871 - val_loss: 2383.2856\n",
      "Epoch 230/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 476.4605 - val_loss: 2359.2129\n",
      "Epoch 231/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 439.0458 - val_loss: 2381.0425\n",
      "Epoch 232/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 436.8425 - val_loss: 2322.8191\n",
      "Epoch 233/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 517.6960 - val_loss: 2385.1235\n",
      "Epoch 234/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 507.7201 - val_loss: 2414.8062\n",
      "Epoch 235/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 491.5549 - val_loss: 2325.6025\n",
      "Epoch 236/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 458.4627 - val_loss: 2347.7537\n",
      "Epoch 237/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 451.9429 - val_loss: 2406.9736\n",
      "Epoch 238/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 466.7213 - val_loss: 2390.7656\n",
      "Epoch 239/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 454.8582 - val_loss: 2336.1777\n",
      "Epoch 240/1000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 432.9983 - val_loss: 2324.2363\n",
      "Epoch 241/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 479.8085 - val_loss: 2359.9758\n",
      "Epoch 242/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 469.4340 - val_loss: 2315.5100\n",
      "Epoch 243/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 431.5267 - val_loss: 2335.5007\n",
      "Epoch 244/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 460.0913 - val_loss: 2456.6978\n",
      "Epoch 245/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 442.0849 - val_loss: 2363.5225\n",
      "Epoch 246/1000\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 423.0774 - val_loss: 2361.7651\n",
      "Epoch 247/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 429.4334 - val_loss: 2406.1934\n",
      "Epoch 248/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 445.2122 - val_loss: 2408.3694\n",
      "Epoch 249/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 420.8381 - val_loss: 2429.0627\n",
      "Epoch 250/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 487.8006 - val_loss: 2455.6907\n",
      "Epoch 251/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 495.0916 - val_loss: 2441.1589\n",
      "Epoch 252/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 450.7436 - val_loss: 2368.7563\n",
      "Epoch 253/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 422.9844 - val_loss: 2450.4778\n",
      "Epoch 254/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 430.2102 - val_loss: 2414.3215\n",
      "Epoch 255/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 417.0113 - val_loss: 2378.1858\n",
      "Epoch 256/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 427.4017 - val_loss: 2418.5461\n",
      "Epoch 257/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 442.1168 - val_loss: 2367.8804\n",
      "Epoch 258/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 413.0372 - val_loss: 2517.9131\n",
      "Epoch 259/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 451.1615 - val_loss: 2408.5437\n",
      "Epoch 260/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 437.6119 - val_loss: 2389.1392\n",
      "Epoch 261/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 478.4255 - val_loss: 2390.8523\n",
      "Epoch 262/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 421.5088 - val_loss: 2448.1765\n",
      "Epoch 263/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 425.6610 - val_loss: 2354.0386\n",
      "Epoch 264/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 409.4919 - val_loss: 2340.8325\n",
      "Epoch 265/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 448.4445 - val_loss: 2392.4727\n",
      "Epoch 266/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 442.8920 - val_loss: 2335.0847\n",
      "Epoch 267/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 406.6188 - val_loss: 2488.0403\n",
      "Epoch 268/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 456.9764 - val_loss: 2336.9146\n",
      "Epoch 269/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 423.2496 - val_loss: 2305.2500\n",
      "Epoch 270/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 421.9496 - val_loss: 2381.7244\n",
      "Epoch 271/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 427.2532 - val_loss: 2344.8291\n",
      "Epoch 272/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 413.9264 - val_loss: 2375.9902\n",
      "Epoch 273/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 418.9038 - val_loss: 2341.3101\n",
      "Epoch 274/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 433.2907 - val_loss: 2336.1135\n",
      "Epoch 275/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 415.0743 - val_loss: 2402.5469\n",
      "Epoch 276/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 414.0376 - val_loss: 2390.7673\n",
      "Epoch 277/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 411.0920 - val_loss: 2392.7444\n",
      "Epoch 278/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 444.2478 - val_loss: 2283.5203\n",
      "Epoch 279/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 434.5211 - val_loss: 2390.7812\n",
      "Epoch 280/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 420.7747 - val_loss: 2391.5217\n",
      "Epoch 281/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 424.7306 - val_loss: 2316.4648\n",
      "Epoch 282/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 384.3887 - val_loss: 2389.2073\n",
      "Epoch 283/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 393.3806 - val_loss: 2420.7441\n",
      "Epoch 284/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 393.6125 - val_loss: 2366.6426\n",
      "Epoch 285/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 405.6985 - val_loss: 2569.6179\n",
      "Epoch 286/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 419.6471 - val_loss: 2475.2446\n",
      "Epoch 287/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 413.3239 - val_loss: 2372.5515\n",
      "Epoch 288/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 405.3014 - val_loss: 2393.5552\n",
      "Epoch 289/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 411.9329 - val_loss: 2341.1870\n",
      "Epoch 290/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 400.4853 - val_loss: 2379.4531\n",
      "Epoch 291/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 407.9518 - val_loss: 2357.6594\n",
      "Epoch 292/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 467.2284 - val_loss: 2399.8997\n",
      "Epoch 293/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 503.9244 - val_loss: 2395.2869\n",
      "Epoch 294/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 459.9853 - val_loss: 2369.2976\n",
      "Epoch 295/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 514.8844 - val_loss: 2318.1365\n",
      "Epoch 296/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 437.0059 - val_loss: 2371.1094\n",
      "Epoch 297/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 407.0811 - val_loss: 2333.5339\n",
      "Epoch 298/1000\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 382.2688 - val_loss: 2379.3430\n",
      "Epoch 299/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 417.4625 - val_loss: 2354.5205\n",
      "Epoch 300/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 393.8486 - val_loss: 2308.8486\n",
      "Epoch 301/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 397.9148 - val_loss: 2557.4941\n",
      "Epoch 302/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 427.3956 - val_loss: 2316.7227\n",
      "Epoch 303/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 386.9521 - val_loss: 2364.3606\n",
      "Epoch 304/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 386.0984 - val_loss: 2308.1780\n",
      "Epoch 305/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 373.9587 - val_loss: 2322.2109\n",
      "Epoch 306/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 384.4533 - val_loss: 2337.1467\n",
      "Epoch 307/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 382.8480 - val_loss: 2337.3169\n",
      "Epoch 308/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 391.8553 - val_loss: 2345.4324\n",
      "Epoch 309/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 383.9818 - val_loss: 2357.8430\n",
      "Epoch 310/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 387.9142 - val_loss: 2383.9834\n",
      "Epoch 311/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 379.4254 - val_loss: 2391.5161\n",
      "Epoch 312/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 384.9779 - val_loss: 2332.5049\n",
      "Epoch 313/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 386.2274 - val_loss: 2330.2627\n",
      "Epoch 314/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 389.3026 - val_loss: 2391.1218\n",
      "Epoch 315/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 395.6612 - val_loss: 2387.0051\n",
      "Epoch 316/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 392.1715 - val_loss: 2381.4041\n",
      "Epoch 317/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 393.0101 - val_loss: 2351.5725\n",
      "Epoch 318/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 389.8379 - val_loss: 2350.5188\n",
      "Epoch 319/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 403.2925 - val_loss: 2373.0706\n",
      "Epoch 320/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 382.4875 - val_loss: 2331.1519\n",
      "Epoch 321/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 391.3691 - val_loss: 2393.1384\n",
      "Epoch 322/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 389.8196 - val_loss: 2391.3767\n",
      "Epoch 323/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 413.1665 - val_loss: 2348.0940\n",
      "Epoch 324/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 380.2249 - val_loss: 2320.2751\n",
      "Epoch 325/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 377.9469 - val_loss: 2425.9519\n",
      "Epoch 326/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 405.8831 - val_loss: 2340.1958\n",
      "Epoch 327/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 403.2887 - val_loss: 2430.1516\n",
      "Epoch 328/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 391.9637 - val_loss: 2346.3525\n",
      "Epoch 329/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 397.8895 - val_loss: 2400.0906\n",
      "Epoch 330/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 446.0548 - val_loss: 2356.5217\n",
      "Epoch 331/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 439.0903 - val_loss: 2401.2354\n",
      "Epoch 332/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 442.5417 - val_loss: 2272.6060\n",
      "Epoch 333/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 431.6345 - val_loss: 2365.9141\n",
      "Epoch 334/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 402.7421 - val_loss: 2345.3887\n",
      "Epoch 335/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 386.5824 - val_loss: 2372.4971\n",
      "Epoch 336/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 414.8664 - val_loss: 2362.5051\n",
      "Epoch 337/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 401.0275 - val_loss: 2383.3853\n",
      "Epoch 338/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 395.5303 - val_loss: 2327.2715\n",
      "Epoch 339/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 402.2868 - val_loss: 2359.1558\n",
      "Epoch 340/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 391.0314 - val_loss: 2380.9329\n",
      "Epoch 341/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 399.7598 - val_loss: 2315.6655\n",
      "Epoch 342/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 400.7089 - val_loss: 2370.0322\n",
      "Epoch 343/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 451.1782 - val_loss: 2344.0093\n",
      "Epoch 344/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 408.2799 - val_loss: 2334.4751\n",
      "Epoch 345/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 374.3781 - val_loss: 2376.9229\n",
      "Epoch 346/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 377.8063 - val_loss: 2321.9810\n",
      "Epoch 347/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 386.3133 - val_loss: 2272.9458\n",
      "Epoch 348/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 370.7086 - val_loss: 2395.7766\n",
      "Epoch 349/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 382.8606 - val_loss: 2288.1008\n",
      "Epoch 350/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 368.1709 - val_loss: 2305.4529\n",
      "Epoch 351/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 374.9574 - val_loss: 2306.3135\n",
      "Epoch 352/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 377.5676 - val_loss: 2330.0171\n",
      "Epoch 353/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 377.2419 - val_loss: 2338.5210\n",
      "Epoch 354/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 376.2179 - val_loss: 2336.5864\n",
      "Epoch 355/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 370.2628 - val_loss: 2393.3147\n",
      "Epoch 356/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 370.4819 - val_loss: 2301.3372\n",
      "Epoch 357/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 368.4824 - val_loss: 2299.6514\n",
      "Epoch 358/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 389.2777 - val_loss: 2345.6335\n",
      "Epoch 359/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 364.6412 - val_loss: 2346.0266\n",
      "Epoch 360/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 370.2820 - val_loss: 2323.9429\n",
      "Epoch 361/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 366.7935 - val_loss: 2281.7837\n",
      "Epoch 362/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 344.8226 - val_loss: 2336.3308\n",
      "Epoch 363/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 393.9011 - val_loss: 2403.9092\n",
      "Epoch 364/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 377.9948 - val_loss: 2433.7241\n",
      "Epoch 365/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 382.3756 - val_loss: 2338.6956\n",
      "Epoch 366/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 370.2523 - val_loss: 2306.5588\n",
      "Epoch 367/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 371.6614 - val_loss: 2413.6052\n",
      "Epoch 368/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 373.6311 - val_loss: 2332.9216\n",
      "Epoch 369/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 391.6585 - val_loss: 2329.7639\n",
      "Epoch 370/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 407.6490 - val_loss: 2329.1553\n",
      "Epoch 371/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 416.0857 - val_loss: 2317.8201\n",
      "Epoch 372/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 403.1801 - val_loss: 2391.1130\n",
      "Epoch 373/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 419.1884 - val_loss: 2362.0120\n",
      "Epoch 374/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 387.0366 - val_loss: 2292.1831\n",
      "Epoch 375/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 430.9228 - val_loss: 3630.4094\n",
      "Epoch 376/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 530.1571 - val_loss: 2696.5283\n",
      "Epoch 377/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 559.7311 - val_loss: 2426.4512\n",
      "Epoch 378/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 525.1412 - val_loss: 2363.1101\n",
      "Epoch 379/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 493.3752 - val_loss: 2398.0688\n",
      "Epoch 380/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 449.7971 - val_loss: 2360.9871\n",
      "Epoch 381/1000\n",
      "32/32 [==============================] - 1s 20ms/step - loss: 423.5523 - val_loss: 2347.5664\n",
      "Epoch 382/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 379.3227 - val_loss: 2422.4641\n",
      "Epoch 383/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 428.3165 - val_loss: 2395.7153\n",
      "Epoch 384/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 381.9171 - val_loss: 2335.6147\n",
      "Epoch 385/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 382.2596 - val_loss: 2335.0786\n",
      "Epoch 386/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 388.6490 - val_loss: 2363.1162\n",
      "Epoch 387/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 360.7260 - val_loss: 2338.6501\n",
      "Epoch 388/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 352.3583 - val_loss: 2392.2402\n",
      "Epoch 389/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 392.0673 - val_loss: 2392.4534\n",
      "Epoch 390/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 389.2879 - val_loss: 2361.7725\n",
      "Epoch 391/1000\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 370.9938 - val_loss: 2405.1558\n",
      "Epoch 392/1000\n",
      "32/32 [==============================] - 1s 22ms/step - loss: 358.5617 - val_loss: 2363.9927\n",
      "Epoch 393/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 352.1818 - val_loss: 2337.5564\n",
      "Epoch 394/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 344.8098 - val_loss: 2344.7732\n",
      "Epoch 395/1000\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 360.9268 - val_loss: 2340.3110\n",
      "Epoch 396/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 338.4073 - val_loss: 2411.0894\n",
      "Epoch 397/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 347.3956 - val_loss: 2344.2683\n",
      "Epoch 398/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 344.9256 - val_loss: 2320.8875\n",
      "Epoch 399/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 348.6283 - val_loss: 2389.0740\n",
      "Epoch 400/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 343.6143 - val_loss: 2397.7046\n",
      "Epoch 401/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 342.1041 - val_loss: 2383.2236\n",
      "Epoch 402/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 331.5382 - val_loss: 2400.6838\n",
      "Epoch 403/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 331.0948 - val_loss: 2345.1177\n",
      "Epoch 404/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 332.3872 - val_loss: 2358.1465\n",
      "Epoch 405/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 344.3488 - val_loss: 2370.9558\n",
      "Epoch 406/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 332.3335 - val_loss: 2345.6956\n",
      "Epoch 407/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 333.8015 - val_loss: 2328.9646\n",
      "Epoch 408/1000\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 322.2513 - val_loss: 2360.1479\n",
      "Epoch 409/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 327.0642 - val_loss: 2357.1960\n",
      "Epoch 410/1000\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 326.6037 - val_loss: 2354.3894\n",
      "Epoch 411/1000\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 332.8650 - val_loss: 2437.5969\n",
      "Epoch 412/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 342.3214 - val_loss: 2320.2847\n",
      "Epoch 413/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 334.8253 - val_loss: 2383.1077\n",
      "Epoch 414/1000\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 347.1447 - val_loss: 2427.3306\n",
      "Epoch 415/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 359.9579 - val_loss: 2367.5254\n",
      "Epoch 416/1000\n",
      "32/32 [==============================] - 1s 41ms/step - loss: 378.0313 - val_loss: 2363.4995\n",
      "Epoch 417/1000\n",
      "32/32 [==============================] - 1s 37ms/step - loss: 383.7350 - val_loss: 2344.3716\n",
      "Epoch 418/1000\n",
      "32/32 [==============================] - 1s 40ms/step - loss: 363.7118 - val_loss: 2417.8872\n",
      "Epoch 419/1000\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 358.6043 - val_loss: 2432.5103\n",
      "Epoch 420/1000\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 353.6615 - val_loss: 2337.5422\n",
      "Epoch 421/1000\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 359.5137 - val_loss: 2443.2961\n",
      "Epoch 422/1000\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 380.1718 - val_loss: 2417.8574\n",
      "Epoch 423/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 379.1381 - val_loss: 2348.4084\n",
      "Epoch 424/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 359.0839 - val_loss: 2339.1885\n",
      "Epoch 425/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 366.1471 - val_loss: 2321.6389\n",
      "Epoch 426/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 361.7781 - val_loss: 2336.3862\n",
      "Epoch 427/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 355.2607 - val_loss: 2335.9290\n",
      "Epoch 428/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 351.4331 - val_loss: 2344.7751\n",
      "Epoch 429/1000\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 340.4472 - val_loss: 2328.2585\n",
      "Epoch 430/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 335.8657 - val_loss: 2351.6069\n",
      "Epoch 431/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 330.8627 - val_loss: 2361.1152\n",
      "Epoch 432/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 328.1932 - val_loss: 2357.7070\n",
      "Epoch 433/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 334.7032 - val_loss: 2315.5537\n",
      "Epoch 434/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 336.2219 - val_loss: 2438.2837\n",
      "Epoch 435/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 343.4603 - val_loss: 2390.9978\n",
      "Epoch 436/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 331.6529 - val_loss: 2341.5007\n",
      "Epoch 437/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 332.5300 - val_loss: 2368.0122\n",
      "Epoch 438/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 326.0134 - val_loss: 2342.9116\n",
      "Epoch 439/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 354.7506 - val_loss: 2372.4707\n",
      "Epoch 440/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 349.8801 - val_loss: 2366.2175\n",
      "Epoch 441/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 340.8616 - val_loss: 2388.9121\n",
      "Epoch 442/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 333.8441 - val_loss: 2331.6216\n",
      "Epoch 443/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 351.4565 - val_loss: 2368.1907\n",
      "Epoch 444/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 337.1345 - val_loss: 2412.8779\n",
      "Epoch 445/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 341.1398 - val_loss: 2418.2305\n",
      "Epoch 446/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 363.8820 - val_loss: 2358.6069\n",
      "Epoch 447/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 345.7309 - val_loss: 2354.1118\n",
      "Epoch 448/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 347.9773 - val_loss: 2375.2913\n",
      "Epoch 449/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 350.5749 - val_loss: 2355.6089\n",
      "Epoch 450/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 344.7054 - val_loss: 2415.9521\n",
      "Epoch 451/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 352.4467 - val_loss: 2375.2637\n",
      "Epoch 452/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 341.0109 - val_loss: 2349.4666\n",
      "Epoch 453/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 340.8439 - val_loss: 2365.5850\n",
      "Epoch 454/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 379.6103 - val_loss: 2382.4172\n",
      "Epoch 455/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 359.6526 - val_loss: 2432.3323\n",
      "Epoch 456/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 363.1633 - val_loss: 2449.4253\n",
      "Epoch 457/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 366.9597 - val_loss: 2360.2751\n",
      "Epoch 458/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 369.1118 - val_loss: 2383.9575\n",
      "Epoch 459/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 351.3116 - val_loss: 2451.8601\n",
      "Epoch 460/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 340.5770 - val_loss: 2373.1262\n",
      "Epoch 461/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 328.9383 - val_loss: 2344.8281\n",
      "Epoch 462/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 351.5263 - val_loss: 2347.1572\n",
      "Epoch 463/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 341.7361 - val_loss: 2333.9539\n",
      "Epoch 464/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 348.0898 - val_loss: 2302.3547\n",
      "Epoch 465/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 335.1122 - val_loss: 2340.8091\n",
      "Epoch 466/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 332.0836 - val_loss: 2350.5715\n",
      "Epoch 467/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 331.6904 - val_loss: 2344.5850\n",
      "Epoch 468/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 323.7611 - val_loss: 2344.6133\n",
      "Epoch 469/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 326.6788 - val_loss: 2402.8582\n",
      "Epoch 470/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 345.3259 - val_loss: 2372.6519\n",
      "Epoch 471/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 330.7148 - val_loss: 2344.1321\n",
      "Epoch 472/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 343.2419 - val_loss: 2389.5991\n",
      "Epoch 473/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 335.2043 - val_loss: 2350.1221\n",
      "Epoch 474/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 329.4387 - val_loss: 2372.3777\n",
      "Epoch 475/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 335.8460 - val_loss: 2374.0530\n",
      "Epoch 476/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 330.6296 - val_loss: 2340.0996\n",
      "Epoch 477/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 324.7241 - val_loss: 2315.5776\n",
      "Epoch 478/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 320.4684 - val_loss: 2384.9907\n",
      "Epoch 479/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 327.4134 - val_loss: 2348.2566\n",
      "Epoch 480/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 324.1262 - val_loss: 2372.9705\n",
      "Epoch 481/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 336.6523 - val_loss: 2385.2666\n",
      "Epoch 482/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 337.4860 - val_loss: 2345.9980\n",
      "Epoch 483/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 349.7557 - val_loss: 2381.6453\n",
      "Epoch 484/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 341.4755 - val_loss: 2319.6743\n",
      "Epoch 485/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 350.0664 - val_loss: 2310.0505\n",
      "Epoch 486/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 344.9288 - val_loss: 2324.9260\n",
      "Epoch 487/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 345.7512 - val_loss: 2476.6226\n",
      "Epoch 488/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 336.7214 - val_loss: 2329.1846\n",
      "Epoch 489/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 343.0841 - val_loss: 2418.9192\n",
      "Epoch 490/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 345.5708 - val_loss: 2318.6826\n",
      "Epoch 491/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 340.8944 - val_loss: 2382.6072\n",
      "Epoch 492/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 347.2121 - val_loss: 2368.1350\n",
      "Epoch 493/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 335.1430 - val_loss: 2316.8049\n",
      "Epoch 494/1000\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 335.7984 - val_loss: 2355.2341\n",
      "Epoch 495/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 339.4734 - val_loss: 2425.4565\n",
      "Epoch 496/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 354.1680 - val_loss: 2404.9705\n",
      "Epoch 497/1000\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 350.2911 - val_loss: 2292.6489\n",
      "Epoch 498/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 338.3429 - val_loss: 2339.5051\n",
      "Epoch 499/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 352.0494 - val_loss: 2295.0334\n",
      "Epoch 500/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 336.3691 - val_loss: 2395.8845\n",
      "Epoch 501/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 335.1122 - val_loss: 2351.1621\n",
      "Epoch 502/1000\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 329.7222 - val_loss: 2328.4189\n",
      "Epoch 503/1000\n",
      "32/32 [==============================] - 1s 38ms/step - loss: 316.2732 - val_loss: 2327.6091\n",
      "Epoch 504/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 318.6587 - val_loss: 2385.7698\n",
      "Epoch 505/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 330.3919 - val_loss: 2330.2395\n",
      "Epoch 506/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 328.5724 - val_loss: 2341.4431\n",
      "Epoch 507/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 327.3907 - val_loss: 2349.7817\n",
      "Epoch 508/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 320.8020 - val_loss: 2373.3848\n",
      "Epoch 509/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 320.6837 - val_loss: 2341.1760\n",
      "Epoch 510/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 315.8715 - val_loss: 2337.6865\n",
      "Epoch 511/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 324.0680 - val_loss: 2335.1895\n",
      "Epoch 512/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 329.4385 - val_loss: 2398.9929\n",
      "Epoch 513/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 332.0150 - val_loss: 2344.0305\n",
      "Epoch 514/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 332.6348 - val_loss: 2302.9368\n",
      "Epoch 515/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 338.8609 - val_loss: 2400.2996\n",
      "Epoch 516/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 332.6710 - val_loss: 2324.3572\n",
      "Epoch 517/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 322.3648 - val_loss: 2463.4810\n",
      "Epoch 518/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 356.6755 - val_loss: 2359.7439\n",
      "Epoch 519/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 340.3134 - val_loss: 2336.6953\n",
      "Epoch 520/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 331.8091 - val_loss: 2347.1311\n",
      "Epoch 521/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 332.2080 - val_loss: 2385.2114\n",
      "Epoch 522/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 318.0461 - val_loss: 2337.9214\n",
      "Epoch 523/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 313.8893 - val_loss: 2332.7034\n",
      "Epoch 524/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 318.4975 - val_loss: 2352.6812\n",
      "Epoch 525/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 312.7157 - val_loss: 2336.6313\n",
      "Epoch 526/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 325.3529 - val_loss: 2363.6213\n",
      "Epoch 527/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 330.2113 - val_loss: 2321.2759\n",
      "Epoch 528/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 328.6710 - val_loss: 2395.3345\n",
      "Epoch 529/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 335.2903 - val_loss: 2335.2378\n",
      "Epoch 530/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 329.4710 - val_loss: 2346.0925\n",
      "Epoch 531/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 336.4542 - val_loss: 2358.3401\n",
      "Epoch 532/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 328.6051 - val_loss: 2379.6687\n",
      "Epoch 533/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 332.9523 - val_loss: 2335.8789\n",
      "Epoch 534/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 340.2036 - val_loss: 2336.3901\n",
      "Epoch 535/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 346.6652 - val_loss: 2290.6228\n",
      "Epoch 536/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 337.3658 - val_loss: 2325.3052\n",
      "Epoch 537/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 337.5004 - val_loss: 2335.3474\n",
      "Epoch 538/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 349.5652 - val_loss: 2368.8877\n",
      "Epoch 539/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 329.8564 - val_loss: 2369.3325\n",
      "Epoch 540/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 319.3682 - val_loss: 2359.8689\n",
      "Epoch 541/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 331.7371 - val_loss: 2380.5771\n",
      "Epoch 542/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 328.9013 - val_loss: 2321.8389\n",
      "Epoch 543/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 323.4210 - val_loss: 2409.4529\n",
      "Epoch 544/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 319.7104 - val_loss: 2312.6050\n",
      "Epoch 545/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 317.0388 - val_loss: 2354.1189\n",
      "Epoch 546/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 325.4532 - val_loss: 2307.5981\n",
      "Epoch 547/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 324.2366 - val_loss: 2350.9507\n",
      "Epoch 548/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 317.9670 - val_loss: 2394.0591\n",
      "Epoch 549/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 317.4075 - val_loss: 2358.4763\n",
      "Epoch 550/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 331.1929 - val_loss: 2308.2097\n",
      "Epoch 551/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 337.1536 - val_loss: 2375.8972\n",
      "Epoch 552/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 348.3433 - val_loss: 2389.8772\n",
      "Epoch 553/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 334.1681 - val_loss: 2413.5562\n",
      "Epoch 554/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 418.3668 - val_loss: 2381.5747\n",
      "Epoch 555/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 367.6131 - val_loss: 2398.0610\n",
      "Epoch 556/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 356.6399 - val_loss: 2347.3035\n",
      "Epoch 557/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 353.5107 - val_loss: 2354.5479\n",
      "Epoch 558/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 361.1369 - val_loss: 2398.8813\n",
      "Epoch 559/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 358.7705 - val_loss: 2321.4668\n",
      "Epoch 560/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 369.1373 - val_loss: 2314.8765\n",
      "Epoch 561/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 344.6324 - val_loss: 2351.0061\n",
      "Epoch 562/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 369.0102 - val_loss: 2352.1577\n",
      "Epoch 563/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 335.0378 - val_loss: 2318.5540\n",
      "Epoch 564/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 338.5803 - val_loss: 2319.1731\n",
      "Epoch 565/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 348.6790 - val_loss: 2359.7437\n",
      "Epoch 566/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 361.9950 - val_loss: 2528.4609\n",
      "Epoch 567/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 361.0097 - val_loss: 2322.3774\n",
      "Epoch 568/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 356.0989 - val_loss: 2375.7595\n",
      "Epoch 569/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 339.2417 - val_loss: 2426.5845\n",
      "Epoch 570/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 347.8527 - val_loss: 2370.0015\n",
      "Epoch 571/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 352.3067 - val_loss: 2335.6582\n",
      "Epoch 572/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 367.5015 - val_loss: 2403.4272\n",
      "Epoch 573/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 345.0851 - val_loss: 2402.8750\n",
      "Epoch 574/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 377.5174 - val_loss: 2341.7681\n",
      "Epoch 575/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 343.2607 - val_loss: 2353.4890\n",
      "Epoch 576/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 340.8425 - val_loss: 2328.9617\n",
      "Epoch 577/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 329.5734 - val_loss: 2376.1262\n",
      "Epoch 578/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 365.7893 - val_loss: 2322.3232\n",
      "Epoch 579/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 339.0727 - val_loss: 2316.7881\n",
      "Epoch 580/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 375.1413 - val_loss: 2324.8770\n",
      "Epoch 581/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 353.0887 - val_loss: 2345.6091\n",
      "Epoch 582/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 349.6260 - val_loss: 2360.5583\n",
      "Epoch 583/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 350.1816 - val_loss: 2556.9348\n",
      "Epoch 584/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 394.0224 - val_loss: 2387.3784\n",
      "Epoch 585/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 348.9997 - val_loss: 2354.4692\n",
      "Epoch 586/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 326.8395 - val_loss: 2368.8877\n",
      "Epoch 587/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 321.5277 - val_loss: 2376.3455\n",
      "Epoch 588/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 331.6479 - val_loss: 2311.3877\n",
      "Epoch 589/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 328.5893 - val_loss: 2355.9255\n",
      "Epoch 590/1000\n",
      "32/32 [==============================] - 1s 35ms/step - loss: 325.9983 - val_loss: 2385.7104\n",
      "Epoch 591/1000\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 314.1129 - val_loss: 2354.4187\n",
      "Epoch 592/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 315.5120 - val_loss: 2325.0615\n",
      "Epoch 593/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 312.9904 - val_loss: 2348.6816\n",
      "Epoch 594/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 314.5088 - val_loss: 2421.4675\n",
      "Epoch 595/1000\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 329.0073 - val_loss: 2371.7424\n",
      "Epoch 596/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 327.4393 - val_loss: 2432.1606\n",
      "Epoch 597/1000\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 317.1879 - val_loss: 2354.2839\n",
      "Epoch 598/1000\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 318.0993 - val_loss: 2372.2632\n",
      "Epoch 599/1000\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 316.7604 - val_loss: 2429.2407\n",
      "Epoch 600/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 345.8604 - val_loss: 2405.1699\n",
      "Epoch 601/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 332.7626 - val_loss: 2397.0996\n",
      "Epoch 602/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 318.2872 - val_loss: 2364.2412\n",
      "Epoch 603/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 322.7094 - val_loss: 2374.7815\n",
      "Epoch 604/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 320.2741 - val_loss: 2373.5510\n",
      "Epoch 605/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 308.1457 - val_loss: 2369.0801\n",
      "Epoch 606/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 306.6901 - val_loss: 2334.7485\n",
      "Epoch 607/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 307.1240 - val_loss: 2446.4829\n",
      "Epoch 608/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 322.0778 - val_loss: 2383.7502\n",
      "Epoch 609/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 316.9406 - val_loss: 2373.1130\n",
      "Epoch 610/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 316.7865 - val_loss: 2371.7273\n",
      "Epoch 611/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 313.4903 - val_loss: 2385.5061\n",
      "Epoch 612/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 324.6778 - val_loss: 2389.3687\n",
      "Epoch 613/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 326.9753 - val_loss: 2407.3582\n",
      "Epoch 614/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 324.8080 - val_loss: 2349.7073\n",
      "Epoch 615/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 322.0002 - val_loss: 2369.8267\n",
      "Epoch 616/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 329.6699 - val_loss: 2388.8904\n",
      "Epoch 617/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 331.8102 - val_loss: 2364.6714\n",
      "Epoch 618/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 317.2699 - val_loss: 2378.2358\n",
      "Epoch 619/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 315.5890 - val_loss: 2392.2681\n",
      "Epoch 620/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 325.9412 - val_loss: 2449.2253\n",
      "Epoch 621/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 339.8181 - val_loss: 2388.7600\n",
      "Epoch 622/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 321.8682 - val_loss: 2393.6067\n",
      "Epoch 623/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 328.1331 - val_loss: 2386.6023\n",
      "Epoch 624/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 334.2248 - val_loss: 2356.9241\n",
      "Epoch 625/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 322.9754 - val_loss: 2343.7686\n",
      "Epoch 626/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 334.8686 - val_loss: 2365.2349\n",
      "Epoch 627/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 312.7518 - val_loss: 2355.6902\n",
      "Epoch 628/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 312.6586 - val_loss: 2358.8706\n",
      "Epoch 629/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 309.5353 - val_loss: 2360.6257\n",
      "Epoch 630/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 319.4006 - val_loss: 2358.5457\n",
      "Epoch 631/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 318.3627 - val_loss: 2379.9436\n",
      "Epoch 632/1000\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 328.1098 - val_loss: 2404.6565\n",
      "Epoch 633/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 317.6397 - val_loss: 2375.9792\n",
      "Epoch 634/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 318.7290 - val_loss: 2312.7454\n",
      "Epoch 635/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 337.1446 - val_loss: 2384.3916\n",
      "Epoch 636/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 328.5070 - val_loss: 2344.8433\n",
      "Epoch 637/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 316.2047 - val_loss: 2324.0364\n",
      "Epoch 638/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 312.4717 - val_loss: 2331.9275\n",
      "Epoch 639/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 303.6838 - val_loss: 2353.5195\n",
      "Epoch 640/1000\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 307.7654 - val_loss: 2480.2903\n",
      "Epoch 641/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 330.4542 - val_loss: 2439.4670\n",
      "Epoch 642/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 351.8007 - val_loss: 2341.3616\n",
      "Epoch 643/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 315.5748 - val_loss: 2355.4775\n",
      "Epoch 644/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 311.8933 - val_loss: 2368.5310\n",
      "Epoch 645/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 308.4472 - val_loss: 2378.7617\n",
      "Epoch 646/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 318.1348 - val_loss: 2392.8633\n",
      "Epoch 647/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 329.5349 - val_loss: 2346.6841\n",
      "Epoch 648/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 330.3146 - val_loss: 2384.9287\n",
      "Epoch 649/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 330.3400 - val_loss: 2349.5754\n",
      "Epoch 650/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 328.8226 - val_loss: 2353.0754\n",
      "Epoch 651/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 321.5108 - val_loss: 2340.3794\n",
      "Epoch 652/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 331.4258 - val_loss: 2325.1223\n",
      "Epoch 653/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 344.5150 - val_loss: 2296.5032\n",
      "Epoch 654/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 321.2577 - val_loss: 2350.6160\n",
      "Epoch 655/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 315.0124 - val_loss: 2340.4160\n",
      "Epoch 656/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 316.3605 - val_loss: 2334.0681\n",
      "Epoch 657/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 323.2209 - val_loss: 2346.2781\n",
      "Epoch 658/1000\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 341.6729 - val_loss: 2318.9189\n",
      "Epoch 659/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 340.6837 - val_loss: 2318.8040\n",
      "Epoch 660/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 324.4239 - val_loss: 2358.1465\n",
      "Epoch 661/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 323.2921 - val_loss: 2316.2612\n",
      "Epoch 662/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 344.1925 - val_loss: 2345.0808\n",
      "Epoch 663/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 327.5603 - val_loss: 2347.1882\n",
      "Epoch 664/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 323.4500 - val_loss: 2316.9556\n",
      "Epoch 665/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 311.7214 - val_loss: 2320.2083\n",
      "Epoch 666/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 336.4637 - val_loss: 2336.7615\n",
      "Epoch 667/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 320.8953 - val_loss: 2351.8928\n",
      "Epoch 668/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 314.7421 - val_loss: 2390.5229\n",
      "Epoch 669/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 313.0696 - val_loss: 2324.9058\n",
      "Epoch 670/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 299.9146 - val_loss: 2372.9944\n",
      "Epoch 671/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 305.9615 - val_loss: 2389.5862\n",
      "Epoch 672/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 311.6581 - val_loss: 2360.3428\n",
      "Epoch 673/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 305.1922 - val_loss: 2415.7688\n",
      "Epoch 674/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 306.3258 - val_loss: 2391.2451\n",
      "Epoch 675/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 312.9437 - val_loss: 2368.9509\n",
      "Epoch 676/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 313.5705 - val_loss: 2349.5547\n",
      "Epoch 677/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 308.1082 - val_loss: 2335.8005\n",
      "Epoch 678/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 298.9525 - val_loss: 2332.3020\n",
      "Epoch 679/1000\n",
      "32/32 [==============================] - 1s 36ms/step - loss: 299.4146 - val_loss: 2358.7561\n",
      "Epoch 680/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 292.6608 - val_loss: 2341.1843\n",
      "Epoch 681/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 293.6557 - val_loss: 2412.1682\n",
      "Epoch 682/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 303.7720 - val_loss: 2399.7820\n",
      "Epoch 683/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 295.1313 - val_loss: 2350.1858\n",
      "Epoch 684/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 299.3505 - val_loss: 2361.7708\n",
      "Epoch 685/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 303.2257 - val_loss: 2375.0042\n",
      "Epoch 686/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 298.6439 - val_loss: 2368.8313\n",
      "Epoch 687/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 303.8432 - val_loss: 2347.0032\n",
      "Epoch 688/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 324.0980 - val_loss: 2361.1045\n",
      "Epoch 689/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 322.1736 - val_loss: 2353.9434\n",
      "Epoch 690/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 320.3380 - val_loss: 2331.2915\n",
      "Epoch 691/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 337.1271 - val_loss: 2417.9246\n",
      "Epoch 692/1000\n",
      "32/32 [==============================] - 1s 33ms/step - loss: 351.1170 - val_loss: 2325.4585\n",
      "Epoch 693/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 352.4797 - val_loss: 2355.7190\n",
      "Epoch 694/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 343.9661 - val_loss: 2318.0483\n",
      "Epoch 695/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 317.8326 - val_loss: 2323.8411\n",
      "Epoch 696/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 315.4213 - val_loss: 2339.4326\n",
      "Epoch 697/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 310.5001 - val_loss: 2343.0962\n",
      "Epoch 698/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 309.2821 - val_loss: 2325.2637\n",
      "Epoch 699/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 318.3340 - val_loss: 2345.7017\n",
      "Epoch 700/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 320.1167 - val_loss: 2384.4124\n",
      "Epoch 701/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 312.7963 - val_loss: 2365.5776\n",
      "Epoch 702/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 310.9652 - val_loss: 2316.2654\n",
      "Epoch 703/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 330.9545 - val_loss: 2402.5969\n",
      "Epoch 704/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 325.7234 - val_loss: 2340.5854\n",
      "Epoch 705/1000\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 321.4012 - val_loss: 2397.9319\n",
      "Epoch 706/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 319.5003 - val_loss: 2311.7966\n",
      "Epoch 707/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 317.5101 - val_loss: 2321.4973\n",
      "Epoch 708/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 313.8009 - val_loss: 2357.6921\n",
      "Epoch 709/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 312.8743 - val_loss: 2324.8354\n",
      "Epoch 710/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 302.5242 - val_loss: 2357.3435\n",
      "Epoch 711/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 302.7504 - val_loss: 2319.6729\n",
      "Epoch 712/1000\n",
      "32/32 [==============================] - 1s 30ms/step - loss: 312.2209 - val_loss: 2401.2800\n",
      "Epoch 713/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 304.7202 - val_loss: 2343.7214\n",
      "Epoch 714/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 301.8244 - val_loss: 2323.8560\n",
      "Epoch 715/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 302.7033 - val_loss: 2432.5364\n",
      "Epoch 716/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 316.9613 - val_loss: 2281.9573\n",
      "Epoch 717/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 311.1284 - val_loss: 2326.7266\n",
      "Epoch 718/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 306.3472 - val_loss: 2339.7358\n",
      "Epoch 719/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 307.5815 - val_loss: 2360.4836\n",
      "Epoch 720/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 303.7618 - val_loss: 2381.7366\n",
      "Epoch 721/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 302.5213 - val_loss: 2329.1035\n",
      "Epoch 722/1000\n",
      "32/32 [==============================] - 1s 25ms/step - loss: 297.7023 - val_loss: 2336.9241\n",
      "Epoch 723/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 297.6346 - val_loss: 2309.1213\n",
      "Epoch 724/1000\n",
      "32/32 [==============================] - 1s 23ms/step - loss: 295.9581 - val_loss: 2393.1479\n",
      "Epoch 725/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 301.6155 - val_loss: 2336.5500\n",
      "Epoch 726/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 309.8387 - val_loss: 2326.3027\n",
      "Epoch 727/1000\n",
      "32/32 [==============================] - 1s 26ms/step - loss: 304.4392 - val_loss: 2329.1074\n",
      "Epoch 728/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 292.2187 - val_loss: 2335.8818\n",
      "Epoch 729/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 302.6806 - val_loss: 2358.2163\n",
      "Epoch 730/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 292.9132 - val_loss: 2341.2834\n",
      "Epoch 731/1000\n",
      "32/32 [==============================] - 1s 28ms/step - loss: 291.1359 - val_loss: 2363.0088\n",
      "Epoch 732/1000\n",
      "32/32 [==============================] - 1s 24ms/step - loss: 309.7481 - val_loss: 2340.6467\n",
      "Epoch 733/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 327.4423 - val_loss: 2379.5864\n",
      "Epoch 734/1000\n",
      "32/32 [==============================] - 1s 29ms/step - loss: 314.6932 - val_loss: 2312.9255\n",
      "Epoch 735/1000\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 314.2947 - val_loss: 2310.3530\n",
      "Epoch 736/1000\n",
      "32/32 [==============================] - 1s 31ms/step - loss: 313.2878 - val_loss: 2377.4661\n",
      "Epoch 737/1000\n",
      "32/32 [==============================] - 1s 27ms/step - loss: 317.1376 - val_loss: 2327.1794\n",
      "Epoch 738/1000\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 326.9206 - val_loss: 2381.1604\n",
      "Epoch 739/1000\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 329.3197 - val_loss: 2306.3198\n",
      "Epoch 740/1000\n",
      " 6/32 [====>.........................] - ETA: 0s - loss: 359.1923"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find a model.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# Entranez le modle\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X40sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m rmse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(loss)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adam\n",
    "\n",
    "optimizer = Adam(learning_rate=0.005)  # Adam\n",
    "\n",
    "# Crez votre modle de rgression\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle avec l'optimiseur choisi\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=512, validation_data=(X_test, y_test))\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "\n",
    "rmse = np.sqrt(loss)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4c5a5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "32/32 [==============================] - 11s 196ms/step - loss: 8650.5840 - val_loss: 5477.4302\n",
      "Epoch 2/100\n",
      "32/32 [==============================] - 83s 3s/step - loss: 4798.6538 - val_loss: 4557.0996\n",
      "Epoch 3/100\n",
      "32/32 [==============================] - 6s 190ms/step - loss: 3923.2996 - val_loss: 3598.5273\n",
      "Epoch 4/100\n",
      "32/32 [==============================] - 5s 156ms/step - loss: 3535.8748 - val_loss: 3352.5701\n",
      "Epoch 5/100\n",
      "32/32 [==============================] - 5s 167ms/step - loss: 3335.4060 - val_loss: 3516.1731\n",
      "Epoch 6/100\n",
      "32/32 [==============================] - 7s 235ms/step - loss: 3200.3135 - val_loss: 3047.9924\n",
      "Epoch 7/100\n",
      "32/32 [==============================] - 6s 181ms/step - loss: 3155.5757 - val_loss: 3029.2061\n",
      "Epoch 8/100\n",
      "32/32 [==============================] - 5s 165ms/step - loss: 3090.0005 - val_loss: 3122.6899\n",
      "Epoch 9/100\n",
      "32/32 [==============================] - 5s 168ms/step - loss: 3025.9790 - val_loss: 2864.4084\n",
      "Epoch 10/100\n",
      "32/32 [==============================] - 7s 205ms/step - loss: 2958.0925 - val_loss: 3107.5437\n",
      "Epoch 11/100\n",
      "32/32 [==============================] - 7s 224ms/step - loss: 2923.0459 - val_loss: 2870.2871\n",
      "Epoch 12/100\n",
      "26/32 [=======================>......] - ETA: 1s - loss: 2843.4033"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find a model.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39moptimizer, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# Entranez le modle\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49m(X_test, y_test))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(X_test, y_test)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m rmse \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msqrt(loss)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/keras/engine/training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1556\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1557\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1558\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1562\u001b[0m ):\n\u001b[1;32m   1563\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1564\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1565\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1566\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2494\u001b[0m   (graph_function,\n\u001b[1;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1865\u001b[0m     args,\n\u001b[1;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1867\u001b[0m     executing_eagerly)\n\u001b[1;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Choisissez l'optimiseur RMSprop avec un learning rate\n",
    "optimizer = RMSprop(learning_rate=0.001)\n",
    "\n",
    "# Crez votre modle de rgression avec des couches Conv1D\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Conv1D(64, 3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle avec l'optimiseur choisi\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=512, validation_data=(X_test, y_test))\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "rmse = np.sqrt(loss)\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5a9374fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "32/32 [==============================] - 68s 911ms/step - loss: 10938.5195 - val_loss: 8681.7393\n",
      "Epoch 2/50\n",
      " 5/32 [===>..........................] - ETA: 23s - loss: 8796.5215"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adagrad\n",
    "\n",
    "# Choisissez l'optimiseur Adagrad avec un learning rate\n",
    "optimizer = Adagrad(learning_rate=0.01)\n",
    "\n",
    "# Crez votre modle de rgression avec des couches LSTM\n",
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(128, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle avec l'optimiseur choisi\n",
    "model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=512, validation_data=(X_test, y_test))\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "rmse = np.sqrt(loss)\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d875ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-30 00:31:28.111384: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-30 00:31:30.492873: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-30 00:31:30.861112: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-30 00:31:30.861153: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-30 00:31:31.135027: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-30 00:31:38.495474: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-30 00:31:38.495917: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-30 00:31:38.495930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Transformer' from 'tensorflow.keras.layers' (/home/guillaumelewagon/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/keras/api/_v2/keras/layers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find a model.ipynb Cell 27\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Utilisez l'optimiseur Adam avec le learning rate par dfaut\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Crez votre modle de rgression avec une architecture de Transformer\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Transformer\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mSequential()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X44sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39madd(Transformer(num_layers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, d_model\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m, num_heads\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m, input_shape\u001b[39m=\u001b[39m(X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Transformer' from 'tensorflow.keras.layers' (/home/guillaumelewagon/.pyenv/versions/lewagon_current/lib/python3.10/site-packages/keras/api/_v2/keras/layers/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Utilisez l'optimiseur Adam avec le learning rate par dfaut\n",
    "\n",
    "# Crez votre modle de rgression avec une architecture de Transformer\n",
    "from tensorflow.keras.layers import Transformer\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Transformer(num_layers=4, d_model=64, num_heads=4, input_shape=(X_train.shape[1], 1)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(1))  # Couche de sortie avec une seule sortie pour la prdiction de la temprature de fusion\n",
    "\n",
    "# Compilez le modle avec l'optimiseur choisi\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=512, validation_data=(X_test, y_test))\n",
    "\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "rmse = np.sqrt(loss)\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "377b2a20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find a model.ipynb Cell 28\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mimport\u001b[39;00m Input, concatenate\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m Model\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m input_layer \u001b[39m=\u001b[39m Input(shape\u001b[39m=\u001b[39m(X_train\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m shared_layer \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDense(\u001b[39m512\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m)(input_layer)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/guillaumelewagon/code/Guillaume2126/Melting-point-prediction/Find%20a%20model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m output_layer \u001b[39m=\u001b[39m layers\u001b[39m.\u001b[39mDense(\u001b[39m1\u001b[39m)(shared_layer)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Choisissez l'optimiseur RMSprop avec un learning rate\n",
    "optimizer = RMSprop(learning_rate=0.001)\n",
    "\n",
    "# Crez deux branches identiques pour le rseau de neurones siamois\n",
    "from tensorflow.keras.layers import Input, concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_layer = Input(shape=(X_train.shape[1],))\n",
    "shared_layer = layers.Dense(512, activation='relu')(input_layer)\n",
    "output_layer = layers.Dense(1)(shared_layer)\n",
    "\n",
    "# Crez le modle siamois en combinant les deux branches\n",
    "siamese_model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compilez le modle avec l'optimiseur choisi\n",
    "siamese_model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "\n",
    "# Entranez le modle\n",
    "siamese_model.fit(X_train, y_train, epochs=100, batch_size=512, validation_data=(X_test, y_test))\n",
    "\n",
    "loss = siamese_model.evaluate(X_test, y_test)\n",
    "rmse = np.sqrt(loss)\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35335022",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bc45d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33426e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model work but improve the learning rate and number of epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b48e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"    Rseaux de Neurones Convolutifs (CNN) : Les CNN sont gnralement utiliss pour la vision par\n",
    "ordinateur, mais ils peuvent galement tre appliqus  d'autres types de donnes sous forme de\n",
    "squences (par exemple, traitement du langage naturel avec des embeddings de mots).\n",
    "Ils sont particulirement bons pour extraire des caractristiques locales  partir des donnes.\n",
    "\n",
    "    Rseaux de Neurones Rcurents (RNN) : Les RNN sont couramment utiliss pour les donnes squentielles, telles que la srie temporelle ou le traitement du langage naturel. Les LSTM (Long Short-Term Memory) et les GRU (Gated Recurrent Unit) sont des variantes populaires des RNN qui peuvent capturer des dpendances  long terme dans les donnes squentielles.\n",
    "\n",
    "    Rseaux de Neurones  Mmoire  Court Terme (LSTM) : Les LSTM sont spcialement conus pour traiter les donnes squentielles et sont capables de grer les dpendances  long terme. Ils sont souvent utiliss dans les applications de traitement du langage naturel et de srie temporelle.\n",
    "\n",
    "    Rseaux de Neurones Convolutifs 1D (1D CNN) : Les CNN 1D sont adapts aux donnes squentielles unidimensionnelles, telles que les squences de texte ou de temps. Ils sont utiles pour extraire des caractristiques  partir de squences.\n",
    "\n",
    "    Rseaux de Neurones  Convolution 2D (2D CNN) : Les CNN 2D sont couramment utiliss pour la vision par ordinateur, mais peuvent galement tre appliqus  d'autres donnes sous forme d'images. Ils sont excellents pour la dtection de motifs spatiaux dans les images.\n",
    "\n",
    "    Rseaux de Neurones Rcurents Bidirectionnels (Bi-LSTM, Bi-GRU) : Ces modles permettent de prendre en compte les informations passes et futures lors de la prdiction. Ils sont utiles pour les tches de squence o les informations des deux directions sont importantes.\n",
    "\n",
    "    Transformers : Les modles Transformer sont devenus trs populaires pour le traitement du langage naturel, mais ils peuvent galement tre appliqus  d'autres types de donnes squentielles. Ils sont particulirement adapts aux tches de modlisation de langage et de traduction.\n",
    "\n",
    "    Rseaux de Neurones Rsiduels (ResNet) : Les ResNets sont souvent utiliss pour des tches de vision par ordinateur. Ils utilisent des connexions rsiduelles pour faciliter l'entranement de rseaux plus profonds.\n",
    "\n",
    "    Rseaux de Neurones Siameses : Ces modles sont utiliss pour des tches de comparaison, de similarit ou de recherche d'anomalies. Ils utilisent deux branches de rseau pour comparer des paires de donnes.\n",
    "\n",
    "    Rseaux de Neurones Gnratifs (GAN, VAE) : Les GAN (Generative Adversarial Networks) et les VAE (Variational Autoencoders) sont utiliss pour la gnration de donnes, la synthse d'images et la cration de contenu.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248a7f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 3ms/step - loss: 2564.5828\n",
      "RMSE: 50.641709723032406\n"
     ]
    }
   ],
   "source": [
    "mse = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Calcul de la RMSE en prenant la racine carre de la MSE\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(\"RMSE:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bab116d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 2ms/step\n",
      "[[256.9517  ]\n",
      " [ 94.37111 ]\n",
      " [160.5401  ]\n",
      " ...\n",
      " [ 13.007814]\n",
      " [187.63498 ]\n",
      " [121.90892 ]]\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
